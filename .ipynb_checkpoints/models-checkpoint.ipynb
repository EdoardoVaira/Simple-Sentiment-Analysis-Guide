{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb69b0e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis models\n",
    "\n",
    "In this notebook we will present all the models used for our problem and we will compare their performance.\n",
    "\n",
    "First of all, we load our preprocessed dataset and do all the different vectorizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22f7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#¬†Text processing\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from gensim import models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6c4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a75c4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9010cf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40309\n",
      "(113292, 40309)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the data\n",
    "bow = vectorizer.fit_transform(df['Content_cleaned'])\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc27ed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40309\n",
      "(113292, 40309)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the model and transform the data\n",
    "tfidf = vectorizer.fit_transform(df['Content_cleaned'])\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4aec5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = models.KeyedVectors.load_word2vec_format(\n",
    "'../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dff6267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>[plsssss, stoppppp, giving, screen, limit, lik...</td>\n",
       "      <td>[0.057294574, 0.033310752, 0.050110083, 0.1152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumbs_up thumbs_up</td>\n",
       "      <td>[thumbs_up, thumbs_up]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app is useful to certain phone brand it is not...</td>\n",
       "      <td>[app, is, useful, to, certain, phone, brand, i...</td>\n",
       "      <td>[0.016780308, -0.041579314, 0.043486457, 0.058...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plsssss stoppppp giving screen limit like when...   \n",
       "1                                               good   \n",
       "2                                thumbs_up thumbs_up   \n",
       "3                                               good   \n",
       "4  app is useful to certain phone brand it is not...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plsssss, stoppppp, giving, screen, limit, lik...   \n",
       "1                                             [good]   \n",
       "2                             [thumbs_up, thumbs_up]   \n",
       "3                                             [good]   \n",
       "4  [app, is, useful, to, certain, phone, brand, i...   \n",
       "\n",
       "                                 word2vec_pretrained  \n",
       "0  [0.057294574, 0.033310752, 0.050110083, 0.1152...  \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...  \n",
       "4  [0.016780308, -0.041579314, 0.043486457, 0.058...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_word2vec(tokens_list, model, vector_size):\n",
    "    \"\"\"\n",
    "    This function computes the average Word2Vec for a given list of tokens.\n",
    "    \"\"\"\n",
    "    # Filter the tokens that are present in the Word2Vec model\n",
    "    valid_tokens = [token for token in tokens_list if token in model]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Compute the average Word2Vec\n",
    "    word_vectors = [model[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['Content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Compute the average Word2Vec for each row\n",
    "vector_size = w2v.vector_size\n",
    "df['word2vec_pretrained'] = df['tokens'].apply(lambda x: get_average_word2vec(x, w2v, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862d91e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>[plsssss, stoppppp, giving, screen, limit, lik...</td>\n",
       "      <td>[0.057294574, 0.033310752, 0.050110083, 0.1152...</td>\n",
       "      <td>[0.02010641, 0.16096787, -0.32297274, 0.196979...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.74799234, -1.4148954, 0.66798717, -0.108562...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumbs_up thumbs_up</td>\n",
       "      <td>[thumbs_up, thumbs_up]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.37583166, 0.09423453, 0.109816454, -0.0307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.74799234, -1.4148954, 0.66798717, -0.108562...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app is useful to certain phone brand it is not...</td>\n",
       "      <td>[app, is, useful, to, certain, phone, brand, i...</td>\n",
       "      <td>[0.016780308, -0.041579314, 0.043486457, 0.058...</td>\n",
       "      <td>[-0.024173666, -0.40380958, -0.22606301, 0.166...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plsssss stoppppp giving screen limit like when...   \n",
       "1                                               good   \n",
       "2                                thumbs_up thumbs_up   \n",
       "3                                               good   \n",
       "4  app is useful to certain phone brand it is not...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plsssss, stoppppp, giving, screen, limit, lik...   \n",
       "1                                             [good]   \n",
       "2                             [thumbs_up, thumbs_up]   \n",
       "3                                             [good]   \n",
       "4  [app, is, useful, to, certain, phone, brand, i...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.057294574, 0.033310752, 0.050110083, 0.1152...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.016780308, -0.041579314, 0.043486457, 0.058...   \n",
       "\n",
       "                                       word2vec_cbow  \n",
       "0  [0.02010641, 0.16096787, -0.32297274, 0.196979...  \n",
       "1  [0.74799234, -1.4148954, 0.66798717, -0.108562...  \n",
       "2  [-0.37583166, 0.09423453, 0.109816454, -0.0307...  \n",
       "3  [0.74799234, -1.4148954, 0.66798717, -0.108562...  \n",
       "4  [-0.024173666, -0.40380958, -0.22606301, 0.166...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_word2vec2(tokens_list, model, vector_size):\n",
    "    valid_tokens = [token for token in tokens_list if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    word_vectors = [model.wv[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define model parameters\n",
    "vector_size = 300   # Dimensionality of the word vectors\n",
    "window_size = 5     # Context window size\n",
    "min_count = 1       # Minimum word frequency\n",
    "workers = multiprocessing.cpu_count()  # Number of worker threads to use\n",
    "\n",
    "# Train the Word2Vec model\n",
    "cbow = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=0, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "df['word2vec_cbow'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, cbow, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299c8acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors from GloVe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>glove_6B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>[plsssss, stoppppp, giving, screen, limit, lik...</td>\n",
       "      <td>[0.057294574, 0.033310752, 0.050110083, 0.1152...</td>\n",
       "      <td>[0.02010641, 0.16096787, -0.32297274, 0.196979...</td>\n",
       "      <td>[-0.09509568, 0.22709598, 0.4678306, -0.256105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.74799234, -1.4148954, 0.66798717, -0.108562...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumbs_up thumbs_up</td>\n",
       "      <td>[thumbs_up, thumbs_up]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.37583166, 0.09423453, 0.109816454, -0.0307...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.74799234, -1.4148954, 0.66798717, -0.108562...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app is useful to certain phone brand it is not...</td>\n",
       "      <td>[app, is, useful, to, certain, phone, brand, i...</td>\n",
       "      <td>[0.016780308, -0.041579314, 0.043486457, 0.058...</td>\n",
       "      <td>[-0.024173666, -0.40380958, -0.22606301, 0.166...</td>\n",
       "      <td>[-0.19991928, 0.11995281, 0.36286283, -0.22692...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plsssss stoppppp giving screen limit like when...   \n",
       "1                                               good   \n",
       "2                                thumbs_up thumbs_up   \n",
       "3                                               good   \n",
       "4  app is useful to certain phone brand it is not...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plsssss, stoppppp, giving, screen, limit, lik...   \n",
       "1                                             [good]   \n",
       "2                             [thumbs_up, thumbs_up]   \n",
       "3                                             [good]   \n",
       "4  [app, is, useful, to, certain, phone, brand, i...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.057294574, 0.033310752, 0.050110083, 0.1152...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.016780308, -0.041579314, 0.043486457, 0.058...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [0.02010641, 0.16096787, -0.32297274, 0.196979...   \n",
       "1  [0.74799234, -1.4148954, 0.66798717, -0.108562...   \n",
       "2  [-0.37583166, 0.09423453, 0.109816454, -0.0307...   \n",
       "3  [0.74799234, -1.4148954, 0.66798717, -0.108562...   \n",
       "4  [-0.024173666, -0.40380958, -0.22606301, 0.166...   \n",
       "\n",
       "                                            glove_6B  \n",
       "0  [-0.09509568, 0.22709598, 0.4678306, -0.256105...  \n",
       "1  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...  \n",
       "4  [-0.19991928, 0.11995281, 0.36286283, -0.22692...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GloVe embeddings file\n",
    "glove_file = '../glove.6B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_6b = load_glove_embeddings(glove_file)\n",
    "print(f\"Loaded {len(glove_6b)} word vectors from GloVe.\")\n",
    "\n",
    "# Define a function to get the average GloVe vector for a list of tokens\n",
    "def get_average_glove(tokens_list, embeddings, embedding_dim):\n",
    "    valid_tokens = [token for token in tokens_list if token in embeddings]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(embedding_dim)\n",
    "    word_vectors = [embeddings[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define the embedding dimension (e.g., 100 for 'glove.6B.100d.txt')\n",
    "embedding_dim = 100\n",
    "\n",
    "# Compute the average GloVe vector for each row\n",
    "df['glove_6B'] = df['tokens'].apply(lambda x: get_average_glove(x, glove_6b, embedding_dim))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dcc6bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors from GloVe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>glove_6B</th>\n",
       "      <th>glove_twitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>[plsssss, stoppppp, giving, screen, limit, lik...</td>\n",
       "      <td>[0.057294574, 0.033310752, 0.050110083, 0.1152...</td>\n",
       "      <td>[0.02010641, 0.16096787, -0.32297274, 0.196979...</td>\n",
       "      <td>[-0.09509568, 0.22709598, 0.4678306, -0.256105...</td>\n",
       "      <td>[0.070526205, 0.16308755, 0.0894654, 0.1647201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.74799234, -1.4148954, 0.66798717, -0.108562...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "      <td>[0.091552, 0.093336, -0.028113, 0.3699, 0.1895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumbs_up thumbs_up</td>\n",
       "      <td>[thumbs_up, thumbs_up]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.37583166, 0.09423453, 0.109816454, -0.0307...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.74799234, -1.4148954, 0.66798717, -0.108562...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "      <td>[0.091552, 0.093336, -0.028113, 0.3699, 0.1895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app is useful to certain phone brand it is not...</td>\n",
       "      <td>[app, is, useful, to, certain, phone, brand, i...</td>\n",
       "      <td>[0.016780308, -0.041579314, 0.043486457, 0.058...</td>\n",
       "      <td>[-0.024173666, -0.40380958, -0.22606301, 0.166...</td>\n",
       "      <td>[-0.19991928, 0.11995281, 0.36286283, -0.22692...</td>\n",
       "      <td>[0.23760791, 0.07707109, 0.06094666, 0.2031615...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plsssss stoppppp giving screen limit like when...   \n",
       "1                                               good   \n",
       "2                                thumbs_up thumbs_up   \n",
       "3                                               good   \n",
       "4  app is useful to certain phone brand it is not...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plsssss, stoppppp, giving, screen, limit, lik...   \n",
       "1                                             [good]   \n",
       "2                             [thumbs_up, thumbs_up]   \n",
       "3                                             [good]   \n",
       "4  [app, is, useful, to, certain, phone, brand, i...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.057294574, 0.033310752, 0.050110083, 0.1152...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.016780308, -0.041579314, 0.043486457, 0.058...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [0.02010641, 0.16096787, -0.32297274, 0.196979...   \n",
       "1  [0.74799234, -1.4148954, 0.66798717, -0.108562...   \n",
       "2  [-0.37583166, 0.09423453, 0.109816454, -0.0307...   \n",
       "3  [0.74799234, -1.4148954, 0.66798717, -0.108562...   \n",
       "4  [-0.024173666, -0.40380958, -0.22606301, 0.166...   \n",
       "\n",
       "                                            glove_6B  \\\n",
       "0  [-0.09509568, 0.22709598, 0.4678306, -0.256105...   \n",
       "1  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...   \n",
       "4  [-0.19991928, 0.11995281, 0.36286283, -0.22692...   \n",
       "\n",
       "                                       glove_twitter  \n",
       "0  [0.070526205, 0.16308755, 0.0894654, 0.1647201...  \n",
       "1  [0.091552, 0.093336, -0.028113, 0.3699, 0.1895...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.091552, 0.093336, -0.028113, 0.3699, 0.1895...  \n",
       "4  [0.23760791, 0.07707109, 0.06094666, 0.2031615...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GloVe embeddings file\n",
    "glove_file = '../glove.twitter.27B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_twitter = load_glove_embeddings(glove_file)\n",
    "print(f\"Loaded {len(glove_twitter)} word vectors from GloVe.\")\n",
    "\n",
    "# Define a function to get the average GloVe vector for a list of tokens\n",
    "def get_average_glove(tokens_list, embeddings, embedding_dim):\n",
    "    valid_tokens = [token for token in tokens_list if token in embeddings]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(embedding_dim)\n",
    "    word_vectors = [embeddings[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define the embedding dimension (e.g., 100 for 'glove.6B.100d.txt')\n",
    "embedding_dim = 100\n",
    "\n",
    "# Compute the average GloVe vector for each row\n",
    "df['glove_twitter'] = df['tokens'].apply(lambda x: get_average_glove(x, glove_twitter, embedding_dim))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae405d1",
   "metadata": {},
   "source": [
    "\n",
    "### Preparing the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54527bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = df['Sentiment']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c2588",
   "metadata": {},
   "source": [
    "### Performing the train-test splits for the different vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0349ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train, bow_test, y_train, y_test = train_test_split(bow, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1de93546",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train, tfidf_test, y_train, y_test = train_test_split(tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d2c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pre = np.vstack(df['word2vec_pretrained'].values)\n",
    "\n",
    "w2v_pre_train, w2v_pre_test, y_train, y_test = train_test_split(w2v_pre, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "084a1c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_cbow = np.vstack(df['word2vec_cbow'].values)\n",
    "\n",
    "w2v_cbow_train, w2v_cbow_test, y_train, y_test = train_test_split(w2v_cbow, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fe2e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b = np.vstack(df['glove_6B'].values)\n",
    "\n",
    "glove_6b_train, glove_6b_test, y_train, y_test = train_test_split(glove_6b, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed8d493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter = np.vstack(df['glove_twitter'].values)\n",
    "\n",
    "glove_twitter_train, glove_twitter_test, y_train, y_test = train_test_split(glove_twitter, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caca3e",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7046d612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using BoW and Logistic Regression: 0.7298203804227901\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False because BoW has sparse matrix format\n",
    "bow_train_scaled = scaler.fit_transform(bow_train)\n",
    "bow_test_scaled = scaler.transform(bow_test)\n",
    "\n",
    "# Logistic Regression model\n",
    "lr_bow = LogisticRegression()\n",
    "lr_bow.fit(bow_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_bow.predict(bow_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using BoW and Logistic Regression: {accuracy}\")\n",
    "\n",
    "results = {}\n",
    "results['lr_bow'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23ae0345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using TFIDF and Logistic Regression: 0.7260249790370272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(with_mean=False)  # with_mean=False because BoW has sparse matrix format\n",
    "tfidf_train_scaled = scaler.fit_transform(tfidf_train)\n",
    "tfidf_test_scaled = scaler.transform(tfidf_test)\n",
    "\n",
    "# Logistic Regression model\n",
    "lr_tfidf = LogisticRegression()\n",
    "lr_tfidf.fit(tfidf_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_tfidf.predict(tfidf_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using TFIDF and Logistic Regression: {accuracy}\")\n",
    "\n",
    "results['lr_tfidf'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9206738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using W2V_pretrained and Logistic Regression: 0.7799991173485149\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model\n",
    "lr_w2v_pre = LogisticRegression()\n",
    "lr_w2v_pre.fit(w2v_pre_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_w2v_pre.predict(w2v_pre_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using W2V_pretrained and Logistic Regression: {accuracy}\")\n",
    "\n",
    "results['lr_w2v_pre'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa978ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using W2V_CBOW and Logistic Regression: 0.791032260911779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(with_mean=False)  # with_mean=False because BoW has sparse matrix format\n",
    "w2v_cbow_train_scaled = scaler.fit_transform(w2v_cbow_train)\n",
    "w2v_cbow_test_scaled = scaler.transform(w2v_cbow_test)\n",
    "\n",
    "# Logistic Regression model\n",
    "lr_w2v_cbow = LogisticRegression(max_iter=500)\n",
    "lr_w2v_cbow.fit(w2v_cbow_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_w2v_cbow.predict(w2v_cbow_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using W2V_CBOW and Logistic Regression: {accuracy}\")\n",
    "\n",
    "results['lr_w2b_cbow'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2ec735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using W2V_CBOW and Logistic Regression: 0.7509157509157509\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(with_mean=False)  # with_mean=False because BoW has sparse matrix format\n",
    "glove_6b_train_scaled = scaler.fit_transform(glove_6b_train)\n",
    "glove_6b_test_scaled = scaler.transform(glove_6b_test)\n",
    "\n",
    "# Logistic Regression model\n",
    "lr_glove_6b = LogisticRegression(max_iter=500)\n",
    "lr_glove_6b.fit(glove_6b_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_glove_6b.predict(glove_6b_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using W2V_CBOW and Logistic Regression: {accuracy}\")\n",
    "\n",
    "results['lr_glove_6b'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3fd0d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using W2V_CBOW and Logistic Regression: 0.7677302617061653\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(with_mean=False)  # with_mean=False because BoW has sparse matrix format\n",
    "glove_twitter_train_scaled = scaler.fit_transform(glove_twitter_train)\n",
    "glove_twitter_test_scaled = scaler.transform(glove_twitter_test)\n",
    "\n",
    "# Logistic Regression model\n",
    "lr_glove_twitter = LogisticRegression(max_iter=500)\n",
    "lr_glove_twitter.fit(glove_twitter_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_glove_twitter.predict(glove_twitter_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using W2V_CBOW and Logistic Regression: {accuracy}\")\n",
    "\n",
    "results['lr_glove_twitter'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a2d613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Accuracy\n",
      "0            lr_bow  0.729820\n",
      "1          lr_tfidf  0.726025\n",
      "2        lr_w2v_pre  0.779999\n",
      "3       lr_w2b_cbow  0.791032\n",
      "4       lr_glove_6b  0.750916\n",
      "5  lr_glove_twitter  0.767730\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Print the results table\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa727497",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99e37c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using BoW and SVC: 0.7825588066551922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc_bow = LinearSVC(C=1, max_iter=5000)\n",
    "svc_bow.fit(bow_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svc_bow.predict(bow_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using BoW and SVC: {accuracy}\")\n",
    "\n",
    "results = {}\n",
    "results['svc_bow'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1358f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using tfidf and SVC: 0.7966812304161702\n"
     ]
    }
   ],
   "source": [
    "svc_tfidf = LinearSVC(C=1, max_iter=3000)\n",
    "svc_tfidf.fit(tfidf_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svc_tfidf.predict(tfidf_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using tfidf and SVC: {accuracy}\")\n",
    "\n",
    "results['svc_tfidf'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6319ebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using w2v_pre and SVC: 0.7809259014078291\n"
     ]
    }
   ],
   "source": [
    "svc_w2v_pre = LinearSVC(C=1, max_iter=5000)\n",
    "svc_w2v_pre.fit(w2v_pre_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svc_w2v_pre.predict(w2v_pre_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using w2v_pre and SVC: {accuracy}\")\n",
    "\n",
    "results['svc_w2v_pre'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c28fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using w2v_cbow and SVC: 0.7904585374464893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svc_w2v_cbow = LinearSVC(C=1, max_iter=5000)\n",
    "svc_w2v_cbow.fit(w2v_cbow_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svc_w2v_cbow.predict(w2v_cbow_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using w2v_cbow and SVC: {accuracy}\")\n",
    "\n",
    "results['svc_w2v_cbow'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb2e8358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using glove_6b and SVC: 0.7519749326978242\n"
     ]
    }
   ],
   "source": [
    "svc_glove_6b = LinearSVC(C=1, max_iter=5000)\n",
    "svc_glove_6b.fit(glove_6b_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svc_glove_6b.predict(glove_6b_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using glove_6b and SVC: {accuracy}\")\n",
    "\n",
    "results['svc_glove_6b'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d426e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using glove_twitter and SVC: 0.766671079924092\n"
     ]
    }
   ],
   "source": [
    "svc_glove_twitter = LinearSVC(C=1, max_iter=5000)\n",
    "svc_glove_twitter.fit(glove_twitter_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svc_glove_twitter.predict(glove_twitter_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using glove_twitter and SVC: {accuracy}\")\n",
    "\n",
    "results['svc_glove_twitter'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bfabc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  Accuracy\n",
      "0            svc_bow  0.773688\n",
      "1          svc_tfidf  0.786796\n",
      "2        svc_w2v_pre  0.776689\n",
      "3       svc_w2v_cbow  0.786222\n",
      "4       svc_glove_6b  0.753034\n",
      "5  svc_glove_twitter  0.761596\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Print the results table\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d185702",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de32fd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using BoW and RF: 0.784765435367845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_bow = RandomForestClassifier()\n",
    "\n",
    "rf_bow.fit(bow_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_bow.predict(bow_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using BoW and RF: {accuracy}\")\n",
    "\n",
    "results = {}\n",
    "results['rf_bow'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7a674b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using TFIDF and RF: 0.7811465642790943\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model\n",
    "rf_tfidf = RandomForestClassifier()\n",
    "rf_tfidf.fit(tfidf_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_tfidf.predict(tfidf_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using TFIDF and RF: {accuracy}\")\n",
    "\n",
    "results['rf_tfidf'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f39b7f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using W2V_pretrained and RF: 0.754887682598526\n"
     ]
    }
   ],
   "source": [
    "rf_w2v_pre = RandomForestClassifier()\n",
    "rf_w2v_pre.fit(w2v_pre_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_w2v_pre.predict(w2v_pre_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using W2V_pretrained and RF: {accuracy}\")\n",
    "\n",
    "results['rf_w2v_pre'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be2b8973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using W2V_cbow and RF: 0.7751445341806787\n"
     ]
    }
   ],
   "source": [
    "rf_w2v_cbow = RandomForestClassifier()\n",
    "rf_w2v_cbow.fit(w2v_cbow_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_w2v_cbow.predict(w2v_cbow_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using W2V_cbow and RF: {accuracy}\")\n",
    "\n",
    "results['rf_w2v_cbow'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcc216ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using glove_6b and RF: 0.7398384747782338\n"
     ]
    }
   ],
   "source": [
    "rf_glove_6b = RandomForestClassifier()\n",
    "rf_glove_6b.fit(glove_6b_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_glove_6b.predict(glove_6b_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using glove_6b and RF: {accuracy}\")\n",
    "\n",
    "results['rf_glove_6b'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1474c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using glove_twitter and RF: 0.7510481486385101\n"
     ]
    }
   ],
   "source": [
    "rf_glove_twitter = RandomForestClassifier()\n",
    "rf_glove_twitter.fit(glove_twitter_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_glove_twitter.predict(glove_twitter_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using glove_twitter and RF: {accuracy}\")\n",
    "\n",
    "results['rf_glove_twitter'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5d9f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Accuracy\n",
      "0            rf_bow  0.783441\n",
      "1          rf_tfidf  0.781147\n",
      "2        rf_w2v_pre  0.757845\n",
      "3       rf_w2v_cbow  0.775145\n",
      "4       rf_glove_6b  0.741780\n",
      "5  rf_glove_twitter  0.751048\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Print the results table\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52122915",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "308a1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7927092987333951\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(bow_train, label=y_train)\n",
    "dtest = xgb.DMatrix(bow_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'max_depth': 8,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softprob',  # or 'multi:softmax' for multi-class classification\n",
    "    'num_class': len(set(y))  # specify the number of classes if doing multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100  # number of boosting rounds\n",
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "class_preds = preds.argmax(axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, class_preds)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "results = {}\n",
    "results['xgb_bow'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9dadd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7907674654662606\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(tfidf_train, label=y_train)\n",
    "dtest = xgb.DMatrix(tfidf_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'max_depth': 8,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softprob',  # or 'multi:softmax' for multi-class classification\n",
    "    'num_class': len(set(y))  # specify the number of classes if doing multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100  # number of boosting rounds\n",
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "class_preds = preds.argmax(axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, class_preds)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "results['xgb_tfidf'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1b912a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.777704223487356\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(w2v_pre_train, label=y_train)\n",
    "dtest = xgb.DMatrix(w2v_pre_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'max_depth': 8,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softprob',  # or 'multi:softmax' for multi-class classification\n",
    "    'num_class': len(set(y))  # specify the number of classes if doing multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100  # number of boosting rounds\n",
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "class_preds = preds.argmax(axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, class_preds)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "results['xgb_w2v_pre'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4d1b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7870161966547509\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(w2v_cbow_train, label=y_train)\n",
    "dtest = xgb.DMatrix(w2v_cbow_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'max_depth': 8,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softprob',  # or 'multi:softmax' for multi-class classification\n",
    "    'num_class': len(set(y))  # specify the number of classes if doing multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100  # number of boosting rounds\n",
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "class_preds = preds.argmax(axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, class_preds)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "results['xgb_w2v_cbow'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "131f9d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7573149741824441\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(glove_6b_train, label=y_train)\n",
    "dtest = xgb.DMatrix(glove_6b_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softprob',  # or 'multi:softmax' for multi-class classification\n",
    "    'num_class': len(set(y))  # specify the number of classes if doing multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100  # number of boosting rounds\n",
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "class_preds = preds.argmax(axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, class_preds)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "results['xgb_glove_6b'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01480b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7698486252703121\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(glove_twitter_train, label=y_train)\n",
    "dtest = xgb.DMatrix(glove_twitter_test, label=y_test)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softprob',  # or 'multi:softmax' for multi-class classification\n",
    "    'num_class': len(set(y))  # specify the number of classes if doing multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 100  # number of boosting rounds\n",
    "bst = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Make predictions\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "class_preds = preds.argmax(axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, class_preds)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "results['xgb_glove_twitter'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8cf7808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  Accuracy\n",
      "0            xgb_bow  0.792312\n",
      "1          xgb_tfidf  0.789973\n",
      "2        xgb_w2v_pre  0.777704\n",
      "3       xgb_w2v_cbow  0.784545\n",
      "4       xgb_glove_6b  0.757315\n",
      "5  xgb_glove_twitter  0.768878\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Print the results table\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51c32d",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "In RNN BoW and TFIDF are not used. Instead the sequences are tokenized and paded and thee embeddings are either learned by the network or are given to them as pretrained, like glove or w2v embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f535e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "MAX_NB_WORDS = 50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(df['Content_cleaned'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['Content_cleaned'].values)\n",
    "\n",
    "sequence_lengths = [len(sequence) for sequence in sequences]\n",
    "\n",
    "# Plot histogram of sequence lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sequence_lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Sequence Lengths')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c5984",
   "metadata": {},
   "source": [
    "We notice that the majority of our inputs are up to 150 sequence length, therefore we will pad to 170, to contain some edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "642f8d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 188ms/step - accuracy: 0.7217 - loss: 0.7047 - val_accuracy: 0.8049 - val_loss: 0.5535\n",
      "Epoch 2/3\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 181ms/step - accuracy: 0.8096 - loss: 0.5226 - val_accuracy: 0.8109 - val_loss: 0.5151\n",
      "Epoch 3/3\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 188ms/step - accuracy: 0.8215 - loss: 0.4745 - val_accuracy: 0.8079 - val_loss: 0.5202\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step\n",
      "Accuracy: 0.8108919193256543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86     11152\n",
      "           1       0.20      0.00      0.00      2318\n",
      "           2       0.82      0.87      0.85      9189\n",
      "\n",
      "    accuracy                           0.81     22659\n",
      "   macro avg       0.61      0.60      0.57     22659\n",
      "weighted avg       0.75      0.81      0.77     22659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 170\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "batch_size = 128\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "772ff10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 406ms/step - accuracy: 0.7209 - loss: 0.6939 - val_accuracy: 0.8024 - val_loss: 0.5357\n",
      "Epoch 2/3\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 409ms/step - accuracy: 0.8117 - loss: 0.5083 - val_accuracy: 0.8103 - val_loss: 0.5160\n",
      "Epoch 3/3\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 469ms/step - accuracy: 0.8200 - loss: 0.4781 - val_accuracy: 0.8085 - val_loss: 0.5198\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 115ms/step\n",
      "Accuracy: 0.8103181958603646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86     11152\n",
      "           1       0.37      0.03      0.05      2318\n",
      "           2       0.83      0.86      0.85      9189\n",
      "\n",
      "    accuracy                           0.81     22659\n",
      "   macro avg       0.67      0.61      0.59     22659\n",
      "weighted avg       0.77      0.81      0.77     22659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 170\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "batch_size = 128\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a89109a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 233ms/step - accuracy: 0.7428 - loss: 0.6682 - val_accuracy: 0.8047 - val_loss: 0.5278\n",
      "Epoch 2/3\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 220ms/step - accuracy: 0.7968 - loss: 0.5469 - val_accuracy: 0.8083 - val_loss: 0.5169\n",
      "Epoch 3/3\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 216ms/step - accuracy: 0.8043 - loss: 0.5226 - val_accuracy: 0.8113 - val_loss: 0.5047\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 92ms/step\n",
      "Accuracy: 0.8113332450681848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86     11152\n",
      "           1       0.36      0.05      0.09      2318\n",
      "           2       0.82      0.87      0.85      9189\n",
      "\n",
      "    accuracy                           0.81     22659\n",
      "   macro avg       0.67      0.61      0.60     22659\n",
      "weighted avg       0.77      0.81      0.78     22659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = cbow#models.KeyedVectors.load_word2vec_format(\n",
    "#'../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "MAX_SEQUENCE_LENGTH = 170\n",
    "EMBEDDING_DIM = 300  # Word2Vec model dimension\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Content_cleaned'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Content_cleaned'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the embedding matrix\n",
    "num_words = min(len(word_index) + 1, len(word2vec_model.wv.index_to_key))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word2vec_model.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "batch_size = 128\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ff38a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 197ms/step - accuracy: 0.8100 - loss: 0.5048 - val_accuracy: 0.8131 - val_loss: 0.4992\n",
      "Epoch 2/3\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 192ms/step - accuracy: 0.8142 - loss: 0.4958 - val_accuracy: 0.8147 - val_loss: 0.4970\n",
      "Epoch 3/3\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 187ms/step - accuracy: 0.8140 - loss: 0.4930 - val_accuracy: 0.8160 - val_loss: 0.4970\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 85ms/step\n",
      "Accuracy: 0.8159671653647558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87     11152\n",
      "           1       0.39      0.07      0.12      2318\n",
      "           2       0.83      0.88      0.85      9189\n",
      "\n",
      "    accuracy                           0.82     22659\n",
      "   macro avg       0.68      0.62      0.61     22659\n",
      "weighted avg       0.78      0.82      0.78     22659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 3\n",
    "batch_size = 128\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.0003), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03b34150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1417/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 95ms/step - accuracy: 0.7075 - loss: 0.7336 - val_accuracy: 0.7935 - val_loss: 0.5636\n",
      "Epoch 2/10\n",
      "\u001b[1m1417/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 96ms/step - accuracy: 0.7805 - loss: 0.5941 - val_accuracy: 0.8016 - val_loss: 0.5371\n",
      "Epoch 3/10\n",
      "\u001b[1m1417/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 94ms/step - accuracy: 0.7873 - loss: 0.5769 - val_accuracy: 0.8054 - val_loss: 0.5256\n",
      "Epoch 4/10\n",
      "\u001b[1m 399/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:34\u001b[0m 93ms/step - accuracy: 0.7970 - loss: 0.5473"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m     59\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test), callbacks\u001b[38;5;241m=\u001b[39m[early_stopping], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     64\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1506\u001b[0m   )\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Path to the GloVe embeddings file\n",
    "glove_file = '../glove.twitter.27B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "embeddings_index  = load_glove_embeddings(glove_file)\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 100  # Word2Vec model dimension\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Content_cleaned'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Content_cleaned'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the embedding matrix\n",
    "num_words = min(len(word_index) + 1, len(word2vec_model.index_to_key))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "        \n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af526b2",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ca6cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1417/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 147ms/step - accuracy: 0.7082 - loss: 3082503936.0000 - val_accuracy: 0.7028 - val_loss: 0.7842\n",
      "Epoch 2/3\n",
      "\u001b[1m1417/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 156ms/step - accuracy: 0.6973 - loss: 364761710592.0000 - val_accuracy: 0.6833 - val_loss: 0.7870\n",
      "Epoch 3/3\n",
      "\u001b[1m1417/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 152ms/step - accuracy: 0.7185 - loss: 38482272256.0000 - val_accuracy: 0.6737 - val_loss: 0.8057\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step\n",
      "Accuracy: 0.6737278785471557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.73     11152\n",
      "           1       0.50      0.00      0.00      2318\n",
      "           2       0.65      0.73      0.69      9189\n",
      "\n",
      "    accuracy                           0.67     22659\n",
      "   macro avg       0.61      0.50      0.47     22659\n",
      "weighted avg       0.66      0.67      0.64     22659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "MAX_NB_WORDS = 50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(df['Content_cleaned'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['Content_cleaned'].values)\n",
    "\n",
    "sequence_lengths = [len(sequence) for sequence in sequences]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ce296ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1417/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 126ms/step - accuracy: 0.6479 - loss: 1.6253 - val_accuracy: 0.6702 - val_loss: 0.8088\n",
      "Epoch 2/3\n",
      "\u001b[1m 634/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:39\u001b[0m 127ms/step - accuracy: 0.6838 - loss: 0.7939"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m     47\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test), callbacks\u001b[38;5;241m=\u001b[39m[early_stopping], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     52\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1506\u001b[0m   )\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word2vec_model = models.KeyedVectors.load_word2vec_format(\n",
    "'../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 300  # Word2Vec model dimension\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Content_cleaned'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['Content_cleaned'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the embedding matrix\n",
    "num_words = min(len(word_index) + 1, len(word2vec_model.index_to_key))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word2vec_model[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4f021",
   "metadata": {},
   "source": [
    "### MultiHeadAttention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "490f714d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                  </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape              </span>‚îÉ<span style=\"font-weight: bold\">         Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to               </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)               ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                          ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> ‚îÇ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ multi_head_attention_5        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> ‚îÇ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)          ‚îÇ                           ‚îÇ                 ‚îÇ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ multi_head_attention_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          ‚îÇ\n",
       "‚îÇ                               ‚îÇ                           ‚îÇ                 ‚îÇ dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalization_10        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> ‚îÇ layer_normalization_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> ‚îÇ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ layer_normalization_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ                               ‚îÇ                           ‚îÇ                 ‚îÇ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalization_11        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ multi_head_attention_6        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> ‚îÇ layer_normalization_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)          ‚îÇ                           ‚îÇ                 ‚îÇ layer_normalization_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ multi_head_attention_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ layer_normalization_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ                               ‚îÇ                           ‚îÇ                 ‚îÇ dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalization_12        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> ‚îÇ layer_normalization_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> ‚îÇ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ layer_normalization_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ                               ‚îÇ                           ‚îÇ                 ‚îÇ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalization_13        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_average_pooling1d_2    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ layer_normalization_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ global_average_pooling1d_‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> ‚îÇ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)               ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ -                          ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ       \u001b[38;5;34m1,280,000\u001b[0m ‚îÇ input_layer_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_12 (\u001b[38;5;33mAdd\u001b[0m)                  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ multi_head_attention_5        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ         \u001b[38;5;34m527,488\u001b[0m ‚îÇ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)          ‚îÇ                           ‚îÇ                 ‚îÇ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ multi_head_attention_5[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_13 (\u001b[38;5;33mAdd\u001b[0m)                  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          ‚îÇ\n",
       "‚îÇ                               ‚îÇ                           ‚îÇ                 ‚îÇ dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalization_10        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ             \u001b[38;5;34m256\u001b[0m ‚îÇ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mLayerNormalization\u001b[0m)          ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_18 (\u001b[38;5;33mDense\u001b[0m)              ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ          \u001b[38;5;34m16,512\u001b[0m ‚îÇ layer_normalization_10[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_19 (\u001b[38;5;33mDense\u001b[0m)              ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ          \u001b[38;5;34m16,512\u001b[0m ‚îÇ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_14 (\u001b[38;5;33mAdd\u001b[0m)                  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ layer_normalization_10[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ                               ‚îÇ                           ‚îÇ                 ‚îÇ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalization_11        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ             \u001b[38;5;34m256\u001b[0m ‚îÇ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mLayerNormalization\u001b[0m)          ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ multi_head_attention_6        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ         \u001b[38;5;34m527,488\u001b[0m ‚îÇ layer_normalization_11[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)          ‚îÇ                           ‚îÇ                 ‚îÇ layer_normalization_11[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ multi_head_attention_6[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_15 (\u001b[38;5;33mAdd\u001b[0m)                  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ layer_normalization_11[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ                               ‚îÇ                           ‚îÇ                 ‚îÇ dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalization_12        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ             \u001b[38;5;34m256\u001b[0m ‚îÇ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mLayerNormalization\u001b[0m)          ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_20 (\u001b[38;5;33mDense\u001b[0m)              ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ          \u001b[38;5;34m16,512\u001b[0m ‚îÇ layer_normalization_12[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_21 (\u001b[38;5;33mDense\u001b[0m)              ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ          \u001b[38;5;34m16,512\u001b[0m ‚îÇ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ add_16 (\u001b[38;5;33mAdd\u001b[0m)                  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ layer_normalization_12[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ                               ‚îÇ                           ‚îÇ                 ‚îÇ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ layer_normalization_13        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m128\u001b[0m)          ‚îÇ             \u001b[38;5;34m256\u001b[0m ‚îÇ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mLayerNormalization\u001b[0m)          ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_average_pooling1d_2    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ layer_normalization_13[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      ‚îÇ                           ‚îÇ                 ‚îÇ                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ global_average_pooling1d_‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_22 (\u001b[38;5;33mDense\u001b[0m)              ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 ‚îÇ             \u001b[38;5;34m387\u001b[0m ‚îÇ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,402,435</span> (9.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,402,435\u001b[0m (9.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,402,435</span> (9.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,402,435\u001b[0m (9.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m1417/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1051s\u001b[0m 738ms/step - accuracy: 0.6337 - loss: 0.8257 - val_accuracy: 0.7998 - val_loss: 0.5525\n",
      "Epoch 2/2\n",
      "\u001b[1m1417/1417\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1105s\u001b[0m 780ms/step - accuracy: 0.8013 - loss: 0.5515 - val_accuracy: 0.7908 - val_loss: 0.5673\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 130ms/step\n",
      "Accuracy: 0.7998146431881371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.96      0.85     11152\n",
      "           1       0.00      0.00      0.00      2318\n",
      "           2       0.87      0.81      0.84      9189\n",
      "\n",
      "    accuracy                           0.80     22659\n",
      "   macro avg       0.54      0.59      0.56     22659\n",
      "weighted avg       0.73      0.80      0.76     22659\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\geoch\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming df is your dataframe and 'Content_cleaned' is the column with cleaned text data\n",
    "# 'target' is your target variable\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "vocab_size = 10000  # Example vocabulary size\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df['Content_cleaned'].values)\n",
    "sequences = tokenizer.texts_to_sequences(df['Content_cleaned'].values)\n",
    "\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transformer Parameters\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "# Positional Encoding\n",
    "def positional_encoding(seq_length, embed_dim):\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (i // 2) / embed_dim) for i in range(embed_dim)]\n",
    "        if pos != 0 else np.zeros(embed_dim) for pos in range(seq_length)])\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # 2i+1\n",
    "    return position_enc\n",
    "\n",
    "# Transformer Block\n",
    "def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "    # Attention and Normalization\n",
    "    attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attn_output = Dropout(dropout)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    \n",
    "    # Feed Forward and Normalization\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = Dropout(dropout)(ffn_output)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "# Transformer Model\n",
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "position_enc = positional_encoding(MAX_SEQUENCE_LENGTH, embed_dim)\n",
    "x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs) + position_enc\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "for _ in range(2):  # Stack 4 transformer layers\n",
    "    x = transformer_encoder(x, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(len(le.classes_), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                    validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0536426",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e5606c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\geoch\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.6/43.6 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\geoch\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.3 MB 5.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/9.3 MB 8.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.4/9.3 MB 9.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.9/9.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.5/9.3 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.1/9.3 MB 10.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.6/9.3 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.7/9.3 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.3 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.4/9.3 MB 9.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.8/9.3 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.4/9.3 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.0/9.3 MB 9.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.5/9.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.3 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.0/9.3 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.6/9.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 10.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "   ---------------------------------------- 0.0/402.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 402.6/402.6 kB 12.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp311-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 287.3/287.3 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 11.8 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "   ---------------------------------------- 0.0/177.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 177.6/177.6 kB 10.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.3.0\n",
      "    Uninstalling fsspec-2023.3.0:\n",
      "      Successfully uninstalled fsspec-2023.3.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.1.1\n",
      "    Uninstalling transformers-2.1.1:\n",
      "      Successfully uninstalled transformers-2.1.1\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.23.4 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.3.0 requires fsspec==2023.3.0, but you have fsspec 2024.6.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "647a113a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BertTokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Tokenize inputs\u001b[39;00m\n\u001b[0;32m     17\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m  \u001b[38;5;66;03m# Max length of input tokens\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m train_encodings \u001b[38;5;241m=\u001b[39m tokenizer(X_train\u001b[38;5;241m.\u001b[39mtolist(), truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[0;32m     19\u001b[0m test_encodings \u001b[38;5;241m=\u001b[39m tokenizer(X_test\u001b[38;5;241m.\u001b[39mtolist(), truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create TensorFlow datasets\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'BertTokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "X = df['Content_cleaned'].values\n",
    "y = df['Sentiment'].values\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize inputs\n",
    "max_length = 128  # Max length of input tokens\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    ")).shuffle(len(X_train)).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    ")).batch(32)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(le.classes_))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "epochs = 3  # You can adjust this\n",
    "history = model.fit(train_dataset, epochs=epochs, validation_data=test_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(test_dataset)\n",
    "y_pred_classes = np.argmax(y_pred.logits, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_classes, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09e900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

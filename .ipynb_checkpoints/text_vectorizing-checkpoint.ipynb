{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d117ad",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "## Import all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450f1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#¬†Text processing\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c59a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d42df88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \n",
       "0  plss stopp give screen limit like ur watch thi...  \n",
       "1                                               good  \n",
       "2                                  thumb up thumb up  \n",
       "3                                               good  \n",
       "4  app useful certain phone brand except phone tr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1463c41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Content             0\n",
       "Score               0\n",
       "Sentiment           0\n",
       "Content_cleaned    67\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc10d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328b593",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "This method creates literally a bag of words, without taking into account the semantic meaning of the words or their position in the sentence. First, all the inputs are tokenized. Then from all the unique tokens, the algorithm creates a vocabulary in alphabetical order. For every input sequence, the algorithm creates a matrix that has the length of the vocabulary and frequencies of each token are assigned to the corresponding index. The Bag of Words algorithm is implemented with the CountVectorizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa34b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31451\n",
      "(113292, 31451)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "      <td>(0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "      <td>(0, 27621)\\t2\\n  (0, 29212)\\t2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "      <td>(0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plss stopp give screen limit like ur watch thi...   \n",
       "1                                               good   \n",
       "2                                  thumb up thumb up   \n",
       "3                                               good   \n",
       "4  app useful certain phone brand except phone tr...   \n",
       "\n",
       "                                                 bow  \n",
       "0    (0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...  \n",
       "1                                      (0, 11478)\\t1  \n",
       "2                     (0, 27621)\\t2\\n  (0, 29212)\\t2  \n",
       "3                                      (0, 11478)\\t1  \n",
       "4    (0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the data\n",
    "bow = vectorizer.fit_transform(df['Content_cleaned'])\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(bow.shape)\n",
    "\n",
    "# Convert sparse matrix to dense matrix\n",
    "#bow_dense = bow.toarray()\n",
    "\n",
    "# Add BoW vectors as a new column in the DataFrame\n",
    "df['bow'] = list(bow)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec85ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumb up thumb up\n",
      "  (0, 27621)\t2\n",
      "  (0, 29212)\t2\n"
     ]
    }
   ],
   "source": [
    "print(df['Content_cleaned'][2])\n",
    "print(df['bow'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cb8857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27621 is thumb.\n",
      "29212 is up.\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab_keys = sorted(vectorizer.vocabulary_.keys())\n",
    "print(f\"27621 is {sorted_vocab_keys[27621]}.\")\n",
    "print(f\"29212 is {sorted_vocab_keys[29212]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90edf48",
   "metadata": {},
   "source": [
    "We notice that the produced vocabulary is of size 31451, while our bag of words has 113292 vectors, each having the size of the vocabulary. \n",
    "\n",
    "In the example we see the that both words \"thumb\" and \"up\" get value of 2.\n",
    "\n",
    "Positive: \n",
    "- Sequences have a fixed size.\n",
    "\n",
    "Negative:\n",
    "- Very high dimensions.\n",
    "- Order of words or semantic meaning is not preserved.\n",
    "- If we have a new sequence that contains new words that are not part of our vocabulary, it will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e486e",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "\n",
    "TF-IDF, or Term Frequency- Inverse Document Frequency, is an algorithm that creates a frequency-based vocabulary, like Bag of Words, but unlike that, it takes word importance into consideration. Basically, it considers that if a word is part of a lot of sentences/sequences, then it must not be very important. However, if a word is present in only a few sentences/sequences, then it must be of high importance. This way words that get repeated too often don‚Äôt overpower less frequent but important words. The formula for words in a sentence/sequence is as follows:\n",
    "- TF(x) = (frequency of word 'x' in a sequence)/(total number of words in the sequence).\n",
    "- IDF(x) = log((total number of sequences)/(number of sequences that contain word 'x')).\n",
    "- TF-IDF(x) - TF(x) * IDF(x).\n",
    "\n",
    "In IDF(x) the document frequency is inversed so the more common a word is across all documents, the lesser its importance is for the current document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6075ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31451\n",
      "(113292, 31451)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>bow</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "      <td>(0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...</td>\n",
       "      <td>(0, 8867)\\t0.18773482827989177\\n  (0, 18672)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "      <td>(0, 27621)\\t2\\n  (0, 29212)\\t2</td>\n",
       "      <td>(0, 29212)\\t0.5498571095671961\\n  (0, 27621)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "      <td>(0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...</td>\n",
       "      <td>(0, 7121)\\t0.43645434017309587\\n  (0, 31200)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plss stopp give screen limit like ur watch thi...   \n",
       "1                                               good   \n",
       "2                                  thumb up thumb up   \n",
       "3                                               good   \n",
       "4  app useful certain phone brand except phone tr...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0    (0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...   \n",
       "1                                      (0, 11478)\\t1   \n",
       "2                     (0, 27621)\\t2\\n  (0, 29212)\\t2   \n",
       "3                                      (0, 11478)\\t1   \n",
       "4    (0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...   \n",
       "\n",
       "                                               tfidf  \n",
       "0    (0, 8867)\\t0.18773482827989177\\n  (0, 18672)...  \n",
       "1                                    (0, 11478)\\t1.0  \n",
       "2    (0, 29212)\\t0.5498571095671961\\n  (0, 27621)...  \n",
       "3                                    (0, 11478)\\t1.0  \n",
       "4    (0, 7121)\\t0.43645434017309587\\n  (0, 31200)...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the model and transform the data\n",
    "tfidf = vectorizer.fit_transform(df['Content_cleaned'])\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(tfidf.shape)\n",
    "\n",
    "df['tfidf'] = [tfidf[i] for i in range(tfidf.shape[0])]\n",
    "\n",
    "# Show the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a34ef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumb up thumb up\n",
      "  (0, 29212)\t0.5498571095671961\n",
      "  (0, 27621)\t0.8352587377923134\n"
     ]
    }
   ],
   "source": [
    "print(df['Content_cleaned'][2])\n",
    "print(df['tfidf'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "722052c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27621 is thumb.\n",
      "29212 is up.\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab_keys = sorted(vectorizer.vocabulary_.keys())\n",
    "print(f\"27621 is {sorted_vocab_keys[27621]}.\")\n",
    "print(f\"29212 is {sorted_vocab_keys[29212]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15851071",
   "metadata": {},
   "source": [
    "We see that just like Bag of Words, we have a vocabulary of 31451 size and 113292 vectors of the same size.\n",
    "\n",
    "In the example we see that unlike Bag of Words, where both words got value 2, the word \"thumb\" gets a higher value than the word \"up\", meaning it is of more importance. The word \"thumb\" must exist in less sequences than the word \"up\", making it more significant.\n",
    "\n",
    "Positive: \n",
    "- Sequences have a fixed size.\n",
    "- Some word importance is considered, unlike Bag of Words.\n",
    "\n",
    "Negative:\n",
    "- Very high dimensions.\n",
    "- Order of words is still not preserved.\n",
    "- Again if we have a new sequence that contains new words that are not part of our vocabulary, it will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e24",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec is a neural network-based model for learning word embeddings. Unlike in the frequency-based vectorization algorithms, the vector representation of words was said to be contextually aware. Since every word is represented as an n-dimensional vector, one can imagine that all of the words are mapped to this n-dimensional space in such a manner that words having similar meanings exist in close proximity to one another in this hyperspace. \n",
    "\n",
    "There are two main ways to implement Word2Vec, CBoW and Skip-Gram.\n",
    "\n",
    "### CBoW\n",
    "\n",
    "In CBoW, or Continuous Bag of Words, a NN with a single hidden layer is trained. It takes as input context (vincinity) words and its goal is to predict the current word. For example if we have the sentence \"the small kid ate a banana\", if we have vincinity=2, an input to the model can be (small, kid, a, banana) and the output will be \"ate\". In this algorithm we choose a vincinity number m and then for every word in our sequences a dataset is prepared taking the m neighboring words as inputs and the word as a target. All words are turned into one-hot-encodings. Then a NN with a single layer is trained. In the end, we will not use the actual NN anywhere, but we will use the hidden-to-output weight vector as a word embeddings matrix. The size of this matrix is the size of the hidden layer and we can define it as a hyperparameter. Let's say in our case the vocabulary is of size 34326. If we choose a hidden layer of 300 size, then the word embeddings matrix will be of size 34326x300, since every word will be an one-hot vector of 1x34326 size. Then we multiply our word with the embedding matrix and we get a vector of 1x300 size, which is our final goal.\n",
    "\n",
    "### Skip-Gram\n",
    "\n",
    "Skip-Gram is the exact mirrored process of CBoW, in the sense that instead of feeding the network context words and trying to predict the current word, we feed the network the current word and it tries to predict context (vincinity) words. For example if we have the sentence \"the small kid ate a banana\", if we have vincinity=2, an input to the model can be \"ate\" and the output will be (small, kid, a, banana). Again in this algorithm we choose a vincinity number m and then for every word in our sequences a dataset is prepared taking the m neighboring words as targets and the word as input. Then a NN is trained and the input-to-hidden weights are taken as word embeddings. Then the vectorizing of our dataset is done the same way as in CBoW.\n",
    "\n",
    "### Differences\n",
    "\n",
    "Skip-Gram is better when the dataset is small and emphasis on rare words is given. CBoW is better when the dataset is bigger, can better represent frequent words and it is faster to train.\n",
    "\n",
    "\n",
    "There is the possibility to use pretrained word embeddings or train a new model ourselves. The pretrained usually used is provided by Google. In this notebook we will try both of them and see how they compare, both in vectorizing and later in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9e2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fb04abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v = models.KeyedVectors.load_word2vec_format(\n",
    "'../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c2a3d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>bow</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "      <td>(0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...</td>\n",
       "      <td>(0, 8867)\\t0.18773482827989177\\n  (0, 18672)...</td>\n",
       "      <td>[plss, stopp, give, screen, limit, like, ur, w...</td>\n",
       "      <td>[0.08365452, 0.0579847, 0.11433671, -0.0025425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "      <td>(0, 27621)\\t2\\n  (0, 29212)\\t2</td>\n",
       "      <td>(0, 29212)\\t0.5498571095671961\\n  (0, 27621)...</td>\n",
       "      <td>[thumb, up, thumb, up]</td>\n",
       "      <td>[0.08703613, 0.07147217, -0.00390625, 0.005859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "      <td>(0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...</td>\n",
       "      <td>(0, 7121)\\t0.43645434017309587\\n  (0, 31200)...</td>\n",
       "      <td>[app, useful, certain, phone, brand, except, p...</td>\n",
       "      <td>[0.0644662, -0.0806833, -0.0020926339, 0.02535...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plss stopp give screen limit like ur watch thi...   \n",
       "1                                               good   \n",
       "2                                  thumb up thumb up   \n",
       "3                                               good   \n",
       "4  app useful certain phone brand except phone tr...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0    (0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...   \n",
       "1                                      (0, 11478)\\t1   \n",
       "2                     (0, 27621)\\t2\\n  (0, 29212)\\t2   \n",
       "3                                      (0, 11478)\\t1   \n",
       "4    (0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0    (0, 8867)\\t0.18773482827989177\\n  (0, 18672)...   \n",
       "1                                    (0, 11478)\\t1.0   \n",
       "2    (0, 29212)\\t0.5498571095671961\\n  (0, 27621)...   \n",
       "3                                    (0, 11478)\\t1.0   \n",
       "4    (0, 7121)\\t0.43645434017309587\\n  (0, 31200)...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, give, screen, limit, like, ur, w...   \n",
       "1                                             [good]   \n",
       "2                             [thumb, up, thumb, up]   \n",
       "3                                             [good]   \n",
       "4  [app, useful, certain, phone, brand, except, p...   \n",
       "\n",
       "                                 word2vec_pretrained  \n",
       "0  [0.08365452, 0.0579847, 0.11433671, -0.0025425...  \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...  \n",
       "2  [0.08703613, 0.07147217, -0.00390625, 0.005859...  \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...  \n",
       "4  [0.0644662, -0.0806833, -0.0020926339, 0.02535...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_word2vec(tokens_list, model, vector_size):\n",
    "    \"\"\"\n",
    "    This function computes the average Word2Vec for a given list of tokens.\n",
    "    \"\"\"\n",
    "    # Filter the tokens that are present in the Word2Vec model\n",
    "    valid_tokens = [token for token in tokens_list if token in model]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Compute the average Word2Vec\n",
    "    word_vectors = [model[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['Content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Compute the average Word2Vec for each row\n",
    "vector_size = w2v.vector_size\n",
    "df['word2vec_pretrained'] = df['tokens'].apply(lambda x: get_average_word2vec(x, w2v, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac02e9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at cbow.model\n",
      "Model saved at skipgram.model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>bow</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>word2vec_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "      <td>(0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...</td>\n",
       "      <td>(0, 8867)\\t0.18773482827989177\\n  (0, 18672)...</td>\n",
       "      <td>[plss, stopp, give, screen, limit, like, ur, w...</td>\n",
       "      <td>[0.08365452, 0.0579847, 0.11433671, -0.0025425...</td>\n",
       "      <td>[0.20826791, 0.0046617766, -0.04649164, -0.117...</td>\n",
       "      <td>[0.1006552, 0.2239003, -0.07527001, -0.1536161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.1703951, -0.9856324, -0.18964884, -0.521374...</td>\n",
       "      <td>[0.20965211, -0.06506327, -0.08436263, 0.04491...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "      <td>(0, 27621)\\t2\\n  (0, 29212)\\t2</td>\n",
       "      <td>(0, 29212)\\t0.5498571095671961\\n  (0, 27621)...</td>\n",
       "      <td>[thumb, up, thumb, up]</td>\n",
       "      <td>[0.08703613, 0.07147217, -0.00390625, 0.005859...</td>\n",
       "      <td>[0.12779036, 0.83761936, -0.8175876, 0.9235396...</td>\n",
       "      <td>[-0.019263022, 0.4243198, -0.3776014, -0.08818...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.1703951, -0.9856324, -0.18964884, -0.521374...</td>\n",
       "      <td>[0.20965211, -0.06506327, -0.08436263, 0.04491...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "      <td>(0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...</td>\n",
       "      <td>(0, 7121)\\t0.43645434017309587\\n  (0, 31200)...</td>\n",
       "      <td>[app, useful, certain, phone, brand, except, p...</td>\n",
       "      <td>[0.0644662, -0.0806833, -0.0020926339, 0.02535...</td>\n",
       "      <td>[-0.16519807, 0.1276928, 0.13958092, -0.040613...</td>\n",
       "      <td>[0.06709646, 0.07583677, -0.0061207726, -0.034...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plss stopp give screen limit like ur watch thi...   \n",
       "1                                               good   \n",
       "2                                  thumb up thumb up   \n",
       "3                                               good   \n",
       "4  app useful certain phone brand except phone tr...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0    (0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...   \n",
       "1                                      (0, 11478)\\t1   \n",
       "2                     (0, 27621)\\t2\\n  (0, 29212)\\t2   \n",
       "3                                      (0, 11478)\\t1   \n",
       "4    (0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0    (0, 8867)\\t0.18773482827989177\\n  (0, 18672)...   \n",
       "1                                    (0, 11478)\\t1.0   \n",
       "2    (0, 29212)\\t0.5498571095671961\\n  (0, 27621)...   \n",
       "3                                    (0, 11478)\\t1.0   \n",
       "4    (0, 7121)\\t0.43645434017309587\\n  (0, 31200)...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, give, screen, limit, like, ur, w...   \n",
       "1                                             [good]   \n",
       "2                             [thumb, up, thumb, up]   \n",
       "3                                             [good]   \n",
       "4  [app, useful, certain, phone, brand, except, p...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.08365452, 0.0579847, 0.11433671, -0.0025425...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.08703613, 0.07147217, -0.00390625, 0.005859...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.0644662, -0.0806833, -0.0020926339, 0.02535...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [0.20826791, 0.0046617766, -0.04649164, -0.117...   \n",
       "1  [0.1703951, -0.9856324, -0.18964884, -0.521374...   \n",
       "2  [0.12779036, 0.83761936, -0.8175876, 0.9235396...   \n",
       "3  [0.1703951, -0.9856324, -0.18964884, -0.521374...   \n",
       "4  [-0.16519807, 0.1276928, 0.13958092, -0.040613...   \n",
       "\n",
       "                                   word2vec_skipgram  \n",
       "0  [0.1006552, 0.2239003, -0.07527001, -0.1536161...  \n",
       "1  [0.20965211, -0.06506327, -0.08436263, 0.04491...  \n",
       "2  [-0.019263022, 0.4243198, -0.3776014, -0.08818...  \n",
       "3  [0.20965211, -0.06506327, -0.08436263, 0.04491...  \n",
       "4  [0.06709646, 0.07583677, -0.0061207726, -0.034...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def get_average_word2vec2(tokens_list, model, vector_size):\n",
    "    valid_tokens = [token for token in tokens_list if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    word_vectors = [model.wv[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define model parameters\n",
    "vector_size = 300   # Dimensionality of the word vectors\n",
    "window_size = 5     # Context window size\n",
    "min_count = 1       # Minimum word frequency\n",
    "workers = multiprocessing.cpu_count()  # Number of worker threads to use\n",
    "\n",
    "# Train the Word2Vec model\n",
    "cbow = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=0, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "# Save the model\n",
    "model_path = \"cbow.model\"\n",
    "cbow.save(model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n",
    "df['word2vec_cbow'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, cbow, vector_size))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "skipgram = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=1, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "# Save the model\n",
    "model_path = \"skipgram.model\"\n",
    "skipgram.save(model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n",
    "df['word2vec_skipgram'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, skipgram, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26de5999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.7696489691734314), ('stuff', 0.6302829384803772), ('show', 0.5975726246833801), ('drama', 0.5592541694641113), ('genre', 0.5579929947853088), ('category', 0.5558158755302429), ('series', 0.5527818202972412), ('program', 0.5504778027534485), ('programme', 0.5363008379936218), ('kdrama', 0.5327799916267395)]\n"
     ]
    }
   ],
   "source": [
    "print(cbow.wv.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a09ea8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('flim', 0.8157294988632202), ('kdramas', 0.79478919506073), ('flick', 0.7657655477523804), ('oldie', 0.7626849412918091), ('sitcom', 0.7523657083511353), ('limitless', 0.7519016861915588), ('trendy', 0.7518705725669861), ('rerun', 0.7517079710960388), ('spectacular', 0.7492549419403076), ('binging', 0.7458791136741638)]\n"
     ]
    }
   ],
   "source": [
    "print(skipgram.wv.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "304bc8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.8676770329475403), ('movies', 0.8013108372688293), ('films', 0.7363011837005615), ('moive', 0.6830360889434814), ('Movie', 0.6693680286407471), ('horror_flick', 0.6577848792076111), ('sequel', 0.6577793955802917), ('Guy_Ritchie_Revolver', 0.650975227355957), ('romantic_comedy', 0.6413198709487915), ('flick', 0.6321909427642822)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f24a0f",
   "metadata": {},
   "source": [
    "When calculating the vectors of a sequence, we calculate the word embedding of every token seperately and then we average over them to get the sequence vector. This way the overall semantic meaning of the text is captured, while each word contributes to the final vector, allowing the resulting vector to represent the combined meanings of the individual words.\n",
    "\n",
    "In the above example of the word \"movie\" we see what similar words the 3 different models are giving. For all 3 of the different embeddings, we see similar words, however CBoW seems to be better than Skip-Gram. We can notice that CBoW gives more frequent and correct words, while Skip-Gram might be giving more rare words. For this reason we will continue with the pretrained version and CBoW.\n",
    "\n",
    "Positive:\n",
    "- Sequences have a fixed size.\n",
    "- The dimensionality is exceptionally reduced, compared to the frequency-based methods.\n",
    "- Semantics and context are the base element of the vectors.\n",
    "\n",
    "Negative:\n",
    "- Order of words is still not preserved.\n",
    "- Again we work on a finite vocabulary, based on the corpus our model was trained on. If we have a new sequence with new words, they will not be taken into consideration, possibly losing important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "065ef3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['word2vec_skipgram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854faccd",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "\n",
    "GloVe, or Global Vectors, is an algorithm that like Word2Vec is creating contextual embeddings. The difference is that while Word2Vec only considers local information according to the surroundings of a word, GloVe captures both local and global statistics. It relies in the word-word co-occurrence probabilities with a set window-size in our dataset. Again like Word2Vec a NN is trained and the weight matrix is taken as the word embedding matrix.\n",
    "\n",
    "Usually pre-trained GloVe embeddings are used, which are trained on huge datasets. In this notebook we will use the **GloVe 6b**, which is trained on Wikipedia and Gigaword on 6 billion tokens and 400K vocabulary size, and the **GloVe Twitter**, which is trained with 2 billion tweets, 27 billion tokens and 1.2 million vocabulary size. Both embeddings come in 50,100,200 vector sizes. We will use the 100d embeddings to redude dimensionality. We use the first version as a general-purpose embedding and the second version as a more specified embedding, since tweets and netflix reviews can be similar and contain slang and informal language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14f30540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors from GloVe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>bow</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>glove_6B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "      <td>(0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...</td>\n",
       "      <td>(0, 8867)\\t0.18773482827989177\\n  (0, 18672)...</td>\n",
       "      <td>[plss, stopp, give, screen, limit, like, ur, w...</td>\n",
       "      <td>[0.08365452, 0.0579847, 0.11433671, -0.0025425...</td>\n",
       "      <td>[0.20826791, 0.0046617766, -0.04649164, -0.117...</td>\n",
       "      <td>[-0.1198448, 0.12636456, 0.41294017, -0.217294...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.1703951, -0.9856324, -0.18964884, -0.521374...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "      <td>(0, 27621)\\t2\\n  (0, 29212)\\t2</td>\n",
       "      <td>(0, 29212)\\t0.5498571095671961\\n  (0, 27621)...</td>\n",
       "      <td>[thumb, up, thumb, up]</td>\n",
       "      <td>[0.08703613, 0.07147217, -0.00390625, 0.005859...</td>\n",
       "      <td>[0.12779036, 0.83761936, -0.8175876, 0.9235396...</td>\n",
       "      <td>[-0.22568002, 0.342005, 0.248815, -0.577975, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.1703951, -0.9856324, -0.18964884, -0.521374...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "      <td>(0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...</td>\n",
       "      <td>(0, 7121)\\t0.43645434017309587\\n  (0, 31200)...</td>\n",
       "      <td>[app, useful, certain, phone, brand, except, p...</td>\n",
       "      <td>[0.0644662, -0.0806833, -0.0020926339, 0.02535...</td>\n",
       "      <td>[-0.16519807, 0.1276928, 0.13958092, -0.040613...</td>\n",
       "      <td>[-0.21259494, -0.062381856, 0.21229614, 0.0178...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plss stopp give screen limit like ur watch thi...   \n",
       "1                                               good   \n",
       "2                                  thumb up thumb up   \n",
       "3                                               good   \n",
       "4  app useful certain phone brand except phone tr...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0    (0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...   \n",
       "1                                      (0, 11478)\\t1   \n",
       "2                     (0, 27621)\\t2\\n  (0, 29212)\\t2   \n",
       "3                                      (0, 11478)\\t1   \n",
       "4    (0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0    (0, 8867)\\t0.18773482827989177\\n  (0, 18672)...   \n",
       "1                                    (0, 11478)\\t1.0   \n",
       "2    (0, 29212)\\t0.5498571095671961\\n  (0, 27621)...   \n",
       "3                                    (0, 11478)\\t1.0   \n",
       "4    (0, 7121)\\t0.43645434017309587\\n  (0, 31200)...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, give, screen, limit, like, ur, w...   \n",
       "1                                             [good]   \n",
       "2                             [thumb, up, thumb, up]   \n",
       "3                                             [good]   \n",
       "4  [app, useful, certain, phone, brand, except, p...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.08365452, 0.0579847, 0.11433671, -0.0025425...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.08703613, 0.07147217, -0.00390625, 0.005859...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.0644662, -0.0806833, -0.0020926339, 0.02535...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [0.20826791, 0.0046617766, -0.04649164, -0.117...   \n",
       "1  [0.1703951, -0.9856324, -0.18964884, -0.521374...   \n",
       "2  [0.12779036, 0.83761936, -0.8175876, 0.9235396...   \n",
       "3  [0.1703951, -0.9856324, -0.18964884, -0.521374...   \n",
       "4  [-0.16519807, 0.1276928, 0.13958092, -0.040613...   \n",
       "\n",
       "                                            glove_6B  \n",
       "0  [-0.1198448, 0.12636456, 0.41294017, -0.217294...  \n",
       "1  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...  \n",
       "2  [-0.22568002, 0.342005, 0.248815, -0.577975, -...  \n",
       "3  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...  \n",
       "4  [-0.21259494, -0.062381856, 0.21229614, 0.0178...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GloVe embeddings file\n",
    "glove_file = '../glove.6B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_6b = load_glove_embeddings(glove_file)\n",
    "print(f\"Loaded {len(glove_6b)} word vectors from GloVe.\")\n",
    "\n",
    "# Define a function to get the average GloVe vector for a list of tokens\n",
    "def get_average_glove(tokens_list, embeddings, embedding_dim):\n",
    "    valid_tokens = [token for token in tokens_list if token in embeddings]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(embedding_dim)\n",
    "    word_vectors = [embeddings[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define the embedding dimension (e.g., 100 for 'glove.6B.100d.txt')\n",
    "embedding_dim = 100\n",
    "\n",
    "# Compute the average GloVe vector for each row\n",
    "df['glove_6B'] = df['tokens'].apply(lambda x: get_average_glove(x, glove_6b, embedding_dim))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ede84150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors from GloVe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>bow</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>glove_6B</th>\n",
       "      <th>glove_twitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "      <td>(0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...</td>\n",
       "      <td>(0, 8867)\\t0.18773482827989177\\n  (0, 18672)...</td>\n",
       "      <td>[plss, stopp, give, screen, limit, like, ur, w...</td>\n",
       "      <td>[0.08365452, 0.0579847, 0.11433671, -0.0025425...</td>\n",
       "      <td>[0.20826791, 0.0046617766, -0.04649164, -0.117...</td>\n",
       "      <td>[-0.1198448, 0.12636456, 0.41294017, -0.217294...</td>\n",
       "      <td>[0.123258926, 0.09078163, -0.101420276, 0.2712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.1703951, -0.9856324, -0.18964884, -0.521374...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "      <td>[0.091552, 0.093336, -0.028113, 0.3699, 0.1895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "      <td>(0, 27621)\\t2\\n  (0, 29212)\\t2</td>\n",
       "      <td>(0, 29212)\\t0.5498571095671961\\n  (0, 27621)...</td>\n",
       "      <td>[thumb, up, thumb, up]</td>\n",
       "      <td>[0.08703613, 0.07147217, -0.00390625, 0.005859...</td>\n",
       "      <td>[0.12779036, 0.83761936, -0.8175876, 0.9235396...</td>\n",
       "      <td>[-0.22568002, 0.342005, 0.248815, -0.577975, -...</td>\n",
       "      <td>[0.26894343, -0.28983998, 0.164455, -0.166473,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.1703951, -0.9856324, -0.18964884, -0.521374...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "      <td>[0.091552, 0.093336, -0.028113, 0.3699, 0.1895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "      <td>(0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...</td>\n",
       "      <td>(0, 7121)\\t0.43645434017309587\\n  (0, 31200)...</td>\n",
       "      <td>[app, useful, certain, phone, brand, except, p...</td>\n",
       "      <td>[0.0644662, -0.0806833, -0.0020926339, 0.02535...</td>\n",
       "      <td>[-0.16519807, 0.1276928, 0.13958092, -0.040613...</td>\n",
       "      <td>[-0.21259494, -0.062381856, 0.21229614, 0.0178...</td>\n",
       "      <td>[0.30852813, 0.06642222, -0.07303124, 0.210921...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plss stopp give screen limit like ur watch thi...   \n",
       "1                                               good   \n",
       "2                                  thumb up thumb up   \n",
       "3                                               good   \n",
       "4  app useful certain phone brand except phone tr...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0    (0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...   \n",
       "1                                      (0, 11478)\\t1   \n",
       "2                     (0, 27621)\\t2\\n  (0, 29212)\\t2   \n",
       "3                                      (0, 11478)\\t1   \n",
       "4    (0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0    (0, 8867)\\t0.18773482827989177\\n  (0, 18672)...   \n",
       "1                                    (0, 11478)\\t1.0   \n",
       "2    (0, 29212)\\t0.5498571095671961\\n  (0, 27621)...   \n",
       "3                                    (0, 11478)\\t1.0   \n",
       "4    (0, 7121)\\t0.43645434017309587\\n  (0, 31200)...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, give, screen, limit, like, ur, w...   \n",
       "1                                             [good]   \n",
       "2                             [thumb, up, thumb, up]   \n",
       "3                                             [good]   \n",
       "4  [app, useful, certain, phone, brand, except, p...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.08365452, 0.0579847, 0.11433671, -0.0025425...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.08703613, 0.07147217, -0.00390625, 0.005859...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.0644662, -0.0806833, -0.0020926339, 0.02535...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [0.20826791, 0.0046617766, -0.04649164, -0.117...   \n",
       "1  [0.1703951, -0.9856324, -0.18964884, -0.521374...   \n",
       "2  [0.12779036, 0.83761936, -0.8175876, 0.9235396...   \n",
       "3  [0.1703951, -0.9856324, -0.18964884, -0.521374...   \n",
       "4  [-0.16519807, 0.1276928, 0.13958092, -0.040613...   \n",
       "\n",
       "                                            glove_6B  \\\n",
       "0  [-0.1198448, 0.12636456, 0.41294017, -0.217294...   \n",
       "1  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...   \n",
       "2  [-0.22568002, 0.342005, 0.248815, -0.577975, -...   \n",
       "3  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...   \n",
       "4  [-0.21259494, -0.062381856, 0.21229614, 0.0178...   \n",
       "\n",
       "                                       glove_twitter  \n",
       "0  [0.123258926, 0.09078163, -0.101420276, 0.2712...  \n",
       "1  [0.091552, 0.093336, -0.028113, 0.3699, 0.1895...  \n",
       "2  [0.26894343, -0.28983998, 0.164455, -0.166473,...  \n",
       "3  [0.091552, 0.093336, -0.028113, 0.3699, 0.1895...  \n",
       "4  [0.30852813, 0.06642222, -0.07303124, 0.210921...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GloVe embeddings file\n",
    "glove_file = '../glove.twitter.27B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_twitter = load_glove_embeddings(glove_file)\n",
    "print(f\"Loaded {len(glove_twitter)} word vectors from GloVe.\")\n",
    "\n",
    "# Define a function to get the average GloVe vector for a list of tokens\n",
    "def get_average_glove(tokens_list, embeddings, embedding_dim):\n",
    "    valid_tokens = [token for token in tokens_list if token in embeddings]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(embedding_dim)\n",
    "    word_vectors = [embeddings[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define the embedding dimension (e.g., 100 for 'glove.6B.100d.txt')\n",
    "embedding_dim = 100\n",
    "\n",
    "# Compute the average GloVe vector for each row\n",
    "df['glove_twitter'] = df['tokens'].apply(lambda x: get_average_glove(x, glove_twitter, embedding_dim))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7502c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'movie' in GloVe 6B: ['film', 'movies', 'films', 'hollywood', 'comedy']\n",
      "Words similar to 'movie' in GloVe Twitter: ['movies', 'episode', 'story', 'trailer', 'watching']\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def find_closest_embeddings(embeddings_dict, word, top_n=5):\n",
    "    if word not in embeddings_dict:\n",
    "        print(f\"Word '{word}' not found in the embedding dictionary.\")\n",
    "        return []\n",
    "    \n",
    "    embedding = embeddings_dict[word]\n",
    "    closest_words = sorted(\n",
    "        embeddings_dict.keys(), \n",
    "        key=lambda w: spatial.distance.cosine(embeddings_dict[w], embedding) if len(embeddings_dict[w]) == len(embedding) else float('inf')\n",
    "    )\n",
    "    closest_words.remove(word)  # Remove the word itself from the results\n",
    "    return closest_words[:top_n]\n",
    "\n",
    "\n",
    "similar_words_6b = find_closest_embeddings(glove_6b, 'movie')\n",
    "print(f\"Words similar to 'movie' in GloVe 6B: {similar_words_6b}\")\n",
    "\n",
    "# Test with GloVe Twitter\n",
    "similar_words_twitter = find_closest_embeddings(glove_twitter, 'movie')\n",
    "print(f\"Words similar to 'movie' in GloVe Twitter: {similar_words_twitter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf8eaa",
   "metadata": {},
   "source": [
    "When calculating the vectors of a sequence, we calculate the word embedding of every token seperately and then we average over them to get the sequence vector. This way the overall semantic meaning of the text is captured, while each word contributes to the final vector, allowing the resulting vector to represent the combined meanings of the individual words.\n",
    "\n",
    "In the above example of the word \"movie\" we see what similar words the 2 different embeddings are giving. For both of the embeddings, we see similar words.\n",
    "\n",
    "Positive:\n",
    "- Sequences have a fixed size.\n",
    "- The dimensionality is exceptionally reduced, compared to the frequency-based methods.\n",
    "- Semantics and context, both local and global, are the base element of the vectors.\n",
    "- Big range of embedding sizes.\n",
    "\n",
    "Negative:\n",
    "- Order of words is still not preserved.\n",
    "- Again we work on a finite vocabulary, based on the corpus our model was trained on. If we have a new sequence with new words, they will not be taken into consideration, possibly losing important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d1fb0",
   "metadata": {},
   "source": [
    "Finally, we drop the tokens column from our dataset and we have the dataframe with all the different vectorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94805051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>bow</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>glove_6B</th>\n",
       "      <th>glove_twitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "      <td>(0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...</td>\n",
       "      <td>(0, 8867)\\t0.18773482827989177\\n  (0, 18672)...</td>\n",
       "      <td>[0.08365452, 0.0579847, 0.11433671, -0.0025425...</td>\n",
       "      <td>[0.20826791, 0.0046617766, -0.04649164, -0.117...</td>\n",
       "      <td>[-0.1198448, 0.12636456, 0.41294017, -0.217294...</td>\n",
       "      <td>[0.123258926, 0.09078163, -0.101420276, 0.2712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.1703951, -0.9856324, -0.18964884, -0.521374...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "      <td>[0.091552, 0.093336, -0.028113, 0.3699, 0.1895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "      <td>(0, 27621)\\t2\\n  (0, 29212)\\t2</td>\n",
       "      <td>(0, 29212)\\t0.5498571095671961\\n  (0, 27621)...</td>\n",
       "      <td>[0.08703613, 0.07147217, -0.00390625, 0.005859...</td>\n",
       "      <td>[0.12779036, 0.83761936, -0.8175876, 0.9235396...</td>\n",
       "      <td>[-0.22568002, 0.342005, 0.248815, -0.577975, -...</td>\n",
       "      <td>[0.26894343, -0.28983998, 0.164455, -0.166473,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>(0, 11478)\\t1</td>\n",
       "      <td>(0, 11478)\\t1.0</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.1703951, -0.9856324, -0.18964884, -0.521374...</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "      <td>[0.091552, 0.093336, -0.028113, 0.3699, 0.1895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "      <td>(0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...</td>\n",
       "      <td>(0, 7121)\\t0.43645434017309587\\n  (0, 31200)...</td>\n",
       "      <td>[0.0644662, -0.0806833, -0.0020926339, 0.02535...</td>\n",
       "      <td>[-0.16519807, 0.1276928, 0.13958092, -0.040613...</td>\n",
       "      <td>[-0.21259494, -0.062381856, 0.21229614, 0.0178...</td>\n",
       "      <td>[0.30852813, 0.06642222, -0.07303124, 0.210921...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plss stopp give screen limit like ur watch thi...   \n",
       "1                                               good   \n",
       "2                                  thumb up thumb up   \n",
       "3                                               good   \n",
       "4  app useful certain phone brand except phone tr...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0    (0, 20804)\\t1\\n  (0, 26026)\\t1\\n  (0, 11312)...   \n",
       "1                                      (0, 11478)\\t1   \n",
       "2                     (0, 27621)\\t2\\n  (0, 29212)\\t2   \n",
       "3                                      (0, 11478)\\t1   \n",
       "4    (0, 1571)\\t1\\n  (0, 29353)\\t1\\n  (0, 4536)\\t...   \n",
       "\n",
       "                                               tfidf  \\\n",
       "0    (0, 8867)\\t0.18773482827989177\\n  (0, 18672)...   \n",
       "1                                    (0, 11478)\\t1.0   \n",
       "2    (0, 29212)\\t0.5498571095671961\\n  (0, 27621)...   \n",
       "3                                    (0, 11478)\\t1.0   \n",
       "4    (0, 7121)\\t0.43645434017309587\\n  (0, 31200)...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.08365452, 0.0579847, 0.11433671, -0.0025425...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.08703613, 0.07147217, -0.00390625, 0.005859...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.0644662, -0.0806833, -0.0020926339, 0.02535...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [0.20826791, 0.0046617766, -0.04649164, -0.117...   \n",
       "1  [0.1703951, -0.9856324, -0.18964884, -0.521374...   \n",
       "2  [0.12779036, 0.83761936, -0.8175876, 0.9235396...   \n",
       "3  [0.1703951, -0.9856324, -0.18964884, -0.521374...   \n",
       "4  [-0.16519807, 0.1276928, 0.13958092, -0.040613...   \n",
       "\n",
       "                                            glove_6B  \\\n",
       "0  [-0.1198448, 0.12636456, 0.41294017, -0.217294...   \n",
       "1  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...   \n",
       "2  [-0.22568002, 0.342005, 0.248815, -0.577975, -...   \n",
       "3  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...   \n",
       "4  [-0.21259494, -0.062381856, 0.21229614, 0.0178...   \n",
       "\n",
       "                                       glove_twitter  \n",
       "0  [0.123258926, 0.09078163, -0.101420276, 0.2712...  \n",
       "1  [0.091552, 0.093336, -0.028113, 0.3699, 0.1895...  \n",
       "2  [0.26894343, -0.28983998, 0.164455, -0.166473,...  \n",
       "3  [0.091552, 0.093336, -0.028113, 0.3699, 0.1895...  \n",
       "4  [0.30852813, 0.06642222, -0.07303124, 0.210921...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['tokens'])\n",
    "df.head()\n",
    "#df.to_csv('vectorized_text.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d117ad",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "## Import all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450f1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#¬†Text processing\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c59a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d42df88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \n",
       "0  plss stopp give screen limit like ur watch thi...  \n",
       "1                                               good  \n",
       "2                                  thumb up thumb up  \n",
       "3                                               good  \n",
       "4  app useful certain phone brand except phone tr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1463c41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Content             0\n",
       "Score               0\n",
       "Sentiment           0\n",
       "Content_cleaned    67\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc10d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328b593",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "This method creates literally a bag of words, without taking into account the semantic meaning of the words or their position in the sentence. First, all the inputs are tokenized. Then from all the unique tokens, the algorithm creates a vocabulary in alphabetical order. For every input sequence, the algorithm creates a matrix that has the length of the vocabulary and frequencies of each token are assigned to the corresponding index. The Bag of Words algorithm is implemented with the CountVectorizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa34b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31451\n",
      "(113292, 31451)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the data\n",
    "bow = vectorizer.fit_transform(df['Content_cleaned'])\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec85ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumb up thumb up\n",
      "  (0, 27621)\t2\n",
      "  (0, 29212)\t2\n"
     ]
    }
   ],
   "source": [
    "print(df['Content_cleaned'][2])\n",
    "print(bow[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61cb8857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27621 is thumb.\n",
      "29212 is up.\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab_keys = sorted(vectorizer.vocabulary_.keys())\n",
    "print(f\"27621 is {sorted_vocab_keys[27621]}.\")\n",
    "print(f\"29212 is {sorted_vocab_keys[29212]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90edf48",
   "metadata": {},
   "source": [
    "We notice that the produced vocabulary is of size 31451, while our bag of words has 113292 vectors, each having the size of the vocabulary. \n",
    "\n",
    "In the example we see the that both words \"thumb\" and \"up\" get value of 2.\n",
    "\n",
    "Positive: \n",
    "- Sequences have a fixed size.\n",
    "\n",
    "Negative:\n",
    "- Very high dimensions.\n",
    "- Order of words or semantic meaning is not preserved.\n",
    "- If we have a new sequence that contains new words that are not part of our vocabulary, it will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e486e",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "\n",
    "TF-IDF, or Term Frequency- Inverse Document Frequency, is an algorithm that creates a frequency-based vocabulary, like Bag of Words, but unlike that, it takes word importance into consideration. Basically, it considers that if a word is part of a lot of sentences/sequences, then it must not be very important. However, if a word is present in only a few sentences/sequences, then it must be of high importance. This way words that get repeated too often don‚Äôt overpower less frequent but important words. The formula for words in a sentence/sequence is as follows:\n",
    "- TF(x) = (frequency of word 'x' in a sequence)/(total number of words in the sequence).\n",
    "- IDF(x) = log((total number of sequences)/(number of sequences that contain word 'x')).\n",
    "- TF-IDF(x) - TF(x) * IDF(x).\n",
    "\n",
    "In IDF(x) the document frequency is inversed so the more common a word is across all documents, the lesser its importance is for the current document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df6075ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31451\n",
      "(113292, 31451)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the model and transform the data\n",
    "tfidf = vectorizer.fit_transform(df['Content_cleaned'])\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a34ef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumb up thumb up\n",
      "  (0, 29212)\t0.5498571095671961\n",
      "  (0, 27621)\t0.8352587377923134\n"
     ]
    }
   ],
   "source": [
    "print(df['Content_cleaned'][2])\n",
    "print(tfidf[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722052c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27621 is thumb.\n",
      "29212 is up.\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab_keys = sorted(vectorizer.vocabulary_.keys())\n",
    "print(f\"27621 is {sorted_vocab_keys[27621]}.\")\n",
    "print(f\"29212 is {sorted_vocab_keys[29212]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15851071",
   "metadata": {},
   "source": [
    "We see that just like Bag of Words, we have a vocabulary of 31451 size and 113292 vectors of the same size.\n",
    "\n",
    "In the example we see that unlike Bag of Words, where both words got value 2, the word \"thumb\" gets a higher value than the word \"up\", meaning it is of more importance. The word \"thumb\" must exist in less sequences than the word \"up\", making it more significant.\n",
    "\n",
    "Positive: \n",
    "- Sequences have a fixed size.\n",
    "- Some word importance is considered, unlike Bag of Words.\n",
    "\n",
    "Negative:\n",
    "- Very high dimensions.\n",
    "- Order of words is still not preserved.\n",
    "- Again if we have a new sequence that contains new words that are not part of our vocabulary, it will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e24",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec is a neural network-based model for learning word embeddings. Unlike in the frequency-based vectorization algorithms, the vector representation of words was said to be contextually aware. Since every word is represented as an n-dimensional vector, one can imagine that all of the words are mapped to this n-dimensional space in such a manner that words having similar meanings exist in close proximity to one another in this hyperspace. \n",
    "\n",
    "There are two main ways to implement Word2Vec, CBoW and Skip-Gram.\n",
    "\n",
    "### CBoW\n",
    "\n",
    "In CBoW, or Continuous Bag of Words, a NN with a single hidden layer is trained. It takes as input context (vincinity) words and its goal is to predict the current word. For example if we have the sentence \"the small kid ate a banana\", if we have vincinity=2, an input to the model can be (small, kid, a, banana) and the output will be \"ate\". In this algorithm we choose a vincinity number m and then for every word in our sequences a dataset is prepared taking the m neighboring words as inputs and the word as a target. All words are turned into one-hot-encodings. Then a NN with a single layer is trained. In the end, we will not use the actual NN anywhere, but we will use the hidden-to-output weight vector as a word embeddings matrix. The size of this matrix is the size of the hidden layer and we can define it as a hyperparameter. Let's say in our case the vocabulary is of size 34326. If we choose a hidden layer of 300 size, then the word embeddings matrix will be of size 34326x300, since every word will be an one-hot vector of 1x34326 size. Then we multiply our word with the embedding matrix and we get a vector of 1x300 size, which is our final goal.\n",
    "\n",
    "### Skip-Gram\n",
    "\n",
    "Skip-Gram is the exact mirrored process of CBoW, in the sense that instead of feeding the network context words and trying to predict the current word, we feed the network the current word and it tries to predict context (vincinity) words. For example if we have the sentence \"the small kid ate a banana\", if we have vincinity=2, an input to the model can be \"ate\" and the output will be (small, kid, a, banana). Again in this algorithm we choose a vincinity number m and then for every word in our sequences a dataset is prepared taking the m neighboring words as targets and the word as input. Then a NN is trained and the input-to-hidden weights are taken as word embeddings. Then the vectorizing of our dataset is done the same way as in CBoW.\n",
    "\n",
    "### Differences\n",
    "\n",
    "Skip-Gram is better when the dataset is small and emphasis on rare words is given. CBoW is better when the dataset is bigger, can better represent frequent words and it is faster to train.\n",
    "\n",
    "\n",
    "There is the possibility to use pretrained word embeddings or train a new model ourselves. The pretrained usually used is provided by Google. In this notebook we will try both of them and see how they compare, both in vectorizing and later in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b42c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "916a0bcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v = models.KeyedVectors.load_word2vec_format(\n",
    "'../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4e5ffe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "      <td>[plss, stopp, give, screen, limit, like, ur, w...</td>\n",
       "      <td>[0.08365452, 0.0579847, 0.11433671, -0.0025425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "      <td>[thumb, up, thumb, up]</td>\n",
       "      <td>[0.08703613, 0.07147217, -0.00390625, 0.005859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "      <td>[app, useful, certain, phone, brand, except, p...</td>\n",
       "      <td>[0.0644662, -0.0806833, -0.0020926339, 0.02535...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plss stopp give screen limit like ur watch thi...   \n",
       "1                                               good   \n",
       "2                                  thumb up thumb up   \n",
       "3                                               good   \n",
       "4  app useful certain phone brand except phone tr...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, give, screen, limit, like, ur, w...   \n",
       "1                                             [good]   \n",
       "2                             [thumb, up, thumb, up]   \n",
       "3                                             [good]   \n",
       "4  [app, useful, certain, phone, brand, except, p...   \n",
       "\n",
       "                                 word2vec_pretrained  \n",
       "0  [0.08365452, 0.0579847, 0.11433671, -0.0025425...  \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...  \n",
       "2  [0.08703613, 0.07147217, -0.00390625, 0.005859...  \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...  \n",
       "4  [0.0644662, -0.0806833, -0.0020926339, 0.02535...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_word2vec(tokens_list, model, vector_size):\n",
    "    \"\"\"\n",
    "    This function computes the average Word2Vec for a given list of tokens.\n",
    "    \"\"\"\n",
    "    # Filter the tokens that are present in the Word2Vec model\n",
    "    valid_tokens = [token for token in tokens_list if token in model]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Compute the average Word2Vec\n",
    "    word_vectors = [model[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['Content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Compute the average Word2Vec for each row\n",
    "vector_size = w2v.vector_size\n",
    "df['word2vec_pretrained'] = df['tokens'].apply(lambda x: get_average_word2vec(x, w2v, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "394d18cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at cbow.model\n",
      "Model saved at skipgram.model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>word2vec_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp give screen limit like ur watch thi...</td>\n",
       "      <td>[plss, stopp, give, screen, limit, like, ur, w...</td>\n",
       "      <td>[0.08365452, 0.0579847, 0.11433671, -0.0025425...</td>\n",
       "      <td>[0.12165112, 0.20407064, -0.18353447, -0.23372...</td>\n",
       "      <td>[0.124642536, 0.02039718, -0.061326027, -0.039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.025911188, -0.83119565, -0.03553019, -0.544...</td>\n",
       "      <td>[0.11477529, -0.09182124, -0.14298776, -0.2736...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "      <td>[thumb, up, thumb, up]</td>\n",
       "      <td>[0.08703613, 0.07147217, -0.00390625, 0.005859...</td>\n",
       "      <td>[-0.16716677, -0.093416005, -1.4007342, 0.4197...</td>\n",
       "      <td>[-0.043917134, 0.13976072, -0.39437324, -0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.025911188, -0.83119565, -0.03553019, -0.544...</td>\n",
       "      <td>[0.11477529, -0.09182124, -0.14298776, -0.2736...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "      <td>[app, useful, certain, phone, brand, except, p...</td>\n",
       "      <td>[0.0644662, -0.0806833, -0.0020926339, 0.02535...</td>\n",
       "      <td>[-0.16657814, 0.06822517, 0.1050694, -0.113447...</td>\n",
       "      <td>[0.16508076, 0.045392197, -0.108809195, -0.125...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \\\n",
       "0  plss stopp give screen limit like ur watch thi...   \n",
       "1                                               good   \n",
       "2                                  thumb up thumb up   \n",
       "3                                               good   \n",
       "4  app useful certain phone brand except phone tr...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, give, screen, limit, like, ur, w...   \n",
       "1                                             [good]   \n",
       "2                             [thumb, up, thumb, up]   \n",
       "3                                             [good]   \n",
       "4  [app, useful, certain, phone, brand, except, p...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.08365452, 0.0579847, 0.11433671, -0.0025425...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.08703613, 0.07147217, -0.00390625, 0.005859...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.0644662, -0.0806833, -0.0020926339, 0.02535...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [0.12165112, 0.20407064, -0.18353447, -0.23372...   \n",
       "1  [0.025911188, -0.83119565, -0.03553019, -0.544...   \n",
       "2  [-0.16716677, -0.093416005, -1.4007342, 0.4197...   \n",
       "3  [0.025911188, -0.83119565, -0.03553019, -0.544...   \n",
       "4  [-0.16657814, 0.06822517, 0.1050694, -0.113447...   \n",
       "\n",
       "                                   word2vec_skipgram  \n",
       "0  [0.124642536, 0.02039718, -0.061326027, -0.039...  \n",
       "1  [0.11477529, -0.09182124, -0.14298776, -0.2736...  \n",
       "2  [-0.043917134, 0.13976072, -0.39437324, -0.012...  \n",
       "3  [0.11477529, -0.09182124, -0.14298776, -0.2736...  \n",
       "4  [0.16508076, 0.045392197, -0.108809195, -0.125...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def get_average_word2vec2(tokens_list, model, vector_size):\n",
    "    valid_tokens = [token for token in tokens_list if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    word_vectors = [model.wv[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define model parameters\n",
    "vector_size = 300   # Dimensionality of the word vectors\n",
    "window_size = 5     # Context window size\n",
    "min_count = 1       # Minimum word frequency\n",
    "workers = multiprocessing.cpu_count()  # Number of worker threads to use\n",
    "\n",
    "# Train the Word2Vec model\n",
    "cbow = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=0, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "# Save the model\n",
    "model_path = \"cbow.model\"\n",
    "cbow.save(model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n",
    "df['word2vec_cbow'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, cbow, vector_size))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "skipgram = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=1, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "# Save the model\n",
    "model_path = \"skipgram.model\"\n",
    "skipgram.save(model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n",
    "df['word2vec_skipgram'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, skipgram, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6dfa099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.7634493112564087), ('stuff', 0.6408114433288574), ('show', 0.5823018550872803), ('programme', 0.5807750821113586), ('program', 0.5739870071411133), ('genre', 0.5574767589569092), ('category', 0.5541295409202576), ('series', 0.5440436601638794), ('watched', 0.543439507484436), ('anime', 0.5359500646591187)]\n"
     ]
    }
   ],
   "source": [
    "print(cbow.wv.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d4c2cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('flim', 0.7929681539535522), ('binging', 0.7497050762176514), ('reccomende', 0.7456972002983093), ('flick', 0.7446443438529968), ('kdramas', 0.7385998368263245), ('ahow', 0.7357904314994812), ('oldie', 0.7341920733451843), ('wonderfull', 0.7281920909881592), ('syfy', 0.7268280982971191), ('sitcom', 0.7250053286552429)]\n"
     ]
    }
   ],
   "source": [
    "print(skipgram.wv.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da37946d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.8676770329475403), ('movies', 0.8013108372688293), ('films', 0.7363011837005615), ('moive', 0.6830360889434814), ('Movie', 0.6693680286407471), ('horror_flick', 0.6577848792076111), ('sequel', 0.6577793955802917), ('Guy_Ritchie_Revolver', 0.650975227355957), ('romantic_comedy', 0.6413198709487915), ('flick', 0.6321909427642822)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aeefc8",
   "metadata": {},
   "source": [
    "In the above example of the word \"movie\" we see what similar words the 3 different models are giving. For both 3 of the different embeddings, we see similar words, however CBoW seems to be better than Skip-Gram. We can notice that CBoW gives more frequent and correct words, while Skip-Gram might be giving more rare words. For this reason we will continue with the pretrained version and CBoW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b18f4",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

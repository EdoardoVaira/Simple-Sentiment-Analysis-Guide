{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe (Global Vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe, or Global Vectors, is an algorithm that like Word2Vec is creating contextual embeddings. The difference is that while Word2Vec only considers local information according to the surroundings of a specific word, GloVe captures both local and global statistics.\n",
    "\n",
    "It relies in the word-word co-occurrence probabilities with a set window-size in our dataset. The algorithm starts by creating a word co-occurence matrix from a large corpus. Firstly, it identifies the vocabulary of the corpus. Then for each word in the vocabulary, for a set context window size, count how often each word within this window around our given word appears throughout our corpus. Finally, the co-occurence matrix is created with each element representing the number the two corresponding words appear together. \n",
    "\n",
    "This word co-occurence matrix is used to examine the ratio of co-occurence probabilities between words. Moreover, this ratio between two target words given a context word can give insights on how close these two words are and is approxximated by the exponential of the dot product of the word vector of the given word and the two word vector of the target words.\n",
    "\n",
    "Unlike Word2Vec, GloVe does not train a neural network. Instead, it tries to minimize a least squares cost function that uses word vectors w_i and w_j between two words, such that their dot product plus bias terms approximates the logarithm of their corresponding co-occurence from the matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Calculating the Co-occurence Matrix\n",
    "\n",
    "#### Step 1: Start with 3 Phrases\n",
    "Let's use the following phrases as our documents as in TF-IDF:\n",
    "\n",
    "1. \"The car is fast\"\n",
    "2. \"The car is red\"\n",
    "3. \"The fast car is blue\"\n",
    "\n",
    "#### Step 2: Create the Co-occurence Matrix\n",
    "First, list all unique words across the phrases: `the`, `car`, `is`, `fast`, `red`, `blue`.\n",
    "\n",
    "For simplicity we will use a context window size of 1.\n",
    "\n",
    "Matrix Construction:\n",
    "\n",
    "Phrase 1: \"The car is fast\"\n",
    "\n",
    "The: (car) ‚Üí 1 \n",
    "\n",
    "Car: (The, is) ‚Üí 1, 1\n",
    "\n",
    "Is: (car, fast) ‚Üí 1, 1\n",
    "\n",
    "Fast: (is) ‚Üí 1\n",
    "\n",
    "Phrase 2: \"The car is red\"\n",
    "\n",
    "The: (car) ‚Üí 1\n",
    "\n",
    "Car: (The, is) ‚Üí 1, 1\n",
    "\n",
    "Is: (car, red) ‚Üí 1, 1\n",
    "\n",
    "Red: (is) ‚Üí 1\n",
    "\n",
    "Phrase 3: \"The fast car is blue\"\n",
    "\n",
    "The: (fast) ‚Üí 1\n",
    "\n",
    "Fast: (The, car) ‚Üí 1, 1\n",
    "\n",
    "Car: (fast, is) ‚Üí 1, 1\n",
    "\n",
    "Is: (car, blue) ‚Üí 1, 1\n",
    "\n",
    "Blue: (is) ‚Üí 1\n",
    "\n",
    "| Term   | the | car | is | fast | red | blue |\n",
    "|--------|-----|-----|----|------|-----|------|\n",
    "| the    |0    | 2   | 0 | 1 |   0   | 0 |\n",
    "| car    | 2   |0   | 3 | 1 | 0 | 0 |\n",
    "| is     | 0 | 3 | 0 |1 | 1 | 1 |\n",
    "| fast   | 1 | 1 | 1 | 0 | 0 | 0  |\n",
    "| red    | 0 | 0 | 1 | 0 | 0 | 0  |\n",
    "| blue   |0 | 0 | 1 | 0 | 0 | 0 |\n",
    "\n",
    "\n",
    "Then the cost function is minimized, creating the word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, usually pre-trained GloVe embeddings are used, which are trained on huge datasets. In this notebook we will use the **GloVe 6b**, which is trained on Wikipedia and Gigaword on 6 billion tokens and 400K vocabulary size, and the **GloVe Twitter**, which is trained with 2 billion tweets, 27 billion tokens and 1.2 million vocabulary size. Both embeddings come in 50,100,200 vector sizes. We will use the 100d embeddings to redude dimensionality. We use the first version as a general-purpose embedding and the second version as a more specified embedding, since tweets and netflix reviews can be similar and contain slang and informal language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in Python\n",
    "\n",
    "Let's begin by importing the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating a sample dataset. Just like we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with Netflix app reviews\n",
    "df = pd.DataFrame({\n",
    "    'Content_cleaned': [\n",
    "        'the app is great new features but crashes often',\n",
    "        'love the app love the content but it crashes',\n",
    "        'the app crashes too much it is frustrating',\n",
    "        'the content is great it is easy to use it is great'\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Content_cleaned\n",
      "0     the app is great new features but crashes often\n",
      "1        love the app love the content but it crashes\n",
      "2          the app crashes too much it is frustrating\n",
      "3  the content is great it is easy to use it is great\n"
     ]
    }
   ],
   "source": [
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the possibility to use pretrained word embeddings or train a new model ourselves. The pretrained usually used is provided by Google. In this notebook we will try both of them and see how they compare, both in vectorizing and later in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for applying pretrained embeddings using the ones provided by Google with vector size 300:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors from GloVe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>glove_6B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[-0.3715152, 0.21509555, 0.6008711, -0.2824737...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[-0.14057177, 0.06771444, 0.64484, -0.33956334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[-0.43096676, 0.11243025, 0.7203987, -0.325277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[-0.30285382, 0.20630115, 0.62620914, -0.18935...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                            glove_6B  \n",
       "0  [-0.3715152, 0.21509555, 0.6008711, -0.2824737...  \n",
       "1  [-0.14057177, 0.06771444, 0.64484, -0.33956334...  \n",
       "2  [-0.43096676, 0.11243025, 0.7203987, -0.325277...  \n",
       "3  [-0.30285382, 0.20630115, 0.62620914, -0.18935...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GloVe embeddings file\n",
    "glove_file = '../../glove.6B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_6b = load_glove_embeddings(glove_file)\n",
    "print(f\"Loaded {len(glove_6b)} word vectors from GloVe.\")\n",
    "\n",
    "# Define a function to get the average GloVe vector for a list of tokens\n",
    "def get_average_glove(tokens_list, embeddings, embedding_dim):\n",
    "    valid_tokens = [token for token in tokens_list if token in embeddings]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(embedding_dim)\n",
    "    word_vectors = [embeddings[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define the embedding dimension (e.g., 100 for 'glove.6B.100d.txt')\n",
    "embedding_dim = 100\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['Content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Compute the average GloVe vector for each row\n",
    "df['glove_6B'] = df['tokens'].apply(lambda x: get_average_glove(x, glove_6b, embedding_dim))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Word2Vec, when calculating the vectors of a sequence, we calculate the word embedding of every token seperately and then we average over them to get the sequence vector. This way the overall semantic meaning of the text is captured, while each word contributes to the final vector, allowing the resulting vector to represent the combined meanings of the individual words.\n",
    "\n",
    "As we see in the example below, our sequences have been turned into a vector of size 1x100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14057177  0.06771444  0.64484    -0.33956334  0.09621267  0.22005375\n",
      " -0.12805289  0.04956245  0.15710913 -0.03636066  0.29230332  0.03594959\n",
      "  0.22148466 -0.07713312  0.07276111 -0.05160556  0.14087535  0.15393445\n",
      " -0.2744702   0.55991334  0.21917889 -0.25078142  0.29296112  0.21898113\n",
      "  0.2882509   0.27953756 -0.01664556 -0.27808443  0.07061221 -0.18676221\n",
      " -0.14077044  0.76046836 -0.02893378  0.02613545  0.1474149   0.25755107\n",
      " -0.04283706  0.2562351   0.04777002 -0.22878289 -0.301087   -0.09659548\n",
      " -0.08913607 -0.21312311 -0.01634667  0.05641835  0.10402111 -0.27211988\n",
      "  0.06073888 -0.52196     0.12872776  0.13962911  0.20483     0.8879567\n",
      " -0.20441791 -2.230979   -0.04423889 -0.03921788  1.5170234   0.42475557\n",
      " -0.05490556  0.7453697  -0.10607912  0.05236555  0.6565278   0.06484122\n",
      "  0.52867174  0.00745687  0.13892445 -0.28618333  0.05879478 -0.11244889\n",
      "  0.218283   -0.18381844  0.1074889   0.23128165 -0.03069188 -0.18924624\n",
      " -0.8462033  -0.07151488  0.17390333 -0.01163    -0.23247777  0.24124724\n",
      " -1.1978452  -0.2074511  -0.12465078 -0.2977737  -0.30798557 -0.146809\n",
      " -0.04501922  0.07974866 -0.23277068  0.28194132 -0.3968849  -0.05042136\n",
      " -0.3560922  -0.40119553  0.5206423   0.25300223]\n"
     ]
    }
   ],
   "source": [
    "print(df['glove_6B'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors from GloVe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>glove_6B</th>\n",
       "      <th>glove_twitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[-0.3715152, 0.21509555, 0.6008711, -0.2824737...</td>\n",
       "      <td>[0.2957551, 0.019755114, 0.002848328, 0.074190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[-0.14057177, 0.06771444, 0.64484, -0.33956334...</td>\n",
       "      <td>[0.08385044, 0.029493107, 0.011498875, 0.21228...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[-0.43096676, 0.11243025, 0.7203987, -0.325277...</td>\n",
       "      <td>[0.20398799, 0.030513998, 0.2892485, 0.11278, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[-0.30285382, 0.20630115, 0.62620914, -0.18935...</td>\n",
       "      <td>[0.2149126, 0.027656665, 0.25671306, 0.2465589...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                            glove_6B  \\\n",
       "0  [-0.3715152, 0.21509555, 0.6008711, -0.2824737...   \n",
       "1  [-0.14057177, 0.06771444, 0.64484, -0.33956334...   \n",
       "2  [-0.43096676, 0.11243025, 0.7203987, -0.325277...   \n",
       "3  [-0.30285382, 0.20630115, 0.62620914, -0.18935...   \n",
       "\n",
       "                                       glove_twitter  \n",
       "0  [0.2957551, 0.019755114, 0.002848328, 0.074190...  \n",
       "1  [0.08385044, 0.029493107, 0.011498875, 0.21228...  \n",
       "2  [0.20398799, 0.030513998, 0.2892485, 0.11278, ...  \n",
       "3  [0.2149126, 0.027656665, 0.25671306, 0.2465589...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the GloVe embeddings file\n",
    "glove_file = '../../glove.twitter.27B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_twitter = load_glove_embeddings(glove_file)\n",
    "print(f\"Loaded {len(glove_twitter)} word vectors from GloVe.\")\n",
    "\n",
    "# Define a function to get the average GloVe vector for a list of tokens\n",
    "def get_average_glove(tokens_list, embeddings, embedding_dim):\n",
    "    valid_tokens = [token for token in tokens_list if token in embeddings]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(embedding_dim)\n",
    "    word_vectors = [embeddings[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define the embedding dimension (e.g., 100 for 'glove.6B.100d.txt')\n",
    "embedding_dim = 100\n",
    "\n",
    "# Compute the average GloVe vector for each row\n",
    "df['glove_twitter'] = df['tokens'].apply(lambda x: get_average_glove(x, glove_twitter, embedding_dim))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now applying what we have learned to the Netflix dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors from GloVe.\n",
      "Loaded 1193514 word vectors from GloVe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>glove_6B</th>\n",
       "      <th>glove_twitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp giving screen limit like when you a...</td>\n",
       "      <td>[plss, stopp, giving, screen, limit, like, whe...</td>\n",
       "      <td>[-0.101591855, 0.21243754, 0.45259842, -0.2616...</td>\n",
       "      <td>[0.058894146, 0.18850434, 0.08296321, 0.174713...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "      <td>[0.091552, 0.093336, -0.028113, 0.3699, 0.1895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumbs_up</td>\n",
       "      <td>[thumbs_up]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[-0.030769, 0.11993, 0.53909, -0.43696, -0.739...</td>\n",
       "      <td>[0.091552, 0.093336, -0.028113, 0.3699, 0.1895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>negative</td>\n",
       "      <td>app is useful to certain phone brand it is not...</td>\n",
       "      <td>[app, is, useful, to, certain, phone, brand, i...</td>\n",
       "      <td>[-0.19991928, 0.11995281, 0.36286283, -0.22692...</td>\n",
       "      <td>[0.23760791, 0.07707109, 0.06094666, 0.2031615...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...  negative   \n",
       "1                                               Good  positive   \n",
       "2                                                 üëçüëç  positive   \n",
       "3                                               Good   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...  negative   \n",
       "\n",
       "                                     content_cleaned  \\\n",
       "0  plss stopp giving screen limit like when you a...   \n",
       "1                                               good   \n",
       "2                                          thumbs_up   \n",
       "3                                               good   \n",
       "4  app is useful to certain phone brand it is not...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, giving, screen, limit, like, whe...   \n",
       "1                                             [good]   \n",
       "2                                        [thumbs_up]   \n",
       "3                                             [good]   \n",
       "4  [app, is, useful, to, certain, phone, brand, i...   \n",
       "\n",
       "                                            glove_6B  \\\n",
       "0  [-0.101591855, 0.21243754, 0.45259842, -0.2616...   \n",
       "1  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [-0.030769, 0.11993, 0.53909, -0.43696, -0.739...   \n",
       "4  [-0.19991928, 0.11995281, 0.36286283, -0.22692...   \n",
       "\n",
       "                                       glove_twitter  \n",
       "0  [0.058894146, 0.18850434, 0.08296321, 0.174713...  \n",
       "1  [0.091552, 0.093336, -0.028113, 0.3699, 0.1895...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.091552, 0.093336, -0.028113, 0.3699, 0.1895...  \n",
       "4  [0.23760791, 0.07707109, 0.06094666, 0.2031615...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('../DATASETS/preprocessed_text.csv')\n",
    "\n",
    "# Filling empty text that occurred after text preprocessing\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Path to the GloVe embeddings file\n",
    "glove_file = '../../glove.6B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_6b = load_glove_embeddings(glove_file)\n",
    "print(f\"Loaded {len(glove_6b)} word vectors from GloVe.\")\n",
    "\n",
    "# Define a function to get the average GloVe vector for a list of tokens\n",
    "def get_average_glove(tokens_list, embeddings, embedding_dim):\n",
    "    valid_tokens = [token for token in tokens_list if token in embeddings]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(embedding_dim)\n",
    "    word_vectors = [embeddings[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define the embedding dimension (e.g., 100 for 'glove.6B.100d.txt')\n",
    "embedding_dim = 100\n",
    "\n",
    "# Compute the average GloVe vector for each row\n",
    "df['glove_6B'] = df['tokens'].apply(lambda x: get_average_glove(x, glove_6b, embedding_dim))\n",
    "\n",
    "# Path to the GloVe embeddings file\n",
    "glove_file = '../../glove.twitter.27B.100d.txt'\n",
    "\n",
    "# Load the GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_twitter = load_glove_embeddings(glove_file)\n",
    "print(f\"Loaded {len(glove_twitter)} word vectors from GloVe.\")\n",
    "\n",
    "# Define a function to get the average GloVe vector for a list of tokens\n",
    "def get_average_glove(tokens_list, embeddings, embedding_dim):\n",
    "    valid_tokens = [token for token in tokens_list if token in embeddings]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(embedding_dim)\n",
    "    word_vectors = [embeddings[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define the embedding dimension (e.g., 100 for 'glove.6B.100d.txt')\n",
    "embedding_dim = 100\n",
    "\n",
    "# Compute the average GloVe vector for each row\n",
    "df['glove_twitter'] = df['tokens'].apply(lambda x: get_average_glove(x, glove_twitter, embedding_dim))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in Word2Vec, for a particular word we can find the most similar words in the corpus. This is done by calculating the cosine similarity of our word with the closest words in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'movie' in GloVe 6B: ['film', 'movies', 'films', 'hollywood', 'comedy']\n",
      "Words similar to 'movie' in GloVe Twitter: ['movies', 'episode', 'story', 'trailer', 'watching']\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def find_closest_embeddings(embeddings_dict, word, top_n=5):\n",
    "    if word not in embeddings_dict:\n",
    "        print(f\"Word '{word}' not found in the embedding dictionary.\")\n",
    "        return []\n",
    "    \n",
    "    embedding = embeddings_dict[word]\n",
    "    closest_words = sorted(\n",
    "        embeddings_dict.keys(), \n",
    "        key=lambda w: spatial.distance.cosine(embeddings_dict[w], embedding) if len(embeddings_dict[w]) == len(embedding) else float('inf')\n",
    "    )\n",
    "    closest_words.remove(word)  # Remove the word itself from the results\n",
    "    return closest_words[:top_n]\n",
    "\n",
    "\n",
    "similar_words_6b = find_closest_embeddings(glove_6b, 'movie')\n",
    "print(f\"Words similar to 'movie' in GloVe 6B: {similar_words_6b}\")\n",
    "\n",
    "# Test with GloVe Twitter\n",
    "similar_words_twitter = find_closest_embeddings(glove_twitter, 'movie')\n",
    "print(f\"Words similar to 'movie' in GloVe Twitter: {similar_words_twitter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example of the word \"movie\" we see what similar words the 2 different embeddings are giving. For both of the embeddings, we see relevant words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Word2Vec\n",
    "\n",
    "Positive:\n",
    "- **Scalability**: Works well with large datasets and can be easily scaled.\n",
    "- **Low Dimensionality**: The dimensionality is exceptionally reduced, compared to the frequency-based methods.\n",
    "- **Context taken into account**: Semantics and context, both local and global, are the base element of the vectors.\n",
    "\n",
    "Negative:\n",
    "- **Needs big dataset** : Requires a large amount of training data to produce high-quality embeddings.\n",
    "- **Out-Of-Vocabulary Issue**: Once again we work on a finite vocabulary, based on the corpus our model was trained on. If we have a new sequence with new words, they will not be taken into consideration when the embedding is produced, possibly losing important information.\n",
    "- **Polysemy issue**: Can struggle with words with multiple meanings since it produces one vector per word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

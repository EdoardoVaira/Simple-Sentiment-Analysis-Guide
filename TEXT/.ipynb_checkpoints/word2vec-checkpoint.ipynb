{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a neural network-based model for learning word embeddings. Unlike in the frequency-based vectorization algorithms we have already seen (BoW and TF-IDF), the vector representation of words is said to be contextually aware. \n",
    "\n",
    "In Word2Vec, every word is represented as an n-dimensional vector. In the vector space, words having similar meanings exist in close proximity to one another, determined by the contecct in which they appar in the training corpus.\n",
    "\n",
    "A very famous example of Word2Vec is the following:\n",
    "\n",
    "**king - man + woman = queen** \n",
    "\n",
    "From this we can see that Word2Vec can understand and solve analogies, by making operations in the vector space.\n",
    "\n",
    "There are two main ways to train Word2Vec, CBoW and Skip-Gram.\n",
    "\n",
    "\n",
    "### CBoW\n",
    "\n",
    "In CBoW, or Continuous Bag of Words, a NN with a single hidden layer is trained. It takes as input context (vincinity) words and its goal is to predict the current word. \n",
    "\n",
    "For example if we have the sentence \"the small kid ate a banana\", if we have vincinity=2, an input to the model can be (small, kid, a, banana) and the target word will be \"ate\". \n",
    "\n",
    "In this algorithm we choose a vincinity number m and then for every word in our sequences, a new dataset is prepared taking the m neighboring (context) words as inputs and the current word as a target. All context words in the vocabulary are turned into one-hot-encodings. For example, if the vocabulary size is 10,000, the word \"cat\" might be represented as a vector of length 10,000 with a 1 in the position corresponding to \"cat\" and 0s elsewhere. The model sums or averages the embeddings of the context words to create a single input vector. Then a NN with a single layer is trained using the new dataset. The objective of the algorithm is to maximize the probability of the target word given the context words.\n",
    "\n",
    "In the end, we will not use the actual NN anywhere, but we will use the input-to-hidden weight vector as a word embeddings matrix. The size of the embeddings is the size of the hidden layer and we can define it as a hyperparameter.\n",
    "\n",
    "Let's say in our case the vocabulary is of size 39783. If we choose a hidden layer of 300 size, then the word embeddings matrix will be of size 39783x300, since every word will be an one-hot vector of 1x39783 size. Then we multiply our word with the embedding matrix and we get a vector of 1x300 size, which is our final goal. Basically, since the one-hot encodings are just 1 in the row of the target word and 0 everywhere else, we can directly get the word embedding by selecting the corresponding row of the word embedding matrix.\n",
    "\n",
    "### Skip-Gram\n",
    "\n",
    "Skip-Gram is the exact mirrored process of CBoW, in the sense that instead of feeding the network context words and trying to predict the current word, we feed the network the current word and it tries to predict context (vincinity) words. \n",
    "\n",
    "For example if we have the sentence \"the small kid ate a banana\", if we have vincinity=2, an input to the model can be \"ate\" and the target will be (small, kid, a, banana).\n",
    "\n",
    "Again in this algorithm we choose a vincinity number m and then for every word in our sequences a dataset is prepared taking the m neighboring words as targets and the word as input. The input words are one-hot encoded the same way as in CBoW.Then a NN with a single layer is trained using the new dataset. The objective of the algorithm is to maximize the probability of context words given a target word.\n",
    "\n",
    "Again, in the end the NN is not used, but the input-to-hidden weight vector as a word embeddings matrix. The size of the embeddings is the size of the hidden layer and we can define it as a hyperparameter.\n",
    "\n",
    "Then the vectorizing of our dataset is done the same way as in CBoW.\n",
    "\n",
    "### CBoW vs Skip-Gram\n",
    "\n",
    "Skip-Gram is better when the dataset is small and emphasis on rare words is given. CBoW is better when the dataset is bigger, can better represent frequent words and it is faster to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in Python\n",
    "\n",
    "Let's begin by importing the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating a sample dataset. Just like we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with Netflix app reviews\n",
    "df = pd.DataFrame({\n",
    "    'Content_cleaned': [\n",
    "        'the app is great new features but crashes often',\n",
    "        'love the app love the content but it crashes',\n",
    "        'the app crashes too much it is frustrating',\n",
    "        'the content is great it is easy to use it is great'\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Content_cleaned\n",
      "0     the app is great new features but crashes often\n",
      "1        love the app love the content but it crashes\n",
      "2          the app crashes too much it is frustrating\n",
      "3  the content is great it is easy to use it is great\n"
     ]
    }
   ],
   "source": [
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the possibility to use pretrained word embeddings or train a new model ourselves. The pretrained usually used is provided by Google. In this notebook we will try both of them and see how they compare, both in vectorizing and later in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for applying pretrained embeddings using the ones provided by Google with vector size 300:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = models.KeyedVectors.load_word2vec_format(\n",
    "'../../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[0.056230333, 0.04981825, -0.042060003, 0.0209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[0.072252065, -0.026354684, -0.020263672, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[0.1003685, 0.001052618, -0.05606079, 0.036048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[0.07695146, 0.021551305, 0.054709695, 0.08551...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                 word2vec_pretrained  \n",
       "0  [0.056230333, 0.04981825, -0.042060003, 0.0209...  \n",
       "1  [0.072252065, -0.026354684, -0.020263672, 0.04...  \n",
       "2  [0.1003685, 0.001052618, -0.05606079, 0.036048...  \n",
       "3  [0.07695146, 0.021551305, 0.054709695, 0.08551...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_word2vec(tokens_list, model, vector_size):\n",
    "    \"\"\"\n",
    "    This function computes the average Word2Vec for a given list of tokens.\n",
    "    \"\"\"\n",
    "    # Filter the tokens that are present in the Word2Vec model\n",
    "    valid_tokens = [token for token in tokens_list if token in model]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Compute the average Word2Vec\n",
    "    word_vectors = [model[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['Content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Compute the average Word2Vec for each row\n",
    "vector_size = w2v.vector_size\n",
    "df['word2vec_pretrained'] = df['tokens'].apply(lambda x: get_average_word2vec(x, w2v, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the vectors of a sequence, we calculate the word embedding of every token seperately and then we average over them to get the sequence vector. This way the overall semantic meaning of the text is captured, while each word contributes to the final vector, allowing the resulting vector to represent the combined meanings of the individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the example below, our sequences have been turned into a vector of size 1x300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07225206 -0.02635468 -0.02026367  0.04443359 -0.10232883  0.01063368\n",
      "  0.11337619 -0.06597222  0.08262804  0.09733073 -0.08997938 -0.10812717\n",
      " -0.04382324 -0.08186849 -0.13113064  0.0128852   0.06037055  0.08719889\n",
      "  0.04602729 -0.13327366  0.05490451  0.06732856  0.01048448 -0.06485494\n",
      "  0.08699545  0.00037977 -0.03846571  0.1333279   0.01371596 -0.13808864\n",
      " -0.11214015  0.00744629 -0.00478787  0.02857802  0.05989583 -0.01656087\n",
      "  0.0215861  -0.01957533  0.04159885  0.1653239   0.07190619 -0.09299045\n",
      "  0.10758463  0.01479085 -0.05858358 -0.10636054 -0.01426358 -0.01078966\n",
      "  0.10125054  0.02113173 -0.02650282  0.04202949 -0.02031793 -0.04790582\n",
      "  0.056722    0.07293023 -0.03001404 -0.05926514 -0.05763075 -0.01670498\n",
      "  0.09899902  0.07141113 -0.06174045 -0.04302301 -0.06892904  0.0373603\n",
      " -0.07122888 -0.01359389 -0.07541911  0.0830485   0.05344645 -0.03997125\n",
      " -0.02484364 -0.04001193 -0.12537977 -0.10517035  0.05368381  0.05932617\n",
      "  0.03462728  0.13919067 -0.0236952  -0.01462131  0.13108996 -0.04589674\n",
      " -0.0992296  -0.0960829  -0.11265734  0.1585829  -0.0296563   0.06123861\n",
      " -0.01063538 -0.01033529 -0.09084066 -0.13102214 -0.02138943 -0.01801215\n",
      "  0.04533556 -0.01101685  0.01323785  0.04734972 -0.03880183  0.02994792\n",
      "  0.02312554  0.00092231  0.04820421 -0.02225664 -0.06800672 -0.00090875\n",
      " -0.00085449 -0.09966363 -0.06240506  0.04895698  0.02590603 -0.02996826\n",
      "  0.16840278 -0.0045166   0.00169542 -0.13408068  0.02598741  0.11767917\n",
      " -0.14431423 -0.00511339 -0.10270098  0.14344618  0.02945963 -0.09384494\n",
      " -0.04856152 -0.05788846 -0.08056641  0.00420125 -0.11484104 -0.10235087\n",
      " -0.08999295  0.00978258  0.00230577 -0.01024204  0.05014716 -0.05383979\n",
      "  0.02924262  0.05316162  0.01239438 -0.11159939  0.01341417 -0.02856445\n",
      " -0.00697157 -0.03162299  0.03260634  0.00295681 -0.11486138 -0.09554037\n",
      "  0.03575304  0.06583659 -0.05025228  0.01870389 -0.07228597 -0.05245633\n",
      " -0.03122457 -0.04161241 -0.09952799 -0.08247206 -0.06591119  0.064426\n",
      "  0.0164388   0.19455296 -0.00345866 -0.07991537  0.05965169  0.01678128\n",
      " -0.04227702 -0.04696994 -0.06315104 -0.01331923 -0.0784234  -0.07387289\n",
      " -0.03601413  0.0020752   0.08285861 -0.13704427 -0.06178284 -0.05476888\n",
      " -0.14450413 -0.12125651 -0.03290473 -0.03977458  0.01589288 -0.00607639\n",
      " -0.07914225  0.00819227  0.05352105  0.00271946  0.06491428 -0.00442166\n",
      " -0.05142551 -0.01017253 -0.03369141  0.07415093  0.00062391 -0.04106649\n",
      " -0.08927409 -0.14911567  0.04140896  0.01127794 -0.11436632  0.05567763\n",
      " -0.01432292  0.00189887 -0.07915582 -0.0167372   0.03843858  0.02001953\n",
      " -0.03190104  0.12573242 -0.03335232  0.02934435 -0.2203098   0.00951809\n",
      "  0.07488336 -0.00853136 -0.10617404 -0.03316583 -0.04619683  0.00737847\n",
      "  0.02642907 -0.01677789 -0.02105035 -0.01806641  0.01841905 -0.04318576\n",
      "  0.03531901 -0.00086806 -0.09032016 -0.03724501  0.04992676  0.00785997\n",
      "  0.09597608 -0.04169379 -0.03146701 -0.04073758  0.11458334  0.04254405\n",
      "  0.08374701 -0.03667535 -0.0338406  -0.07845052 -0.05696614  0.02775065\n",
      "  0.00576443  0.10512289  0.00803142 -0.09921604  0.00442166  0.06228299\n",
      "  0.04974704  0.03256565  0.06311035 -0.07177734 -0.00301107 -0.04703098\n",
      " -0.08699545 -0.03081597 -0.01896159  0.06469727  0.01814779  0.0978597\n",
      " -0.0422092   0.08569336 -0.00880941  0.00227865 -0.09987386  0.05733914\n",
      "  0.05631511  0.16385905  0.04272461 -0.01066759  0.00138346 -0.10362414\n",
      " -0.04399957 -0.0278795  -0.07006836 -0.01737637  0.02213033 -0.01136271\n",
      "  0.02266439  0.12122938  0.06049008 -0.04814996 -0.05850856  0.04730903\n",
      " -0.0116645   0.05978733  0.00849067  0.02427843 -0.03875054 -0.05843777\n",
      " -0.01997672 -0.01626248  0.03740777 -0.13563368 -0.02815247 -0.04138184]\n"
     ]
    }
   ],
   "source": [
    "print(df['word2vec_pretrained'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for training our own CBoW and Skip-Gram models. As in the pretrained case, the final result is that they turn our sequences into 1x300 vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[0.056230333, 0.04981825, -0.042060003, 0.0209...</td>\n",
       "      <td>[-3.9464252e-05, 8.735372e-05, 0.00023468967, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[0.072252065, -0.026354684, -0.020263672, 0.04...</td>\n",
       "      <td>[-0.00015348375, -0.00045190606, 0.00050380756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[0.1003685, 0.001052618, -0.05606079, 0.036048...</td>\n",
       "      <td>[-6.593054e-05, 0.0005229764, -0.0010689881, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[0.07695146, 0.021551305, 0.054709695, 0.08551...</td>\n",
       "      <td>[-1.8200057e-05, 0.0007764301, 0.0007181768, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.056230333, 0.04981825, -0.042060003, 0.0209...   \n",
       "1  [0.072252065, -0.026354684, -0.020263672, 0.04...   \n",
       "2  [0.1003685, 0.001052618, -0.05606079, 0.036048...   \n",
       "3  [0.07695146, 0.021551305, 0.054709695, 0.08551...   \n",
       "\n",
       "                                       word2vec_cbow  \n",
       "0  [-3.9464252e-05, 8.735372e-05, 0.00023468967, ...  \n",
       "1  [-0.00015348375, -0.00045190606, 0.00050380756...  \n",
       "2  [-6.593054e-05, 0.0005229764, -0.0010689881, -...  \n",
       "3  [-1.8200057e-05, 0.0007764301, 0.0007181768, -...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def get_average_word2vec2(tokens_list, model, vector_size):\n",
    "    valid_tokens = [token for token in tokens_list if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    word_vectors = [model.wv[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define model parameters\n",
    "vector_size = 300   # Dimensionality of the word vectors\n",
    "window_size = 5     # Context window size\n",
    "min_count = 1       # Minimum word frequency\n",
    "workers = multiprocessing.cpu_count()  # Number of worker threads to use\n",
    "\n",
    "# Train the Word2Vec model\n",
    "cbow = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=0, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "df['word2vec_cbow'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, cbow, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>word2vec_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[0.056230333, 0.04981825, -0.042060003, 0.0209...</td>\n",
       "      <td>[-3.9464252e-05, 8.735372e-05, 0.00023468967, ...</td>\n",
       "      <td>[-3.8613955e-05, 8.666539e-05, 0.00023473676, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[0.072252065, -0.026354684, -0.020263672, 0.04...</td>\n",
       "      <td>[-0.00015348375, -0.00045190606, 0.00050380756...</td>\n",
       "      <td>[-0.00015304553, -0.00045252705, 0.00050321664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[0.1003685, 0.001052618, -0.05606079, 0.036048...</td>\n",
       "      <td>[-6.593054e-05, 0.0005229764, -0.0010689881, -...</td>\n",
       "      <td>[-6.565961e-05, 0.0005227553, -0.0010691253, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[0.07695146, 0.021551305, 0.054709695, 0.08551...</td>\n",
       "      <td>[-1.8200057e-05, 0.0007764301, 0.0007181768, -...</td>\n",
       "      <td>[-1.7555154e-05, 0.00077540375, 0.00071704615,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.056230333, 0.04981825, -0.042060003, 0.0209...   \n",
       "1  [0.072252065, -0.026354684, -0.020263672, 0.04...   \n",
       "2  [0.1003685, 0.001052618, -0.05606079, 0.036048...   \n",
       "3  [0.07695146, 0.021551305, 0.054709695, 0.08551...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [-3.9464252e-05, 8.735372e-05, 0.00023468967, ...   \n",
       "1  [-0.00015348375, -0.00045190606, 0.00050380756...   \n",
       "2  [-6.593054e-05, 0.0005229764, -0.0010689881, -...   \n",
       "3  [-1.8200057e-05, 0.0007764301, 0.0007181768, -...   \n",
       "\n",
       "                                   word2vec_skipgram  \n",
       "0  [-3.8613955e-05, 8.666539e-05, 0.00023473676, ...  \n",
       "1  [-0.00015304553, -0.00045252705, 0.00050321664...  \n",
       "2  [-6.565961e-05, 0.0005227553, -0.0010691253, -...  \n",
       "3  [-1.7555154e-05, 0.00077540375, 0.00071704615,...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Word2Vec model\n",
    "skipgram = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=1, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "df['word2vec_skipgram'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, skipgram, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now applying what we have learned to the Netflix dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>word2vec_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp giving screen limit like when you a...</td>\n",
       "      <td>[plss, stopp, giving, screen, limit, like, whe...</td>\n",
       "      <td>[0.060924955, 0.036983438, 0.052580304, 0.1163...</td>\n",
       "      <td>[-0.12855822, 0.192796, -0.13645074, 0.4576288...</td>\n",
       "      <td>[0.054039676, 0.1618012, -0.11154804, 0.030863...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.33246085, -0.19840702, -0.75562155, 0.06854...</td>\n",
       "      <td>[0.2617957, 0.37434918, -0.0420645, 0.16268118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumbs_up</td>\n",
       "      <td>[thumbs_up]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.23665495, -0.22608165, 0.38516867, 0.079759...</td>\n",
       "      <td>[0.3395284, 0.21369722, 0.19105081, -0.0170758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.33246085, -0.19840702, -0.75562155, 0.06854...</td>\n",
       "      <td>[0.2617957, 0.37434918, -0.0420645, 0.16268118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>negative</td>\n",
       "      <td>app is useful to certain phone brand it is not...</td>\n",
       "      <td>[app, is, useful, to, certain, phone, brand, i...</td>\n",
       "      <td>[0.016780308, -0.041579314, 0.043486457, 0.058...</td>\n",
       "      <td>[-0.5931904, 0.101889275, -0.09978452, 0.49786...</td>\n",
       "      <td>[0.048506577, 0.2286796, -0.0663114, 0.0801587...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...  negative   \n",
       "1                                               Good  positive   \n",
       "2                                                 üëçüëç  positive   \n",
       "3                                               Good   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...  negative   \n",
       "\n",
       "                                     content_cleaned  \\\n",
       "0  plss stopp giving screen limit like when you a...   \n",
       "1                                               good   \n",
       "2                                          thumbs_up   \n",
       "3                                               good   \n",
       "4  app is useful to certain phone brand it is not...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, giving, screen, limit, like, whe...   \n",
       "1                                             [good]   \n",
       "2                                        [thumbs_up]   \n",
       "3                                             [good]   \n",
       "4  [app, is, useful, to, certain, phone, brand, i...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.060924955, 0.036983438, 0.052580304, 0.1163...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.016780308, -0.041579314, 0.043486457, 0.058...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [-0.12855822, 0.192796, -0.13645074, 0.4576288...   \n",
       "1  [0.33246085, -0.19840702, -0.75562155, 0.06854...   \n",
       "2  [0.23665495, -0.22608165, 0.38516867, 0.079759...   \n",
       "3  [0.33246085, -0.19840702, -0.75562155, 0.06854...   \n",
       "4  [-0.5931904, 0.101889275, -0.09978452, 0.49786...   \n",
       "\n",
       "                                   word2vec_skipgram  \n",
       "0  [0.054039676, 0.1618012, -0.11154804, 0.030863...  \n",
       "1  [0.2617957, 0.37434918, -0.0420645, 0.16268118...  \n",
       "2  [0.3395284, 0.21369722, 0.19105081, -0.0170758...  \n",
       "3  [0.2617957, 0.37434918, -0.0420645, 0.16268118...  \n",
       "4  [0.048506577, 0.2286796, -0.0663114, 0.0801587...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('../DATASETS/preprocessed_text.csv')\n",
    "\n",
    "# Filling empty text that occurred after text preprocessing\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Compute the average Word2Vec for each row\n",
    "vector_size = w2v.vector_size\n",
    "df['word2vec_pretrained'] = df['tokens'].apply(lambda x: get_average_word2vec(x, w2v, vector_size))\n",
    "\n",
    "\n",
    "# Train the Word2Vec model\n",
    "cbow = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=0, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "df['word2vec_cbow'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, cbow, vector_size))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "skipgram = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=1, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "df['word2vec_skipgram'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, skipgram, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the matrix will be the same as BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love to watch the movie s you have on here and the tv shows i love them growing_heart\n",
      "[-2.08816528e-02  1.15051270e-02  2.57377625e-02  1.76330566e-01\n",
      " -6.28890991e-02 -5.34057617e-05  5.42964935e-02 -1.09580994e-01\n",
      "  1.54647827e-02  7.56883621e-02 -3.56063843e-02 -1.48437500e-01\n",
      " -7.52716064e-02 -1.43718719e-02 -1.27414703e-01  8.87413025e-02\n",
      "  7.78579712e-02  9.60235596e-02  8.70895386e-03 -3.39450836e-02\n",
      " -5.30548096e-02  6.39114380e-02  7.28273392e-02  1.10912323e-03\n",
      "  4.01535034e-02  3.60412598e-02 -5.64794540e-02  4.14695740e-02\n",
      "  1.10197067e-02 -4.98657227e-02 -4.34494019e-02  7.13858604e-02\n",
      " -8.34045410e-02  2.02445984e-02  1.19743347e-02  7.50732422e-03\n",
      " -9.05013084e-03  3.79447937e-02  2.40020752e-02  1.17538452e-01\n",
      "  3.45458984e-02 -8.88824463e-02  1.42479420e-01  2.61783600e-02\n",
      " -1.01394653e-02  1.56650543e-02  3.45573425e-02 -4.45880890e-02\n",
      "  4.49352264e-02 -2.30102539e-02 -2.92930603e-02  5.40466309e-02\n",
      "  1.87149048e-02  8.07876587e-02  9.26971436e-04 -1.28021240e-02\n",
      " -6.44063950e-02 -2.71072388e-02  1.59311295e-02 -2.65610218e-02\n",
      "  2.09045410e-03  5.79319000e-02 -1.18476868e-01 -1.41220093e-02\n",
      " -8.63647461e-03 -2.70414352e-02 -6.69097900e-02  6.64672852e-02\n",
      "  1.00216866e-02  4.15725708e-02  6.54754639e-02  4.84867096e-02\n",
      "  3.16143036e-02  1.87530518e-02 -1.16477966e-01 -6.05697632e-02\n",
      "  5.04465103e-02  6.65550232e-02  6.33453280e-02  1.11518860e-01\n",
      " -7.19528198e-02 -2.11000443e-02  6.89163208e-02 -7.91816711e-02\n",
      " -5.51719666e-02 -1.32980347e-02 -1.13021851e-01  1.26502991e-01\n",
      " -1.10621452e-02  3.68614197e-02 -4.24766541e-03  1.01593018e-01\n",
      " -3.06205750e-02 -9.46655273e-02 -3.90090942e-02  4.39608097e-03\n",
      "  5.74817657e-02  1.33476257e-02  5.25512695e-02  1.27105713e-02\n",
      " -3.09545994e-02 -1.41403675e-02  9.11560059e-02  4.90951538e-03\n",
      " -3.75213623e-02  4.80275154e-02 -7.34529495e-02 -3.55758667e-02\n",
      "  4.52842712e-02 -1.39831543e-01 -1.16088867e-01 -2.12707520e-02\n",
      " -7.67898560e-03 -4.48303223e-02  1.49307251e-01 -8.17871094e-03\n",
      "  1.14898682e-02 -9.58404541e-02  4.00238037e-02 -3.06963921e-03\n",
      " -3.82633209e-02 -3.37448120e-02 -6.70471191e-02  5.91964722e-02\n",
      " -6.97898865e-03 -1.02451324e-01 -1.16607666e-01 -2.02178955e-03\n",
      " -2.68707275e-02  2.73079872e-02 -1.05247498e-01 -1.01848602e-01\n",
      " -7.22351074e-02  2.08892822e-02 -1.31568909e-02 -4.58669662e-02\n",
      " -1.71089172e-02 -1.40686035e-02  2.97546387e-02  2.28576660e-02\n",
      "  1.01593018e-01 -6.64749146e-02  4.73318100e-02 -8.13674927e-03\n",
      " -1.76010132e-02  5.43212891e-03  6.34613037e-02 -2.46725082e-02\n",
      " -3.94401550e-02 -3.38554382e-04  9.12132263e-02  5.01556396e-02\n",
      " -7.70568848e-02 -7.59243965e-03 -1.02466583e-01 -4.16660309e-02\n",
      " -2.36673355e-02 -4.33197021e-02 -5.67359924e-02 -1.74064636e-02\n",
      "  1.49793625e-02  8.94851685e-02 -7.89585114e-02  1.73675537e-01\n",
      "  1.90391541e-02 -1.06060028e-01  1.13124847e-01 -1.06105804e-02\n",
      " -3.60016823e-02  1.29699707e-02 -1.31332397e-01 -3.94439697e-02\n",
      " -7.67898560e-03 -8.98971558e-02 -4.54597473e-02  5.64022064e-02\n",
      "  1.25133216e-01 -1.07540131e-01  4.34875488e-03 -5.63125610e-02\n",
      " -2.91328430e-02 -5.62657118e-02  1.98407173e-02 -1.51710510e-02\n",
      "  1.34363174e-02  8.85391235e-03 -9.70497131e-02  6.03103638e-03\n",
      "  8.40148926e-02  1.97186470e-02  4.56390381e-02  8.45489502e-02\n",
      " -4.77981567e-02 -1.12838745e-02  6.56871796e-02  7.56502151e-02\n",
      "  5.41687012e-04 -3.30066681e-03 -6.23054504e-02 -1.47552490e-01\n",
      " -1.26504898e-02  7.54928589e-02 -1.21841431e-01 -3.50799561e-02\n",
      "  2.26478577e-02 -2.35977173e-02 -4.49457169e-02  3.72514725e-02\n",
      " -3.85131836e-02  8.10890198e-02 -6.58416748e-03  1.78522110e-01\n",
      " -9.84115601e-02 -3.18107605e-02 -1.42797470e-01  1.18160248e-02\n",
      "  1.49399757e-01 -3.06615829e-02 -1.66305542e-01 -2.11277008e-02\n",
      " -4.09393311e-02  2.37770081e-02  3.11279297e-03  3.18145752e-02\n",
      " -4.19769287e-02 -8.30965042e-02  5.04608154e-02  8.49628448e-02\n",
      " -3.25357914e-02 -8.51135254e-02  2.48107910e-02 -9.22851562e-02\n",
      "  3.09959054e-02  6.65740967e-02  1.15777731e-01  2.32620239e-02\n",
      "  1.74560547e-02  1.62391663e-02  1.00845337e-01  4.29916382e-02\n",
      "  6.52065277e-02 -4.14371490e-04  7.92503357e-03 -6.10867739e-02\n",
      " -8.12759399e-02  4.87060547e-02  1.64078474e-02  7.76901245e-02\n",
      " -1.95817798e-02 -6.79063797e-02  6.97159767e-02  6.62317276e-02\n",
      "  5.02052307e-02  5.58700562e-02  7.19642639e-03 -3.32469940e-02\n",
      " -7.47561455e-03 -2.40631104e-02 -8.44001770e-02 -5.85479736e-02\n",
      " -3.52783203e-02 -9.01794434e-03  2.50511169e-02  6.50978088e-03\n",
      "  1.56879425e-02  1.56066895e-01 -2.92015076e-02 -1.64165497e-02\n",
      "  5.15365601e-03  6.78443909e-03  7.94887543e-03  1.20155334e-01\n",
      "  1.15600586e-01  6.21566772e-02  9.04541016e-02 -6.76918030e-02\n",
      " -6.18302822e-02 -1.25659943e-01 -6.88380003e-02 -1.79023743e-02\n",
      " -1.20925903e-02  4.44221497e-03 -1.65939331e-03  9.82055664e-02\n",
      "  2.64835358e-03  2.83892155e-02 -6.98013306e-02 -1.46160126e-02\n",
      " -4.16984558e-02  1.64260864e-02 -3.25264931e-02  8.89968872e-02\n",
      " -1.15447998e-01 -4.02385592e-02  5.38635254e-03 -1.12630844e-01\n",
      " -3.43360901e-02 -4.90150452e-02 -2.57425308e-02 -2.18200684e-02]\n"
     ]
    }
   ],
   "source": [
    "print(df['content_cleaned'][44])\n",
    "print(df['word2vec_pretrained'][44])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we cannot understand just by seeing the embedding vector what the sequence represents. However, the machine learning algorithms that use these embeddings as inputs, can understand much better the features and the sequences, since they are created in a way that takes the context into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting feature of the Word2Vec models is that for a particular word, they can give its most similar words in the corpus. This is done by calculating the cosine similarity of our word with the closest words in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.8676770329475403), ('movies', 0.8013108372688293), ('films', 0.7363011837005615), ('moive', 0.6830360889434814), ('Movie', 0.6693680286407471), ('horror_flick', 0.6577848792076111), ('sequel', 0.6577793955802917), ('Guy_Ritchie_Revolver', 0.650975227355957), ('romantic_comedy', 0.6413198709487915), ('flick', 0.6321909427642822)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.8720470070838928), ('show', 0.6472330093383789), ('program', 0.6291745901107788), ('movies', 0.6163832545280457), ('films', 0.579606294631958), ('trailer', 0.5703409910202026), ('episode', 0.5682822465896606), ('programme', 0.5584979057312012), ('scene', 0.5447840690612793), ('title', 0.539584755897522)]\n"
     ]
    }
   ],
   "source": [
    "print(cbow.wv.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.7316080331802368), ('tittle', 0.6571516394615173), ('movies', 0.6406260132789612), ('continuation', 0.6287208199501038), ('moive', 0.626655638217926), ('flim', 0.6253799200057983), ('serie', 0.6253756880760193), ('weather', 0.6172146797180176), ('flick', 0.6149208545684814), ('unpopular', 0.6134818196296692)]\n"
     ]
    }
   ],
   "source": [
    "print(skipgram.wv.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example of the word \"movie\" we see what similar words the 3 different models are giving. For all 3 of the different embeddings, we see similar words, however CBoW seems to be better than Skip-Gram. We can notice that CBoW gives more frequent and correct words, while Skip-Gram might be giving more rare words. For this reason we will continue with the pretrained version and CBoW when we deploy our models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Word2Vec\n",
    "\n",
    "Positive:\n",
    "- **Scalability**: Works well with large datasets and can be easily scaled.\n",
    "- **Low Dimensionality**: The dimensionality is exceptionally reduced, compared to the frequency-based methods.\n",
    "- **Context taken into account**: Semantics and context are the base element of the vectors.\n",
    "\n",
    "Negative:\n",
    "- **Needs big dataset** : Requires a large amount of training data to produce high-quality embeddings.\n",
    "- **Out-Of-Vocabulary Issue**: Once again we work on a finite vocabulary, based on the corpus our model was trained on. If we have a new sequence with new words, they will not be taken into consideration when the embedding is produced, possibly losing important information.\n",
    "- **Polysemy issue**: Can struggle with words with multiple meanings since it produces one vector per word.\n",
    "- **Fixed window space**: Context is limited to a certain window size around the target word, losing long range dependencies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

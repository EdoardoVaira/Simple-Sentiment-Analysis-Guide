{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a method for turning words into numbers (vectors) in a way that captures their meaning based on the context in which they appear. Unlike simpler methods like Bag-of-Words (BoW) or TF-IDF, which just count word occurrences, Word2Vec builds contextual word representations. This means that words with similar meanings end up with similar vectors in the Word2Vec model.\n",
    "\n",
    "A famous example of Word2Vec's power is the analogy:\n",
    "**king - man + woman = queen**\n",
    "\n",
    "This shows that Word2Vec understands relationships between words so well that you can perform basic arithmetic on the vectors to solve analogies!\n",
    "\n",
    "### How Word2Vec Works\n",
    "\n",
    "Word2Vec works by creating word embeddings, which are n-dimensional vectors representing words. So a single word will be represented as a list of n numbers. These vectors are trained using a neural network with two key architectures:\n",
    "\n",
    "- CBOW (Continuous Bag of Words)\n",
    "- Skip-Gram\n",
    "\n",
    "In both cases, the network is simple (just two layers) and the goal isn't to classify data but to learn the word embeddings from the training data. Once trained, you throw away the neural network and use the input-to-hidden weight matrix as your word embeddings.\n",
    "\n",
    "\n",
    "### Continuous Bag of Words (CBOW)\n",
    "\n",
    "**How CBOW Works:**\n",
    "\n",
    "In CBOW, the model takes words from the context (the words surrounding a target word) as input and tries to predict the target word.\n",
    "\n",
    "**Example:** For the sentence:\n",
    "‚ÄúI enjoy reading books about history,‚Äù\n",
    "if the context window size is 2, the model will take the words:\n",
    "[\"I\", \"enjoy\", \"books\", \"about\"]\n",
    "and try to predict the middle word, which is \"reading\".\n",
    "\n",
    "This is repeated for every word in the training data, using the neighboring words to predict the target word. You can set the context window size to decide how many words on each side of the target word to include.\n",
    "\n",
    "**How the Neural Network is Built:**\n",
    "\n",
    "1. **Vocabulary and One-Hot Encoding:** First, every word in your vocabulary (all unique words in the text) is assigned a one-hot encoded vector. This is a large vector with a length equal to the vocabulary size, with a 1 in the position of the word and 0s everywhere else.\n",
    "    - Example: If the vocabulary size is 10,000, the word \"cat\" might be represented as [0, 0, 0, ..., 1, 0, 0].\n",
    "\n",
    "2. **Word Embeddings Matrix:** The input layer of the network is the one-hot encoded vectors for the context words. These vectors are multiplied by the input-to-hidden weights matrix, which gives you a much smaller vector called the word embedding (e.g., instead of a 10,000-length vector, you might get a 300-length vector).\n",
    "\n",
    "    After training, the weight matrix becomes your word embedding matrix, where each row corresponds to a word‚Äôs embedding.\n",
    "\n",
    "3. **Training Objective:** The network learns by adjusting the weights to maximize the probability of correctly predicting the target word, given the context words. Once trained, you can discard the neural network and keep the learned embeddings.\n",
    "\n",
    "**Embedding Matrix Example:**\n",
    "\n",
    "- Suppose you have a vocabulary of 39,783 words.\n",
    "- You define the hidden layer (embedding size) to be 300.\n",
    "- After training, you get a 39,783 x 300 matrix, where each word in your vocabulary is represented by a 300-dimensional vector.\n",
    "\n",
    "When you input a word (like \"cat\"), it is converted to a one-hot vector, which is then multiplied by the embedding matrix to get its word embedding.\n",
    "\n",
    "**CBOW Summary:**\n",
    "- Input: Surrounding words (context).\n",
    "- Goal: Predict the middle word.\n",
    "- Training Output: A word embeddings matrix.\n",
    "\n",
    "\n",
    "### Skip-Gram\n",
    "\n",
    "Skip-Gram is the reverse of CBOW. Instead of using context words to predict a target word, Skip-Gram takes a target word and predicts the context words around it.\n",
    "\n",
    "**Example:** For the sentence:\n",
    "‚ÄúThe small kid ate a banana‚Äù, with a context window size of 2, you would use the word \"ate\" as the input, and the targets would be the words [\"small\", \"kid\", \"a\", \"banana\"].\n",
    "\n",
    "**How the Neural Network is Built:**\n",
    "\n",
    "The process is similar to CBOW, but in Skip-Gram:\n",
    "\n",
    "- The input to the neural network is a single target word.\n",
    "- The output is the prediction of the surrounding context words.\n",
    "\n",
    "Again, the neural network is trained to maximize the probability of predicting the correct context words given the target word. Once trained, the word embeddings are stored in the input-to-hidden weight matrix, just like with CBOW.\n",
    "\n",
    "**Skip-Gram Summary:**\n",
    "\n",
    "- Input: Target word.\n",
    "- Goal: Predict surrounding words.\n",
    "- Training Output: A word embeddings matrix.\n",
    "\n",
    "### CBOW vs. Skip-Gram: Which One to Use?\n",
    "\n",
    "- CBOW is faster and works better on large datasets. It‚Äôs good at predicting common words and is often the default choice when you have a lot of data.\n",
    "- Skip-Gram is slower but better for smaller datasets and works well when you're focusing on rare words. It does a better job learning the embeddings for less frequent words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use it\n",
    "\n",
    "Preprocess the Text:\n",
    "\n",
    "- Tokenize the text (split it into words).\n",
    "- Remove stopwords (optional).\n",
    "- Convert words into embeddings using the trained Word2Vec model.\n",
    "\n",
    "For example, the sentence \"I loved the movie\" could become:\n",
    "\n",
    "- \"I\" ‚Üí [0.01, 0.02, -0.01, ...]\n",
    "- \"loved\" ‚Üí [0.3, -0.1, 0.2, ...]\n",
    "- \"movie\" ‚Üí [0.25, -0.15, 0.05, ...]\n",
    "\n",
    "Aggregate the Embeddings:\n",
    "\n",
    "Since the sentence has multiple words, you need to combine their embeddings into a single vector.\n",
    "Common ways to do this are:\n",
    "- Average: Take the average of the embeddings.\n",
    "- Sum: Sum the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in Python\n",
    "\n",
    "Let's begin by importing the libraries we need. We will use a library called Gensim..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a sample dataset. Just like we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with Netflix app reviews\n",
    "df = pd.DataFrame({\n",
    "    'Content_cleaned': [\n",
    "        'the app is great new features but crashes often',\n",
    "        'love the app love the content but it crashes',\n",
    "        'the app crashes too much it is frustrating',\n",
    "        'the content is great it is easy to use it is great'\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Content_cleaned\n",
      "0     the app is great new features but crashes often\n",
      "1        love the app love the content but it crashes\n",
      "2          the app crashes too much it is frustrating\n",
      "3  the content is great it is easy to use it is great\n"
     ]
    }
   ],
   "source": [
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the possibility to use pretrained word embeddings or train a new model ourselves. The pretrained usually used is provided by Google. In this notebook we will try both of them and see how they compare, both in vectorizing and later in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for applying pretrained embeddings using the ones provided by Google with vector size 300. It consists of 100 bilion words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = models.KeyedVectors.load_word2vec_format(\n",
    "'../../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[0.056230333, 0.04981825, -0.042060003, 0.0209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[0.072252065, -0.026354684, -0.020263672, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[0.1003685, 0.001052618, -0.05606079, 0.036048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[0.07695146, 0.021551305, 0.054709695, 0.08551...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                 word2vec_pretrained  \n",
       "0  [0.056230333, 0.04981825, -0.042060003, 0.0209...  \n",
       "1  [0.072252065, -0.026354684, -0.020263672, 0.04...  \n",
       "2  [0.1003685, 0.001052618, -0.05606079, 0.036048...  \n",
       "3  [0.07695146, 0.021551305, 0.054709695, 0.08551...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_word2vec(tokens_list, model, vector_size):\n",
    "    \"\"\"\n",
    "    This function computes the average Word2Vec for a given list of tokens.\n",
    "    \"\"\"\n",
    "    # Filter the tokens that are present in the Word2Vec model\n",
    "    valid_tokens = [token for token in tokens_list if token in model]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Compute the average Word2Vec\n",
    "    word_vectors = [model[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "df['tokens'] = df['Content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Compute the average Word2Vec for each row\n",
    "vector_size = w2v.vector_size\n",
    "df['word2vec_pretrained'] = df['tokens'].apply(lambda x: get_average_word2vec(x, w2v, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the vectors of a sequence, we calculate the word embedding of every token seperately and then we average over them to get the sequence vector. This way the overall semantic meaning of the text is captured, while each word contributes to the final vector, allowing the resulting vector to represent the combined meanings of the individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the example below, our sequences have been turned into a vector of size 1x300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07225206 -0.02635468 -0.02026367  0.04443359 -0.10232883  0.01063368\n",
      "  0.11337619 -0.06597222  0.08262804  0.09733073 -0.08997938 -0.10812717\n",
      " -0.04382324 -0.08186849 -0.13113064  0.0128852   0.06037055  0.08719889\n",
      "  0.04602729 -0.13327366  0.05490451  0.06732856  0.01048448 -0.06485494\n",
      "  0.08699545  0.00037977 -0.03846571  0.1333279   0.01371596 -0.13808864\n",
      " -0.11214015  0.00744629 -0.00478787  0.02857802  0.05989583 -0.01656087\n",
      "  0.0215861  -0.01957533  0.04159885  0.1653239   0.07190619 -0.09299045\n",
      "  0.10758463  0.01479085 -0.05858358 -0.10636054 -0.01426358 -0.01078966\n",
      "  0.10125054  0.02113173 -0.02650282  0.04202949 -0.02031793 -0.04790582\n",
      "  0.056722    0.07293023 -0.03001404 -0.05926514 -0.05763075 -0.01670498\n",
      "  0.09899902  0.07141113 -0.06174045 -0.04302301 -0.06892904  0.0373603\n",
      " -0.07122888 -0.01359389 -0.07541911  0.0830485   0.05344645 -0.03997125\n",
      " -0.02484364 -0.04001193 -0.12537977 -0.10517035  0.05368381  0.05932617\n",
      "  0.03462728  0.13919067 -0.0236952  -0.01462131  0.13108996 -0.04589674\n",
      " -0.0992296  -0.0960829  -0.11265734  0.1585829  -0.0296563   0.06123861\n",
      " -0.01063538 -0.01033529 -0.09084066 -0.13102214 -0.02138943 -0.01801215\n",
      "  0.04533556 -0.01101685  0.01323785  0.04734972 -0.03880183  0.02994792\n",
      "  0.02312554  0.00092231  0.04820421 -0.02225664 -0.06800672 -0.00090875\n",
      " -0.00085449 -0.09966363 -0.06240506  0.04895698  0.02590603 -0.02996826\n",
      "  0.16840278 -0.0045166   0.00169542 -0.13408068  0.02598741  0.11767917\n",
      " -0.14431423 -0.00511339 -0.10270098  0.14344618  0.02945963 -0.09384494\n",
      " -0.04856152 -0.05788846 -0.08056641  0.00420125 -0.11484104 -0.10235087\n",
      " -0.08999295  0.00978258  0.00230577 -0.01024204  0.05014716 -0.05383979\n",
      "  0.02924262  0.05316162  0.01239438 -0.11159939  0.01341417 -0.02856445\n",
      " -0.00697157 -0.03162299  0.03260634  0.00295681 -0.11486138 -0.09554037\n",
      "  0.03575304  0.06583659 -0.05025228  0.01870389 -0.07228597 -0.05245633\n",
      " -0.03122457 -0.04161241 -0.09952799 -0.08247206 -0.06591119  0.064426\n",
      "  0.0164388   0.19455296 -0.00345866 -0.07991537  0.05965169  0.01678128\n",
      " -0.04227702 -0.04696994 -0.06315104 -0.01331923 -0.0784234  -0.07387289\n",
      " -0.03601413  0.0020752   0.08285861 -0.13704427 -0.06178284 -0.05476888\n",
      " -0.14450413 -0.12125651 -0.03290473 -0.03977458  0.01589288 -0.00607639\n",
      " -0.07914225  0.00819227  0.05352105  0.00271946  0.06491428 -0.00442166\n",
      " -0.05142551 -0.01017253 -0.03369141  0.07415093  0.00062391 -0.04106649\n",
      " -0.08927409 -0.14911567  0.04140896  0.01127794 -0.11436632  0.05567763\n",
      " -0.01432292  0.00189887 -0.07915582 -0.0167372   0.03843858  0.02001953\n",
      " -0.03190104  0.12573242 -0.03335232  0.02934435 -0.2203098   0.00951809\n",
      "  0.07488336 -0.00853136 -0.10617404 -0.03316583 -0.04619683  0.00737847\n",
      "  0.02642907 -0.01677789 -0.02105035 -0.01806641  0.01841905 -0.04318576\n",
      "  0.03531901 -0.00086806 -0.09032016 -0.03724501  0.04992676  0.00785997\n",
      "  0.09597608 -0.04169379 -0.03146701 -0.04073758  0.11458334  0.04254405\n",
      "  0.08374701 -0.03667535 -0.0338406  -0.07845052 -0.05696614  0.02775065\n",
      "  0.00576443  0.10512289  0.00803142 -0.09921604  0.00442166  0.06228299\n",
      "  0.04974704  0.03256565  0.06311035 -0.07177734 -0.00301107 -0.04703098\n",
      " -0.08699545 -0.03081597 -0.01896159  0.06469727  0.01814779  0.0978597\n",
      " -0.0422092   0.08569336 -0.00880941  0.00227865 -0.09987386  0.05733914\n",
      "  0.05631511  0.16385905  0.04272461 -0.01066759  0.00138346 -0.10362414\n",
      " -0.04399957 -0.0278795  -0.07006836 -0.01737637  0.02213033 -0.01136271\n",
      "  0.02266439  0.12122938  0.06049008 -0.04814996 -0.05850856  0.04730903\n",
      " -0.0116645   0.05978733  0.00849067  0.02427843 -0.03875054 -0.05843777\n",
      " -0.01997672 -0.01626248  0.03740777 -0.13563368 -0.02815247 -0.04138184]\n"
     ]
    }
   ],
   "source": [
    "print(df['word2vec_pretrained'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for training our own CBoW and Skip-Gram models. As in the pretrained case, the final result is that they turn our sequences into 1x300 vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[0.056230333, 0.04981825, -0.042060003, 0.0209...</td>\n",
       "      <td>[-3.9464252e-05, 8.735372e-05, 0.00023468967, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[0.072252065, -0.026354684, -0.020263672, 0.04...</td>\n",
       "      <td>[-0.00015348375, -0.00045190606, 0.00050380756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[0.1003685, 0.001052618, -0.05606079, 0.036048...</td>\n",
       "      <td>[-6.593054e-05, 0.0005229764, -0.0010689881, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[0.07695146, 0.021551305, 0.054709695, 0.08551...</td>\n",
       "      <td>[-1.8200057e-05, 0.0007764301, 0.0007181768, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.056230333, 0.04981825, -0.042060003, 0.0209...   \n",
       "1  [0.072252065, -0.026354684, -0.020263672, 0.04...   \n",
       "2  [0.1003685, 0.001052618, -0.05606079, 0.036048...   \n",
       "3  [0.07695146, 0.021551305, 0.054709695, 0.08551...   \n",
       "\n",
       "                                       word2vec_cbow  \n",
       "0  [-3.9464252e-05, 8.735372e-05, 0.00023468967, ...  \n",
       "1  [-0.00015348375, -0.00045190606, 0.00050380756...  \n",
       "2  [-6.593054e-05, 0.0005229764, -0.0010689881, -...  \n",
       "3  [-1.8200057e-05, 0.0007764301, 0.0007181768, -...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def get_average_word2vec2(tokens_list, model, vector_size):\n",
    "    valid_tokens = [token for token in tokens_list if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    word_vectors = [model.wv[token] for token in valid_tokens]\n",
    "    average_vector = np.mean(word_vectors, axis=0)\n",
    "    return average_vector\n",
    "\n",
    "# Define model parameters\n",
    "vector_size = 300   # Dimensionality of the word vectors\n",
    "window_size = 5     # Context window size\n",
    "min_count = 1       # Minimum word frequency\n",
    "workers = multiprocessing.cpu_count()  # Number of worker threads to use\n",
    "\n",
    "# Train the Word2Vec model\n",
    "cbow = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=0, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "df['word2vec_cbow'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, cbow, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>word2vec_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[0.056230333, 0.04981825, -0.042060003, 0.0209...</td>\n",
       "      <td>[-3.9464252e-05, 8.735372e-05, 0.00023468967, ...</td>\n",
       "      <td>[-3.8613955e-05, 8.666539e-05, 0.00023473676, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[0.072252065, -0.026354684, -0.020263672, 0.04...</td>\n",
       "      <td>[-0.00015348375, -0.00045190606, 0.00050380756...</td>\n",
       "      <td>[-0.00015304553, -0.00045252705, 0.00050321664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[0.1003685, 0.001052618, -0.05606079, 0.036048...</td>\n",
       "      <td>[-6.593054e-05, 0.0005229764, -0.0010689881, -...</td>\n",
       "      <td>[-6.565961e-05, 0.0005227553, -0.0010691253, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[0.07695146, 0.021551305, 0.054709695, 0.08551...</td>\n",
       "      <td>[-1.8200057e-05, 0.0007764301, 0.0007181768, -...</td>\n",
       "      <td>[-1.7555154e-05, 0.00077540375, 0.00071704615,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.056230333, 0.04981825, -0.042060003, 0.0209...   \n",
       "1  [0.072252065, -0.026354684, -0.020263672, 0.04...   \n",
       "2  [0.1003685, 0.001052618, -0.05606079, 0.036048...   \n",
       "3  [0.07695146, 0.021551305, 0.054709695, 0.08551...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [-3.9464252e-05, 8.735372e-05, 0.00023468967, ...   \n",
       "1  [-0.00015348375, -0.00045190606, 0.00050380756...   \n",
       "2  [-6.593054e-05, 0.0005229764, -0.0010689881, -...   \n",
       "3  [-1.8200057e-05, 0.0007764301, 0.0007181768, -...   \n",
       "\n",
       "                                   word2vec_skipgram  \n",
       "0  [-3.8613955e-05, 8.666539e-05, 0.00023473676, ...  \n",
       "1  [-0.00015304553, -0.00045252705, 0.00050321664...  \n",
       "2  [-6.565961e-05, 0.0005227553, -0.0010691253, -...  \n",
       "3  [-1.7555154e-05, 0.00077540375, 0.00071704615,...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Word2Vec model\n",
    "skipgram = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=1, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "df['word2vec_skipgram'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, skipgram, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now applying what we have learned to the Netflix dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>word2vec_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>negative</td>\n",
       "      <td>plss stopp giving screen limit like when you a...</td>\n",
       "      <td>[plss, stopp, giving, screen, limit, like, whe...</td>\n",
       "      <td>[0.060924955, 0.036983438, 0.052580304, 0.1163...</td>\n",
       "      <td>[-0.12855822, 0.192796, -0.13645074, 0.4576288...</td>\n",
       "      <td>[0.054039676, 0.1618012, -0.11154804, 0.030863...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.33246085, -0.19840702, -0.75562155, 0.06854...</td>\n",
       "      <td>[0.2617957, 0.37434918, -0.0420645, 0.16268118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumbs_up</td>\n",
       "      <td>[thumbs_up]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.23665495, -0.22608165, 0.38516867, 0.079759...</td>\n",
       "      <td>[0.3395284, 0.21369722, 0.19105081, -0.0170758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.33246085, -0.19840702, -0.75562155, 0.06854...</td>\n",
       "      <td>[0.2617957, 0.37434918, -0.0420645, 0.16268118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>negative</td>\n",
       "      <td>app is useful to certain phone brand it is not...</td>\n",
       "      <td>[app, is, useful, to, certain, phone, brand, i...</td>\n",
       "      <td>[0.016780308, -0.041579314, 0.043486457, 0.058...</td>\n",
       "      <td>[-0.5931904, 0.101889275, -0.09978452, 0.49786...</td>\n",
       "      <td>[0.048506577, 0.2286796, -0.0663114, 0.0801587...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...  negative   \n",
       "1                                               Good  positive   \n",
       "2                                                 üëçüëç  positive   \n",
       "3                                               Good   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...  negative   \n",
       "\n",
       "                                     content_cleaned  \\\n",
       "0  plss stopp giving screen limit like when you a...   \n",
       "1                                               good   \n",
       "2                                          thumbs_up   \n",
       "3                                               good   \n",
       "4  app is useful to certain phone brand it is not...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, giving, screen, limit, like, whe...   \n",
       "1                                             [good]   \n",
       "2                                        [thumbs_up]   \n",
       "3                                             [good]   \n",
       "4  [app, is, useful, to, certain, phone, brand, i...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.060924955, 0.036983438, 0.052580304, 0.1163...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.016780308, -0.041579314, 0.043486457, 0.058...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [-0.12855822, 0.192796, -0.13645074, 0.4576288...   \n",
       "1  [0.33246085, -0.19840702, -0.75562155, 0.06854...   \n",
       "2  [0.23665495, -0.22608165, 0.38516867, 0.079759...   \n",
       "3  [0.33246085, -0.19840702, -0.75562155, 0.06854...   \n",
       "4  [-0.5931904, 0.101889275, -0.09978452, 0.49786...   \n",
       "\n",
       "                                   word2vec_skipgram  \n",
       "0  [0.054039676, 0.1618012, -0.11154804, 0.030863...  \n",
       "1  [0.2617957, 0.37434918, -0.0420645, 0.16268118...  \n",
       "2  [0.3395284, 0.21369722, 0.19105081, -0.0170758...  \n",
       "3  [0.2617957, 0.37434918, -0.0420645, 0.16268118...  \n",
       "4  [0.048506577, 0.2286796, -0.0663114, 0.0801587...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('../DATASETS/preprocessed_text.csv')\n",
    "\n",
    "# Filling empty text that occurred after text preprocessing\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Tokenize the text data\n",
    "df['tokens'] = df['content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Compute the average Word2Vec for each row\n",
    "vector_size = w2v.vector_size\n",
    "df['word2vec_pretrained'] = df['tokens'].apply(lambda x: get_average_word2vec(x, w2v, vector_size))\n",
    "\n",
    "\n",
    "# Train the Word2Vec model\n",
    "cbow = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=0, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "df['word2vec_cbow'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, cbow, vector_size))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "skipgram = models.Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=1, window=window_size, min_count=min_count, workers=workers)\n",
    "\n",
    "df['word2vec_skipgram'] = df['tokens'].apply(lambda x: get_average_word2vec2(x, skipgram, vector_size))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the matrix will be the same as BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love to watch the movie s you have on here and the tv shows i love them growing_heart\n",
      "[-2.08816528e-02  1.15051270e-02  2.57377625e-02  1.76330566e-01\n",
      " -6.28890991e-02 -5.34057617e-05  5.42964935e-02 -1.09580994e-01\n",
      "  1.54647827e-02  7.56883621e-02 -3.56063843e-02 -1.48437500e-01\n",
      " -7.52716064e-02 -1.43718719e-02 -1.27414703e-01  8.87413025e-02\n",
      "  7.78579712e-02  9.60235596e-02  8.70895386e-03 -3.39450836e-02\n",
      " -5.30548096e-02  6.39114380e-02  7.28273392e-02  1.10912323e-03\n",
      "  4.01535034e-02  3.60412598e-02 -5.64794540e-02  4.14695740e-02\n",
      "  1.10197067e-02 -4.98657227e-02 -4.34494019e-02  7.13858604e-02\n",
      " -8.34045410e-02  2.02445984e-02  1.19743347e-02  7.50732422e-03\n",
      " -9.05013084e-03  3.79447937e-02  2.40020752e-02  1.17538452e-01\n",
      "  3.45458984e-02 -8.88824463e-02  1.42479420e-01  2.61783600e-02\n",
      " -1.01394653e-02  1.56650543e-02  3.45573425e-02 -4.45880890e-02\n",
      "  4.49352264e-02 -2.30102539e-02 -2.92930603e-02  5.40466309e-02\n",
      "  1.87149048e-02  8.07876587e-02  9.26971436e-04 -1.28021240e-02\n",
      " -6.44063950e-02 -2.71072388e-02  1.59311295e-02 -2.65610218e-02\n",
      "  2.09045410e-03  5.79319000e-02 -1.18476868e-01 -1.41220093e-02\n",
      " -8.63647461e-03 -2.70414352e-02 -6.69097900e-02  6.64672852e-02\n",
      "  1.00216866e-02  4.15725708e-02  6.54754639e-02  4.84867096e-02\n",
      "  3.16143036e-02  1.87530518e-02 -1.16477966e-01 -6.05697632e-02\n",
      "  5.04465103e-02  6.65550232e-02  6.33453280e-02  1.11518860e-01\n",
      " -7.19528198e-02 -2.11000443e-02  6.89163208e-02 -7.91816711e-02\n",
      " -5.51719666e-02 -1.32980347e-02 -1.13021851e-01  1.26502991e-01\n",
      " -1.10621452e-02  3.68614197e-02 -4.24766541e-03  1.01593018e-01\n",
      " -3.06205750e-02 -9.46655273e-02 -3.90090942e-02  4.39608097e-03\n",
      "  5.74817657e-02  1.33476257e-02  5.25512695e-02  1.27105713e-02\n",
      " -3.09545994e-02 -1.41403675e-02  9.11560059e-02  4.90951538e-03\n",
      " -3.75213623e-02  4.80275154e-02 -7.34529495e-02 -3.55758667e-02\n",
      "  4.52842712e-02 -1.39831543e-01 -1.16088867e-01 -2.12707520e-02\n",
      " -7.67898560e-03 -4.48303223e-02  1.49307251e-01 -8.17871094e-03\n",
      "  1.14898682e-02 -9.58404541e-02  4.00238037e-02 -3.06963921e-03\n",
      " -3.82633209e-02 -3.37448120e-02 -6.70471191e-02  5.91964722e-02\n",
      " -6.97898865e-03 -1.02451324e-01 -1.16607666e-01 -2.02178955e-03\n",
      " -2.68707275e-02  2.73079872e-02 -1.05247498e-01 -1.01848602e-01\n",
      " -7.22351074e-02  2.08892822e-02 -1.31568909e-02 -4.58669662e-02\n",
      " -1.71089172e-02 -1.40686035e-02  2.97546387e-02  2.28576660e-02\n",
      "  1.01593018e-01 -6.64749146e-02  4.73318100e-02 -8.13674927e-03\n",
      " -1.76010132e-02  5.43212891e-03  6.34613037e-02 -2.46725082e-02\n",
      " -3.94401550e-02 -3.38554382e-04  9.12132263e-02  5.01556396e-02\n",
      " -7.70568848e-02 -7.59243965e-03 -1.02466583e-01 -4.16660309e-02\n",
      " -2.36673355e-02 -4.33197021e-02 -5.67359924e-02 -1.74064636e-02\n",
      "  1.49793625e-02  8.94851685e-02 -7.89585114e-02  1.73675537e-01\n",
      "  1.90391541e-02 -1.06060028e-01  1.13124847e-01 -1.06105804e-02\n",
      " -3.60016823e-02  1.29699707e-02 -1.31332397e-01 -3.94439697e-02\n",
      " -7.67898560e-03 -8.98971558e-02 -4.54597473e-02  5.64022064e-02\n",
      "  1.25133216e-01 -1.07540131e-01  4.34875488e-03 -5.63125610e-02\n",
      " -2.91328430e-02 -5.62657118e-02  1.98407173e-02 -1.51710510e-02\n",
      "  1.34363174e-02  8.85391235e-03 -9.70497131e-02  6.03103638e-03\n",
      "  8.40148926e-02  1.97186470e-02  4.56390381e-02  8.45489502e-02\n",
      " -4.77981567e-02 -1.12838745e-02  6.56871796e-02  7.56502151e-02\n",
      "  5.41687012e-04 -3.30066681e-03 -6.23054504e-02 -1.47552490e-01\n",
      " -1.26504898e-02  7.54928589e-02 -1.21841431e-01 -3.50799561e-02\n",
      "  2.26478577e-02 -2.35977173e-02 -4.49457169e-02  3.72514725e-02\n",
      " -3.85131836e-02  8.10890198e-02 -6.58416748e-03  1.78522110e-01\n",
      " -9.84115601e-02 -3.18107605e-02 -1.42797470e-01  1.18160248e-02\n",
      "  1.49399757e-01 -3.06615829e-02 -1.66305542e-01 -2.11277008e-02\n",
      " -4.09393311e-02  2.37770081e-02  3.11279297e-03  3.18145752e-02\n",
      " -4.19769287e-02 -8.30965042e-02  5.04608154e-02  8.49628448e-02\n",
      " -3.25357914e-02 -8.51135254e-02  2.48107910e-02 -9.22851562e-02\n",
      "  3.09959054e-02  6.65740967e-02  1.15777731e-01  2.32620239e-02\n",
      "  1.74560547e-02  1.62391663e-02  1.00845337e-01  4.29916382e-02\n",
      "  6.52065277e-02 -4.14371490e-04  7.92503357e-03 -6.10867739e-02\n",
      " -8.12759399e-02  4.87060547e-02  1.64078474e-02  7.76901245e-02\n",
      " -1.95817798e-02 -6.79063797e-02  6.97159767e-02  6.62317276e-02\n",
      "  5.02052307e-02  5.58700562e-02  7.19642639e-03 -3.32469940e-02\n",
      " -7.47561455e-03 -2.40631104e-02 -8.44001770e-02 -5.85479736e-02\n",
      " -3.52783203e-02 -9.01794434e-03  2.50511169e-02  6.50978088e-03\n",
      "  1.56879425e-02  1.56066895e-01 -2.92015076e-02 -1.64165497e-02\n",
      "  5.15365601e-03  6.78443909e-03  7.94887543e-03  1.20155334e-01\n",
      "  1.15600586e-01  6.21566772e-02  9.04541016e-02 -6.76918030e-02\n",
      " -6.18302822e-02 -1.25659943e-01 -6.88380003e-02 -1.79023743e-02\n",
      " -1.20925903e-02  4.44221497e-03 -1.65939331e-03  9.82055664e-02\n",
      "  2.64835358e-03  2.83892155e-02 -6.98013306e-02 -1.46160126e-02\n",
      " -4.16984558e-02  1.64260864e-02 -3.25264931e-02  8.89968872e-02\n",
      " -1.15447998e-01 -4.02385592e-02  5.38635254e-03 -1.12630844e-01\n",
      " -3.43360901e-02 -4.90150452e-02 -2.57425308e-02 -2.18200684e-02]\n"
     ]
    }
   ],
   "source": [
    "print(df['content_cleaned'][44])\n",
    "print(df['word2vec_pretrained'][44])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we cannot understand just by seeing the embedding vector what the sequence represents. However, the machine learning algorithms that use these embeddings as inputs, can understand much better the features and the sequences than the frequency-based vectorizing methods, since they are created in a way that takes the context into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting feature of the Word2Vec models is that for a particular word, they can give its most similar words in the corpus. This is done by calculating the cosine similarity of our word with the closest words in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.8676770329475403), ('movies', 0.8013108372688293), ('films', 0.7363011837005615), ('moive', 0.6830360889434814), ('Movie', 0.6693680286407471), ('horror_flick', 0.6577848792076111), ('sequel', 0.6577793955802917), ('Guy_Ritchie_Revolver', 0.650975227355957), ('romantic_comedy', 0.6413198709487915), ('flick', 0.6321909427642822)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.8720470070838928), ('show', 0.6472330093383789), ('program', 0.6291745901107788), ('movies', 0.6163832545280457), ('films', 0.579606294631958), ('trailer', 0.5703409910202026), ('episode', 0.5682822465896606), ('programme', 0.5584979057312012), ('scene', 0.5447840690612793), ('title', 0.539584755897522)]\n"
     ]
    }
   ],
   "source": [
    "print(cbow.wv.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.7316080331802368), ('tittle', 0.6571516394615173), ('movies', 0.6406260132789612), ('continuation', 0.6287208199501038), ('moive', 0.626655638217926), ('flim', 0.6253799200057983), ('serie', 0.6253756880760193), ('weather', 0.6172146797180176), ('flick', 0.6149208545684814), ('unpopular', 0.6134818196296692)]\n"
     ]
    }
   ],
   "source": [
    "print(skipgram.wv.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example of the word \"movie\" we see what similar words the 3 different models are giving. For all 3 of the different embeddings, we see similar words, however CBoW seems to be better than Skip-Gram. We can notice that CBoW gives more frequent and correct words, while Skip-Gram might be giving more rare  and single is-occurences of words. For this reason we will continue with the pretrained version and CBoW when we deploy our models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Word2Vec\n",
    "\n",
    "Positive:\n",
    "- **Scalability**: Works well with large datasets and can be easily scaled.\n",
    "- **Low Dimensionality**: The dimensionality is exceptionally reduced, compared to the frequency-based methods.\n",
    "- **Context taken into account**: Semantics and context are the base element of the vectors.\n",
    "\n",
    "Negative:\n",
    "- **Needs big dataset** : Requires a large amount of training data to produce high-quality embeddings.\n",
    "- **Out-Of-Vocabulary Issue**: Once again we work on a finite vocabulary, based on the corpus our model was trained on. If we have a new sequence with new words, they will not be taken into consideration when the embedding is produced, possibly losing important information.\n",
    "- **Polysemy issue**: Can struggle with words with multiple meanings since it produces one vector per word.\n",
    "- **Fixed window space**: Context is limited to a certain window size around the target word, losing long range dependencies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

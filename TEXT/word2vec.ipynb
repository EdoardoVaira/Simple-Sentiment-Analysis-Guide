{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a method for turning words into numbers (vectors) in a way that captures their meaning based on the context in which they appear. Unlike simpler methods like Bag-of-Words (BoW) or TF-IDF, which just count word occurrences, Word2Vec builds contextual word representations. This means that words with similar meanings end up with similar vectors in the Word2Vec model.\n",
    "\n",
    "A famous example of Word2Vec's power is the analogy:\n",
    "**king - man + woman = queen**\n",
    "\n",
    "This shows that Word2Vec understands relationships between words so well that you can perform basic arithmetic on the vectors to solve analogies!\n",
    "\n",
    "### How Word2Vec Works\n",
    "\n",
    "Word2Vec works by creating word embeddings, which are n-dimensional vectors representing words. So a single word will be represented as a list of n numbers. These vectors are trained using a neural network with two key architectures:\n",
    "\n",
    "- CBOW (Continuous Bag of Words)\n",
    "- Skip-Gram\n",
    "\n",
    "In both cases, the network is simple (just two layers) and the goal isn't to classify data but to learn the word embeddings from the training data. Once trained, you throw away the neural network and use the input-to-hidden weight matrix as your word embeddings.\n",
    "\n",
    "\n",
    "### Continuous Bag of Words (CBOW)\n",
    "\n",
    "**How CBOW Works:**\n",
    "\n",
    "In CBOW, the model takes words from the context (the words surrounding a target word) as input and tries to predict the target word.\n",
    "\n",
    "**Example:** For the sentence:\n",
    "‚ÄúI enjoy reading books about history,‚Äù\n",
    "if the context window size is 2, the model will take the words:\n",
    "[\"I\", \"enjoy\", \"books\", \"about\"]\n",
    "and try to predict the middle word, which is \"reading\".\n",
    "\n",
    "This is repeated for every word in the training data, using the neighboring words to predict the target word. You can set the context window size to decide how many words on each side of the target word to include.\n",
    "\n",
    "**How the Neural Network is Built:**\n",
    "\n",
    "1. **Vocabulary and One-Hot Encoding:** First, every word in your vocabulary (all unique words in the text) is assigned a one-hot encoded vector. This is a large vector with a length equal to the vocabulary size, with a 1 in the position of the word and 0s everywhere else.\n",
    "    - Example: If the vocabulary size is 10,000, the word \"cat\" might be represented as [0, 0, 0, ..., 1, 0, 0] (vector of length 10,000).\n",
    "\n",
    "2. **Word Embeddings Matrix:** The input layer of the network is the one-hot encoded vectors for the context words. These vectors are multiplied by the input-to-hidden weights matrix, which gives you a much smaller vector called the word embedding (e.g., instead of a 10,000-length vector, you might get a 300-length vector).\n",
    "\n",
    "    After training, the weight matrix becomes your word embedding matrix, where each row corresponds to a word‚Äôs embedding.\n",
    "\n",
    "3. **Training Objective:** The network learns by adjusting the weights to maximize the probability of correctly predicting the target word, given the context words. Once trained, you can discard the neural network and keep the learned embeddings.\n",
    "\n",
    "**Embedding Matrix Example:**\n",
    "\n",
    "- Suppose you have a vocabulary of 39,783 words.\n",
    "- You define the hidden layer (embedding size) to be 300.\n",
    "- After training, you get a 39,783 x 300 matrix, where each word in your vocabulary is represented by a 300-dimensional vector.\n",
    "\n",
    "When you input a word (like \"cat\"), it is converted to a one-hot vector, which is then multiplied by the embedding matrix to get its word embedding.\n",
    "\n",
    "**CBOW Summary:**\n",
    "- Input: Surrounding words (context).\n",
    "- Goal: Predict the middle word.\n",
    "- Training Output: A word embeddings matrix.\n",
    "\n",
    "\n",
    "### Skip-Gram\n",
    "\n",
    "Skip-Gram is the reverse of CBOW. Instead of using context words to predict a target word, Skip-Gram takes a target word and predicts the context words around it.\n",
    "\n",
    "**Example:** For the sentence:\n",
    "‚ÄúThe small kid ate a banana‚Äù, with a context window size of 2, you would use the word \"ate\" as the input, and the targets would be the words [\"small\", \"kid\", \"a\", \"banana\"].\n",
    "\n",
    "**How the Neural Network is Built:**\n",
    "\n",
    "The process is similar to CBOW, but in Skip-Gram:\n",
    "\n",
    "- The input to the neural network is a single target word.\n",
    "- The output is the prediction of the surrounding context words.\n",
    "\n",
    "Again, the neural network is trained to maximize the probability of predicting the correct context words given the target word. Once trained, the word embeddings are stored in the input-to-hidden weight matrix, just like with CBOW.\n",
    "\n",
    "**Skip-Gram Summary:**\n",
    "\n",
    "- Input: Target word.\n",
    "- Goal: Predict surrounding words.\n",
    "- Training Output: A word embeddings matrix.\n",
    "\n",
    "### CBOW vs. Skip-Gram: Which One to Use?\n",
    "\n",
    "- CBOW is faster and works better on large datasets. It‚Äôs good at predicting common words and is often the default choice when you have a lot of data.\n",
    "- Skip-Gram is slower but better for smaller datasets and works well when you're focusing on rare words. It does a better job learning the embeddings for less frequent words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use it for Sentiment Analysis\n",
    "\n",
    "Preprocess the Text:\n",
    "\n",
    "- Tokenize the text (split it into words).\n",
    "- Remove stopwords (optional).\n",
    "- Convert words into embeddings using the trained Word2Vec model.\n",
    "\n",
    "For example, the sentence \"I loved the movie\" could become:\n",
    "\n",
    "- \"I\" ‚Üí [0.01, 0.02, -0.01, ...]\n",
    "- \"loved\" ‚Üí [0.3, -0.1, 0.2, ...]\n",
    "- \"movie\" ‚Üí [0.25, -0.15, 0.05, ...]\n",
    "\n",
    "**Aggregating Embeddings**:\n",
    "\n",
    "After obtaining the embeddings, the next step is to combine them into a single vector representing the entire sentence. Some possible methods are:\n",
    "\n",
    "- Averaging (used here): Take the mean of the embeddings.\n",
    "- Summing: Add the embeddings together.\n",
    "- Weighted Averaging: Weight words by importance.\n",
    "- Concatenation: Combine embeddings for the first few words.\n",
    "- Recurrent Models: Use LSTMs or GRUs to capture sequential information.\n",
    "- Attention Mechanisms: Focus on important words.\n",
    "\n",
    "In this notebook, We will keep it simple and use averaging to create a single vector per sentence.\n",
    "\n",
    "**Note**: word2vec is not inherently a method for modeling sentences, only words. So there is no single, official way to use word2vec to represent sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in Python\n",
    "\n",
    "Let's begin by importing the libraries we need. We will use a library called Gensim - you will see why in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import models\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a sample dataset. Just like we did in the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame with Netflix app reviews\n",
    "df = pd.DataFrame({\n",
    "    'Content_cleaned': [\n",
    "        'the app is great new features but crashes often',\n",
    "        'love the app love the content but it crashes',\n",
    "        'the app crashes too much it is frustrating',\n",
    "        'the content is great it is easy to use it is great'\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Content_cleaned\n",
      "0     the app is great new features but crashes often\n",
      "1        love the app love the content but it crashes\n",
      "2          the app crashes too much it is frustrating\n",
      "3  the content is great it is easy to use it is great\n"
     ]
    }
   ],
   "source": [
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained vs. Self-trained Word Embeddings\n",
    "\n",
    "In this notebook, we will explore two methods for generating word embeddings:\n",
    "\n",
    "- Using a pre-trained model: We will use Google‚Äôs pre-trained Word2Vec model, which is widely used and trained on 100 billion words from the Google News dataset. The model generates word vectors of size 300.\n",
    "- Training our own model: We'll also train our own Word2Vec model from scratch to see how it compares in both vectorizing text and in model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-trained Word2Vec Model**\n",
    "\n",
    "Google‚Äôs pre-trained Word2Vec model is available via the Gensim library. If the model is not already downloaded, the notebook will automatically download it, save it locally, and load it into memory.\n",
    "\n",
    "Here‚Äôs how we download and load the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model.\n"
     ]
    }
   ],
   "source": [
    "# Define paths for the model\n",
    "model_dir = 'Pre-trained-models'\n",
    "model_path_gensim = os.path.join(model_dir, 'GoogleNews-vectors-negative300.kv')\n",
    "\n",
    "# Ensure the model directory exists\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Download and load the pre-trained model if not already present\n",
    "if not os.path.isfile(model_path_gensim):\n",
    "    print(\"Downloading Google News Word2Vec model...\")\n",
    "    w2v = api.load('word2vec-google-news-300')\n",
    "    w2v.save(model_path_gensim)\n",
    "    print(\"Model downloaded and saved.\")\n",
    "else:\n",
    "    print(\"Loading existing model.\")\n",
    "    w2v = models.KeyedVectors.load(model_path_gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Averaging Word Vectors to Represent Text Sequences**\n",
    "\n",
    "To represent a sequence of text (like a sentence or a document), we calculate the word embedding for each individual token (word) and then average these vectors. This approach captures the overall meaning of the text. Each word contributes to the final vector, so the meaning of the whole sentence is a blend of all the individual word meanings.\n",
    "\n",
    "The process is implemented in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, model, vector_size):\n",
    "    \n",
    "    # Filter valid tokens and return zero vector if none are found\n",
    "    valid_tokens = [token for token in tokens_list if token in model]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Compute and return the average vector\n",
    "    word_vectors = np.array([model[token] for token in valid_tokens])\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tokenize our text data and apply the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[0.056230333, 0.04981825, -0.042060003, 0.0209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[0.072252065, -0.026354684, -0.020263672, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[0.1003685, 0.001052618, -0.05606079, 0.036048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[0.07695146, 0.021551305, 0.054709695, 0.08551...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                 word2vec_pretrained  \n",
       "0  [0.056230333, 0.04981825, -0.042060003, 0.0209...  \n",
       "1  [0.072252065, -0.026354684, -0.020263672, 0.04...  \n",
       "2  [0.1003685, 0.001052618, -0.05606079, 0.036048...  \n",
       "3  [0.07695146, 0.021551305, 0.054709695, 0.08551...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "df['tokens'] = df['Content_cleaned'].apply(lambda x: x.split())\n",
    "\n",
    "# Compute the average Word2Vec for each row using the custom function\n",
    "vector_size = w2v.vector_size\n",
    "df['word2vec_pretrained'] = df['tokens'].apply(lambda x: get_average_word2vec(x, w2v, vector_size))\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the example below, our sequences have been turned into a vector of length 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(df['word2vec_pretrained'][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Our Own Word2Vec Models\n",
    "\n",
    "In this section, we will train our own Continuous Bag of Words (CBOW) and Skip-Gram models using Gensim. Both models will transform our text sequences into 1x300 dimensional vectors, capturing the meanings of the words in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Implementation**\n",
    "\n",
    "First, let's import the necessary library and define a function to compute the average word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def get_average_word2vec_custom(tokens_list, model, vector_size):\n",
    "    # Filter tokens that are valid (present in the model)\n",
    "    valid_tokens = [token for token in tokens_list if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)  # Return a zero vector if no valid tokens are found\n",
    "    \n",
    "    # Calculate the average vector of valid tokens\n",
    "    word_vectors = [model.wv[token] for token in valid_tokens]\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Parameters**\n",
    "\n",
    "Next, we will define the parameters for our Word2Vec models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters\n",
    "vector_size = 300   # Dimension of word vectors\n",
    "window_size = 5     # Size of the context window\n",
    "min_count = 1       # Minimum frequency for words to be included in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Models**\n",
    "\n",
    "Now, let's train the CBOW and Skip-Gram models. The sg parameter determines the model type: sg=0 for CBOW and sg=1 for Skip-Gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CBoW model (sg=0)\n",
    "cbow_model = Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=0, window=window_size, min_count=min_count, workers=1)\n",
    "\n",
    "# Calculate average word vectors for the CBOW model\n",
    "df['word2vec_cbow'] = df['tokens'].apply(lambda x: get_average_word2vec_custom(x, cbow_model, vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Skip-Gram model (sg=1)\n",
    "skipgram_model = Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=1, window=window_size, min_count=min_count, workers=1)\n",
    "\n",
    "# Calculate average word vectors for the Skip-Gram model\n",
    "df['word2vec_skipgram'] = df['tokens'].apply(lambda x: get_average_word2vec_custom(x, skipgram_model, vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Word2Vec Parameters**\n",
    "\n",
    "- Vector Size: The dimensionality of the word embeddings (300 dimensions).\n",
    "- Window Size: The size of the context window used to predict words.\n",
    "- Minimum Count: The minimum frequency a word must have to be considered for training.\n",
    "\n",
    "**Model Types**\n",
    "\n",
    "- CBOW: Uses the context (surrounding words) to predict the target word (sg=0).\n",
    "- Skip-Gram: Uses the target word to predict its surrounding context (sg=1).\n",
    "\n",
    "**Result**\n",
    "\n",
    "After training both models, we can display the first few entries of our DataFrame to see the average word vector representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>word2vec_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the app is great new features but crashes often</td>\n",
       "      <td>[the, app, is, great, new, features, but, cras...</td>\n",
       "      <td>[0.056230333, 0.04981825, -0.042060003, 0.0209...</td>\n",
       "      <td>[-3.9464252e-05, 8.735372e-05, 0.00023468967, ...</td>\n",
       "      <td>[-3.8613955e-05, 8.666539e-05, 0.00023473676, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love the app love the content but it crashes</td>\n",
       "      <td>[love, the, app, love, the, content, but, it, ...</td>\n",
       "      <td>[0.072252065, -0.026354684, -0.020263672, 0.04...</td>\n",
       "      <td>[-0.00015348375, -0.00045190606, 0.00050380756...</td>\n",
       "      <td>[-0.00015304553, -0.00045252705, 0.00050321664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app crashes too much it is frustrating</td>\n",
       "      <td>[the, app, crashes, too, much, it, is, frustra...</td>\n",
       "      <td>[0.1003685, 0.001052618, -0.05606079, 0.036048...</td>\n",
       "      <td>[-6.593054e-05, 0.0005229764, -0.0010689881, -...</td>\n",
       "      <td>[-6.565961e-05, 0.0005227553, -0.0010691253, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the content is great it is easy to use it is g...</td>\n",
       "      <td>[the, content, is, great, it, is, easy, to, us...</td>\n",
       "      <td>[0.07695146, 0.021551305, 0.054709695, 0.08551...</td>\n",
       "      <td>[-1.8200057e-05, 0.0007764301, 0.0007181768, -...</td>\n",
       "      <td>[-1.7555154e-05, 0.00077540375, 0.00071704615,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Content_cleaned  \\\n",
       "0    the app is great new features but crashes often   \n",
       "1       love the app love the content but it crashes   \n",
       "2         the app crashes too much it is frustrating   \n",
       "3  the content is great it is easy to use it is g...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [the, app, is, great, new, features, but, cras...   \n",
       "1  [love, the, app, love, the, content, but, it, ...   \n",
       "2  [the, app, crashes, too, much, it, is, frustra...   \n",
       "3  [the, content, is, great, it, is, easy, to, us...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.056230333, 0.04981825, -0.042060003, 0.0209...   \n",
       "1  [0.072252065, -0.026354684, -0.020263672, 0.04...   \n",
       "2  [0.1003685, 0.001052618, -0.05606079, 0.036048...   \n",
       "3  [0.07695146, 0.021551305, 0.054709695, 0.08551...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [-3.9464252e-05, 8.735372e-05, 0.00023468967, ...   \n",
       "1  [-0.00015348375, -0.00045190606, 0.00050380756...   \n",
       "2  [-6.593054e-05, 0.0005229764, -0.0010689881, -...   \n",
       "3  [-1.8200057e-05, 0.0007764301, 0.0007181768, -...   \n",
       "\n",
       "                                   word2vec_skipgram  \n",
       "0  [-3.8613955e-05, 8.666539e-05, 0.00023473676, ...  \n",
       "1  [-0.00015304553, -0.00045252705, 0.00050321664...  \n",
       "2  [-6.565961e-05, 0.0005227553, -0.0010691253, -...  \n",
       "3  [-1.7555154e-05, 0.00077540375, 0.00071704615,...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Word2Vec to the Netflix Dataset\n",
    "\n",
    "In this section, we will utilize the Word2Vec models we‚Äôve trained on the Netflix dataset. Let‚Äôs start by loading the dataset and processing the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the preprocessed dataset\n",
    "df = pd.read_csv('../DATASETS/preprocessed_text.csv')\n",
    "\n",
    "# Fill any empty text entries that resulted from preprocessing\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing the Text Data**\n",
    "\n",
    "Next, we will tokenize the cleaned content to prepare it for embedding calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the cleaned text data\n",
    "df['tokens'] = df['content_cleaned'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Average Word Vectors**\n",
    "\n",
    "Now, we will compute the average Word2Vec vectors for each row in the dataset using the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average Word2Vec vectors using the pre-trained model\n",
    "vector_size = w2v.vector_size  # Retrieve vector size from the pre-trained model\n",
    "df['word2vec_pretrained'] = df['tokens'].apply(lambda x: get_average_word2vec(x, w2v, vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Custom Word2Vec Models**\n",
    "\n",
    "Let‚Äôs proceed to train our own CBOW and Skip-Gram models using the tokenized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CBOW model\n",
    "cbow_model = Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=0, window=window_size, min_count=min_count, workers=1)\n",
    "\n",
    "# Calculate average word vectors for the CBOW model\n",
    "df['word2vec_cbow'] = df['tokens'].apply(lambda x: get_average_word2vec_custom(x, cbow_model, vector_size))\n",
    "\n",
    "# Train the Skip-Gram model\n",
    "skipgram_model = Word2Vec(df['tokens'].tolist(), vector_size=vector_size, sg=1, window=window_size, min_count=min_count, workers=1)\n",
    "\n",
    "# Calculate average word vectors for the Skip-Gram model\n",
    "df['word2vec_skipgram'] = df['tokens'].apply(lambda x: get_average_word2vec_custom(x, skipgram_model, vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displaying the Results**\n",
    "\n",
    "After processing, let‚Äôs display the first few entries of our DataFrame to observe the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>content_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word2vec_pretrained</th>\n",
       "      <th>word2vec_cbow</th>\n",
       "      <th>word2vec_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>plss stopp giving screen limit like when you a...</td>\n",
       "      <td>[plss, stopp, giving, screen, limit, like, whe...</td>\n",
       "      <td>[0.060924955, 0.036983438, 0.052580304, 0.1163...</td>\n",
       "      <td>[0.007439245, -0.1203659, -0.104098774, -0.202...</td>\n",
       "      <td>[0.020204937, 0.15391015, -0.06393335, -0.0217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.6157658, -0.33845198, -0.5885259, -0.028577...</td>\n",
       "      <td>[0.070144735, 0.44430047, -0.008070361, 0.1648...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>thumbs_up</td>\n",
       "      <td>[thumbs_up]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.12089115, -0.37054986, 0.04350766, -0.1041...</td>\n",
       "      <td>[0.05704466, 0.121295236, -0.053278703, -0.183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[0.040527344, 0.0625, -0.017456055, 0.07861328...</td>\n",
       "      <td>[0.6157658, -0.33845198, -0.5885259, -0.028577...</td>\n",
       "      <td>[0.070144735, 0.44430047, -0.008070361, 0.1648...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>app is useful to certain phone brand it is not...</td>\n",
       "      <td>[app, is, useful, to, certain, phone, brand, i...</td>\n",
       "      <td>[0.016780308, -0.041579314, 0.043486457, 0.058...</td>\n",
       "      <td>[-0.14768976, -0.20620891, -0.036397066, -0.10...</td>\n",
       "      <td>[0.058957115, 0.17874919, -0.0928607, 0.039189...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  score  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2   \n",
       "1                                               Good      5   \n",
       "2                                                 üëçüëç      5   \n",
       "3                                               Good      3   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1   \n",
       "\n",
       "                                     content_cleaned  \\\n",
       "0  plss stopp giving screen limit like when you a...   \n",
       "1                                               good   \n",
       "2                                          thumbs_up   \n",
       "3                                               good   \n",
       "4  app is useful to certain phone brand it is not...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [plss, stopp, giving, screen, limit, like, whe...   \n",
       "1                                             [good]   \n",
       "2                                        [thumbs_up]   \n",
       "3                                             [good]   \n",
       "4  [app, is, useful, to, certain, phone, brand, i...   \n",
       "\n",
       "                                 word2vec_pretrained  \\\n",
       "0  [0.060924955, 0.036983438, 0.052580304, 0.1163...   \n",
       "1  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.040527344, 0.0625, -0.017456055, 0.07861328...   \n",
       "4  [0.016780308, -0.041579314, 0.043486457, 0.058...   \n",
       "\n",
       "                                       word2vec_cbow  \\\n",
       "0  [0.007439245, -0.1203659, -0.104098774, -0.202...   \n",
       "1  [0.6157658, -0.33845198, -0.5885259, -0.028577...   \n",
       "2  [-0.12089115, -0.37054986, 0.04350766, -0.1041...   \n",
       "3  [0.6157658, -0.33845198, -0.5885259, -0.028577...   \n",
       "4  [-0.14768976, -0.20620891, -0.036397066, -0.10...   \n",
       "\n",
       "                                   word2vec_skipgram  \n",
       "0  [0.020204937, 0.15391015, -0.06393335, -0.0217...  \n",
       "1  [0.070144735, 0.44430047, -0.008070361, 0.1648...  \n",
       "2  [0.05704466, 0.121295236, -0.053278703, -0.183...  \n",
       "3  [0.070144735, 0.44430047, -0.008070361, 0.1648...  \n",
       "4  [0.058957115, 0.17874919, -0.0928607, 0.039189...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Embeddings**\n",
    "\n",
    "For example, we can check a specific content entry and its corresponding Word2Vec vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love to watch the movie s you have on here and the tv shows i love them growing_heart\n",
      "[-2.08816528e-02  1.15051270e-02  2.57377625e-02  1.76330566e-01\n",
      " -6.28890991e-02 -5.34057617e-05  5.42964935e-02 -1.09580994e-01\n",
      "  1.54647827e-02  7.56883621e-02 -3.56063843e-02 -1.48437500e-01\n",
      " -7.52716064e-02 -1.43718719e-02 -1.27414703e-01  8.87413025e-02\n",
      "  7.78579712e-02  9.60235596e-02  8.70895386e-03 -3.39450836e-02\n",
      " -5.30548096e-02  6.39114380e-02  7.28273392e-02  1.10912323e-03\n",
      "  4.01535034e-02  3.60412598e-02 -5.64794540e-02  4.14695740e-02\n",
      "  1.10197067e-02 -4.98657227e-02 -4.34494019e-02  7.13858604e-02\n",
      " -8.34045410e-02  2.02445984e-02  1.19743347e-02  7.50732422e-03\n",
      " -9.05013084e-03  3.79447937e-02  2.40020752e-02  1.17538452e-01\n",
      "  3.45458984e-02 -8.88824463e-02  1.42479420e-01  2.61783600e-02\n",
      " -1.01394653e-02  1.56650543e-02  3.45573425e-02 -4.45880890e-02\n",
      "  4.49352264e-02 -2.30102539e-02 -2.92930603e-02  5.40466309e-02\n",
      "  1.87149048e-02  8.07876587e-02  9.26971436e-04 -1.28021240e-02\n",
      " -6.44063950e-02 -2.71072388e-02  1.59311295e-02 -2.65610218e-02\n",
      "  2.09045410e-03  5.79319000e-02 -1.18476868e-01 -1.41220093e-02\n",
      " -8.63647461e-03 -2.70414352e-02 -6.69097900e-02  6.64672852e-02\n",
      "  1.00216866e-02  4.15725708e-02  6.54754639e-02  4.84867096e-02\n",
      "  3.16143036e-02  1.87530518e-02 -1.16477966e-01 -6.05697632e-02\n",
      "  5.04465103e-02  6.65550232e-02  6.33453280e-02  1.11518860e-01\n",
      " -7.19528198e-02 -2.11000443e-02  6.89163208e-02 -7.91816711e-02\n",
      " -5.51719666e-02 -1.32980347e-02 -1.13021851e-01  1.26502991e-01\n",
      " -1.10621452e-02  3.68614197e-02 -4.24766541e-03  1.01593018e-01\n",
      " -3.06205750e-02 -9.46655273e-02 -3.90090942e-02  4.39608097e-03\n",
      "  5.74817657e-02  1.33476257e-02  5.25512695e-02  1.27105713e-02\n",
      " -3.09545994e-02 -1.41403675e-02  9.11560059e-02  4.90951538e-03\n",
      " -3.75213623e-02  4.80275154e-02 -7.34529495e-02 -3.55758667e-02\n",
      "  4.52842712e-02 -1.39831543e-01 -1.16088867e-01 -2.12707520e-02\n",
      " -7.67898560e-03 -4.48303223e-02  1.49307251e-01 -8.17871094e-03\n",
      "  1.14898682e-02 -9.58404541e-02  4.00238037e-02 -3.06963921e-03\n",
      " -3.82633209e-02 -3.37448120e-02 -6.70471191e-02  5.91964722e-02\n",
      " -6.97898865e-03 -1.02451324e-01 -1.16607666e-01 -2.02178955e-03\n",
      " -2.68707275e-02  2.73079872e-02 -1.05247498e-01 -1.01848602e-01\n",
      " -7.22351074e-02  2.08892822e-02 -1.31568909e-02 -4.58669662e-02\n",
      " -1.71089172e-02 -1.40686035e-02  2.97546387e-02  2.28576660e-02\n",
      "  1.01593018e-01 -6.64749146e-02  4.73318100e-02 -8.13674927e-03\n",
      " -1.76010132e-02  5.43212891e-03  6.34613037e-02 -2.46725082e-02\n",
      " -3.94401550e-02 -3.38554382e-04  9.12132263e-02  5.01556396e-02\n",
      " -7.70568848e-02 -7.59243965e-03 -1.02466583e-01 -4.16660309e-02\n",
      " -2.36673355e-02 -4.33197021e-02 -5.67359924e-02 -1.74064636e-02\n",
      "  1.49793625e-02  8.94851685e-02 -7.89585114e-02  1.73675537e-01\n",
      "  1.90391541e-02 -1.06060028e-01  1.13124847e-01 -1.06105804e-02\n",
      " -3.60016823e-02  1.29699707e-02 -1.31332397e-01 -3.94439697e-02\n",
      " -7.67898560e-03 -8.98971558e-02 -4.54597473e-02  5.64022064e-02\n",
      "  1.25133216e-01 -1.07540131e-01  4.34875488e-03 -5.63125610e-02\n",
      " -2.91328430e-02 -5.62657118e-02  1.98407173e-02 -1.51710510e-02\n",
      "  1.34363174e-02  8.85391235e-03 -9.70497131e-02  6.03103638e-03\n",
      "  8.40148926e-02  1.97186470e-02  4.56390381e-02  8.45489502e-02\n",
      " -4.77981567e-02 -1.12838745e-02  6.56871796e-02  7.56502151e-02\n",
      "  5.41687012e-04 -3.30066681e-03 -6.23054504e-02 -1.47552490e-01\n",
      " -1.26504898e-02  7.54928589e-02 -1.21841431e-01 -3.50799561e-02\n",
      "  2.26478577e-02 -2.35977173e-02 -4.49457169e-02  3.72514725e-02\n",
      " -3.85131836e-02  8.10890198e-02 -6.58416748e-03  1.78522110e-01\n",
      " -9.84115601e-02 -3.18107605e-02 -1.42797470e-01  1.18160248e-02\n",
      "  1.49399757e-01 -3.06615829e-02 -1.66305542e-01 -2.11277008e-02\n",
      " -4.09393311e-02  2.37770081e-02  3.11279297e-03  3.18145752e-02\n",
      " -4.19769287e-02 -8.30965042e-02  5.04608154e-02  8.49628448e-02\n",
      " -3.25357914e-02 -8.51135254e-02  2.48107910e-02 -9.22851562e-02\n",
      "  3.09959054e-02  6.65740967e-02  1.15777731e-01  2.32620239e-02\n",
      "  1.74560547e-02  1.62391663e-02  1.00845337e-01  4.29916382e-02\n",
      "  6.52065277e-02 -4.14371490e-04  7.92503357e-03 -6.10867739e-02\n",
      " -8.12759399e-02  4.87060547e-02  1.64078474e-02  7.76901245e-02\n",
      " -1.95817798e-02 -6.79063797e-02  6.97159767e-02  6.62317276e-02\n",
      "  5.02052307e-02  5.58700562e-02  7.19642639e-03 -3.32469940e-02\n",
      " -7.47561455e-03 -2.40631104e-02 -8.44001770e-02 -5.85479736e-02\n",
      " -3.52783203e-02 -9.01794434e-03  2.50511169e-02  6.50978088e-03\n",
      "  1.56879425e-02  1.56066895e-01 -2.92015076e-02 -1.64165497e-02\n",
      "  5.15365601e-03  6.78443909e-03  7.94887543e-03  1.20155334e-01\n",
      "  1.15600586e-01  6.21566772e-02  9.04541016e-02 -6.76918030e-02\n",
      " -6.18302822e-02 -1.25659943e-01 -6.88380003e-02 -1.79023743e-02\n",
      " -1.20925903e-02  4.44221497e-03 -1.65939331e-03  9.82055664e-02\n",
      "  2.64835358e-03  2.83892155e-02 -6.98013306e-02 -1.46160126e-02\n",
      " -4.16984558e-02  1.64260864e-02 -3.25264931e-02  8.89968872e-02\n",
      " -1.15447998e-01 -4.02385592e-02  5.38635254e-03 -1.12630844e-01\n",
      " -3.43360901e-02 -4.90150452e-02 -2.57425308e-02 -2.18200684e-02]\n"
     ]
    }
   ],
   "source": [
    "# Check the content and corresponding Word2Vec vector\n",
    "print(df['content_cleaned'][44])\n",
    "print(df['word2vec_pretrained'][44])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you might not be able to understand the embedding vectors straight away, machine learning algorithms can use these to capture semantic relationships more effectively than traditional frequency-based methods. Word2Vec embeddings are designed to consider the context of words, which helps to represent textual features better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similar Words\n",
    "\n",
    "One of the powerful features of Word2Vec is its ability to find similar words based on cosine similarity in the vector space. We can explore this functionality as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for 'movie' using pre-trained model: [('film', 0.8676770329475403), ('movies', 0.8013108372688293), ('films', 0.7363011837005615), ('moive', 0.6830360889434814), ('Movie', 0.6693680286407471), ('horror_flick', 0.6577848792076111), ('sequel', 0.6577793955802917), ('Guy_Ritchie_Revolver', 0.650975227355957), ('romantic_comedy', 0.6413198709487915), ('flick', 0.6321909427642822)]\n",
      "Similar words for 'movie' using CBOW model: [('film', 0.8702491521835327), ('show', 0.6649054884910583), ('movies', 0.6187590956687927), ('program', 0.607060432434082), ('trailer', 0.594946026802063), ('episode', 0.5799102783203125), ('films', 0.5590505599975586), ('title', 0.5378875732421875), ('genre', 0.5195510983467102), ('category', 0.5140288472175598)]\n",
      "Similar words for 'movie' using Skip-Gram model: [('film', 0.761932373046875), ('tittle', 0.6838789582252502), ('flim', 0.6558706760406494), ('serie', 0.6544528007507324), ('movies', 0.6517087817192078), ('moive', 0.645378828048706), ('flick', 0.6273512244224548), ('weather', 0.6265140175819397), ('programme', 0.6149237155914307), ('flims', 0.6085973978042603)]\n"
     ]
    }
   ],
   "source": [
    "# Display similar words for the term \"movie\" from all models\n",
    "print(\"Similar words for 'movie' using pre-trained model:\", w2v.most_similar(\"movie\"))\n",
    "print(\"Similar words for 'movie' using CBOW model:\", cbow_model.wv.most_similar(\"movie\"))\n",
    "print(\"Similar words for 'movie' using Skip-Gram model:\", skipgram_model.wv.most_similar(\"movie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we observe the similar words for the term \"movie\" from each model. The results may vary among the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Word2Vec\n",
    "\n",
    "Positive:\n",
    "- **Scalability**: Works well with large datasets and can be easily scaled.\n",
    "- **Low Dimensionality**: The dimensionality is exceptionally reduced, compared to the frequency-based methods.\n",
    "- **Context taken into account**: Semantics and context are the base element of the vectors.\n",
    "\n",
    "Negative:\n",
    "- **Needs big dataset** : Requires a large amount of training data to produce high-quality embeddings.\n",
    "- **Out-Of-Vocabulary Issue**: Once again we work on a finite vocabulary, based on the corpus our model was trained on. If we have a new sequence with new words, they will not be taken into consideration when the embedding is produced, possibly losing important information.\n",
    "- **Polysemy issue**: Can struggle with words with multiple meanings since it produces one vector per word.\n",
    "- **Fixed window space**: Context is limited to a certain window size around the target word, losing long range dependencies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

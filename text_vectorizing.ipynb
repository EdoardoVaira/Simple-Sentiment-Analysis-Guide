{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d117ad",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "## Import all needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450f1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#¬†Text processing\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c59a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d42df88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plsssss stoppppp giving screen limit like when...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>plsssss stoppppp give screen limit like ur wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üëçüëç</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>thumb up thumb up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>App is useful to certain phone brand ,,,,it is...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>app useful certain phone brand except phone tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Score Sentiment  \\\n",
       "0  Plsssss stoppppp giving screen limit like when...      2  negative   \n",
       "1                                               Good      5  positive   \n",
       "2                                                 üëçüëç      5  positive   \n",
       "3                                               Good      3   neutral   \n",
       "4  App is useful to certain phone brand ,,,,it is...      1  negative   \n",
       "\n",
       "                                     Content_cleaned  \n",
       "0  plsssss stoppppp give screen limit like ur wat...  \n",
       "1                                               good  \n",
       "2                                  thumb up thumb up  \n",
       "3                                               good  \n",
       "4  app useful certain phone brand except phone tr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1463c41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Content             0\n",
       "Score               0\n",
       "Sentiment           0\n",
       "Content_cleaned    60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc10d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328b593",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "This method creates literally a bag of words, without taking into account the semantic meaning of the words or their position in the sentence. First, all the inputs are tokenized. Then from all the unique tokens, the algorithm creates a vocabulary in alphabetical order. For every input sequence, the algorithm creates a matrix that has the length of the vocabulary and frequencies of each token are assigned to the corresponding index. The Bag of Words algorithm is implemented with the CountVectorizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa34b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34326\n",
      "(113292, 34326)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the data\n",
    "bow = vectorizer.fit_transform(df['Content_cleaned'])\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec85ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumb up thumb up\n",
      "  (0, 30228)\t2\n",
      "  (0, 31874)\t2\n"
     ]
    }
   ],
   "source": [
    "print(df['Content_cleaned'][2])\n",
    "print(bow[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61cb8857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30228 is thumb.\n",
      "31874 is up.\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab_keys = sorted(vectorizer.vocabulary_.keys())\n",
    "print(f\"30228 is {sorted_vocab_keys[30228]}.\")\n",
    "print(f\"31874 is {sorted_vocab_keys[31874]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90edf48",
   "metadata": {},
   "source": [
    "We notice that the produced vocabulary is of size 34326, while our bag of words has 113292 vectors, each having the size of the vocabulary. \n",
    "\n",
    "In the example we see the that both words \"thumb\" and \"up\" get value of 2.\n",
    "\n",
    "Positive: \n",
    "- Sequences have a fixed size.\n",
    "\n",
    "Negative:\n",
    "- Very high dimensions.\n",
    "- Order of words or semantic meaning is not preserved.\n",
    "- If we have a new sequence that contains new words that are not part of our vocabulary, it will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e486e",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "\n",
    "TF-IDF, or Term Frequency- Inverse Document Frequency, is an algorithm that creates a frequency-based vocabulary, like Bag of Words, but unlike that, it takes word importance into consideration. Basically, it considers that if a word is part of a lot of sentences/sequences, then it must not be very important. However, if a word is present in only a few sentences/sequences, then it must be of high importance. This way words that get repeated too often don‚Äôt overpower less frequent but important words. The formula for words in a sentence/sequence is as follows:\n",
    "- TF(x) = (frequency of word 'x' in a sequence)/(total number of words in the sequence).\n",
    "- IDF(x) = log((total number of sequences)/(number of sequences that contain word 'x')).\n",
    "- TF-IDF(x) - TF(x) * IDF(x).\n",
    "\n",
    "In IDF(x) the document frequency is inversed so the more common a word is across all documents, the lesser its importance is for the current document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df6075ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34326\n",
      "(113292, 34326)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the model and transform the data\n",
    "tfidf = vectorizer.fit_transform(df['Content_cleaned'])\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a34ef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumb up thumb up\n",
      "  (0, 31874)\t0.5497003144270964\n",
      "  (0, 30228)\t0.8353619361203571\n"
     ]
    }
   ],
   "source": [
    "print(df['Content_cleaned'][2])\n",
    "print(tfidf[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722052c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30228 is thumb.\n",
      "31874 is up.\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab_keys = sorted(vectorizer.vocabulary_.keys())\n",
    "print(f\"30228 is {sorted_vocab_keys[30228]}.\")\n",
    "print(f\"31874 is {sorted_vocab_keys[31874]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15851071",
   "metadata": {},
   "source": [
    "We see that just like Bag of Words, we have a vocabulary of 34326 size and 113292 vectors of the same size.\n",
    "\n",
    "In the example we see that unlike Bag of Words, where both words got value 2, the word \"thumb\" gets a higher value than the word \"up\", meaning it is of more importance. The word \"thumb\" must exist in less sequences than the word \"up\", making it more significant.\n",
    "\n",
    "Positive: \n",
    "- Sequences have a fixed size.\n",
    "- Some word importance is considered, unlike Bag of Words.\n",
    "\n",
    "Negative:\n",
    "- Very high dimensions.\n",
    "- Order of words is still not preserved.\n",
    "- Again if we have a new sequence that contains new words that are not part of our vocabulary, it will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e24",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec is a neural network-based model for learning word embeddings. Unlike in the frequency-based vectorization algorithms, the vector representation of words was said to be contextually aware. Since every word is represented as an n-dimensional vector, one can imagine that all of the words are mapped to this n-dimensional space in such a manner that words having similar meanings exist in close proximity to one another in this hyperspace. \n",
    "\n",
    "There are two main ways to implement Word2Vec, CBoW and Skip-Gram.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
